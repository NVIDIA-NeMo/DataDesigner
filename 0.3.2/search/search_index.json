{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83c\udfa8 NeMo Data Designer Library","text":"<p>\ud83d\udc4b Welcome to the Data Designer community! We're excited to have you here.</p> <p>Data Designer is a general framework for generating high-quality synthetic data from scratch or using your own seed data as a starting point for domain-grounded data generation.</p>"},{"location":"#why-data-designer","title":"Why Data Designer?","text":"<p>Generating high-quality synthetic data requires much more than iteratively calling an LLM.</p> <p>Data Designer is purpose-built to support large-scale, high-quality data generation, including</p> <ul> <li>Diversity \u2013 statistical distributions and variety that reflect real-world data patterns, not repetitive LLM outputs\u00a0</li> <li>Correlations \u2013 meaningful relationships between fields that LLMs cannot maintain across independent calls</li> <li>Steerability \u2013 flexible control over data characteristics throughout the generation process</li> <li>Validation \u2013 automated quality checks and verification that data meets specifications</li> <li>Reproducibility \u2013 shareable and reproducible generation workflows</li> </ul>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>Data Designer helps you create datasets through an intuitive, iterative process:</p> <ol> <li>\u2699\ufe0f Configure your model settings<ul> <li>Bring your own OpenAI-compatible model providers and models</li> <li>Or use the default model providers and models to get started quickly</li> <li>Learn more by reading the model docs</li> </ul> </li> <li> <p>\ud83c\udfd7\ufe0f Design your dataset</p> <ul> <li>Iteratively design your dataset, column by column</li> <li>Leverage tools like statistical samplers and LLMs to generate a variety of data types</li> <li>Learn more by reading the column docs</li> </ul> </li> <li> <p>\ud83d\udd01 Preview your results and iterate</p> <ul> <li>Generate a preview dataset stored in memory for fast iteration</li> <li>Inspect sample records and analysis results to refine your configuration</li> <li>Try for yourself by running the tutorial notebooks</li> </ul> </li> <li>\ud83d\uddbc\ufe0f Create your dataset<ul> <li>Generate your full dataset and save results to disk</li> <li>Access the generated dataset and associated artifacts for downstream use</li> <li>Give it a try by running the tutorial notebooks</li> </ul> </li> </ol>"},{"location":"#library-and-microservice","title":"Library and Microservice","text":"<p>Data Designer is available as both an open-source library and a NeMo microservice.</p> <ul> <li>Open-source Library: Purpose-built for flexibility and customization, prioritizing UX excellence, modularity, and extensibility.</li> <li>NeMo Microservice: An enterprise-grade solution that offers a seamless transition from the library, allowing you to leverage other NeMo microservices and generate datasets at scale. See the microservice docs for more details.</li> </ul>"},{"location":"CONTRIBUTING/","title":"\ud83c\udfa8\u2728 Contributing to NeMo Data Designer \ud83c\udfa8\u2728","text":"<p>Thank you for your interest in contributing to Data Designer!</p> <p>We welcome contributions from the community and sincerely appreciate your efforts to improve the project. Whether you're fixing a typo, reporting a bug, proposing a new feature, or implementing a major enhancement, your work helps make Data Designer better for everyone \ud83c\udf89.</p> <p>This guide will help you get started with the contribution process.</p>"},{"location":"CONTRIBUTING/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Getting Started</li> <li>Ways to Contribute</li> <li>Feature Requests</li> <li>Development Guide</li> <li>Submitting Changes</li> <li>Code of Conduct</li> <li>Signing off on your work</li> </ul>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>\ud83d\udc4b Welcome to the Data Designer community! We're excited to have you here.</p> <p>Whether you're new to the project or ready to dive in, the resources below will help you get oriented and productive quickly:</p> <ol> <li> <p>README.md \u2013\u00a0best place to start to learn the basics of the project</p> </li> <li> <p>AGENTS.md\u00a0\u2013 context and instructions to help AI coding agents work on Data Designer (it's also useful for human developers!)</p> </li> <li> <p>Documentation \u2013\u00a0detailed documentation on Data Designer's capabilities and usage</p> </li> </ol>"},{"location":"CONTRIBUTING/#ways-to-contribute","title":"Ways to Contribute","text":"<p>There are many ways to contribute to Data Designer:</p>"},{"location":"CONTRIBUTING/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<p>Found a bug? Before reporting, please 1. Verify you're using the latest version: <code>uv pip install --upgrade data-designer</code> 2. Search for duplicates in the issue tracker</p> <p>When creating a bug report, please include: - Data Designer version - Python version and operating system - Minimal reproducible example - Expected vs. actual behavior - Full error messages and stack traces</p> <p>If you are interested in fixing the bug yourself, that's AWESOME! Please follow the development guide to get started.</p>"},{"location":"CONTRIBUTING/#feature-implementation","title":"\u2728 Feature Implementation","text":"<p>Want to add new functionality? Great! Please review our development approach and open a feature request to discuss the idea and get feedback before investing significant time on the implementation.</p>"},{"location":"CONTRIBUTING/#documentation-improvements","title":"\ud83d\udcd6 Documentation Improvements","text":"<p>Documentation is crucial for user adoption. Contributions that clarify usage, add examples, or fix typos are highly valued.</p>"},{"location":"CONTRIBUTING/#examples-and-tutorials","title":"\ud83d\udca1 Examples and Tutorials","text":"<p>Share your use cases! Example notebooks and tutorials help others understand how to leverage Data Designer effectively.</p>"},{"location":"CONTRIBUTING/#test-coverage","title":"\ud83e\uddea Test Coverage","text":"<p>Help us improve test coverage by adding tests for untested code paths or edge cases.</p>"},{"location":"CONTRIBUTING/#feature-requests","title":"Feature Requests","text":"<p>Data Designer is designed to be as flexible and extensible as possible, and we welcome your ideas for pushing its capabilities even further! To keep the core library maintainable, while also supporting innovation, we take an incremental approach when adding new features \u2013 we explore what's already possible, extend through plugins when needed, and integrate the most broadly useful features into the core library:</p>"},{"location":"CONTRIBUTING/#how-we-grow-data-designer","title":"How We Grow Data Designer","text":"<ol> <li> <p>\ud83e\uddd7 Explore what's possible: Can your use case be achieved with current features? We've designed Data Designer to be composable \u2013 sometimes creative combinations of existing tools can accomplish what you need. Check out our examples or open an issue if you'd like help exploring this!</p> </li> <li> <p>\ud83d\udd0c Extend through plugins: If existing features aren't quite enough, consider implementing your idea as a plugin that extends the core library. Plugins let you experiment and share functionality while keeping the core library focused.</p> </li> <li> <p>\u2699\ufe0f Integrate into the core library: If your feature or plugin proves broadly useful and aligns with Data Designer's goals, we'd love to integrate it into the core library! We're happy to discuss whether it's a good fit and how to move forward together.</p> </li> </ol> <p>This approach helps us grow thoughtfully while keeping Data Designer focused and maintainable.</p>"},{"location":"CONTRIBUTING/#submitting-a-feature-request","title":"Submitting a Feature Request","text":"<p>Open a new issue with:</p> <ul> <li>Clear title: Concise description of the feature</li> <li>Use case: Explain what problem this solves and why it's important</li> <li>Proposed solution: Describe how you envision the feature working</li> <li>Alternatives considered: Other approaches you've thought about</li> <li>Examples: Code examples or mockups of how users would interact with the feature</li> <li>Willingness to implement: Are you interested in implementing this yourself?</li> </ul>"},{"location":"CONTRIBUTING/#development-guide","title":"Development Guide","text":"<p>Data Designer uses <code>uv</code> for dependency management. If you don't have uv installed, follow their installation instructions.</p>"},{"location":"CONTRIBUTING/#initial-setup","title":"Initial Setup","text":"<ol> <li> <p>Create or find an issue</p> <p>Before starting work, ensure there's an issue tracking your contribution:</p> <ul> <li>For bug fixes: Search existing issues or create a new one</li> <li>For new features: Open a feature request to discuss the approach first</li> <li>Comment on the issue to let maintainers know you're working on it</li> </ul> </li> <li> <p>Fork and clone the repository</p> <p>Start by forking the Data Designer repository, then clone your fork and add the upstream remote:</p> <pre><code>git clone https://github.com/YOUR_GITHUB_USERNAME/DataDesigner.git\n\ncd DataDesigner\n\ngit remote add upstream https://github.com/NVIDIA-NeMo/DataDesigner.git\n</code></pre> </li> <li> <p>Install dependencies</p> <pre><code># Install project with dev dependencies\nmake install-dev\n\n# Or, if you use Jupyter / IPython for development\nmake install-dev-notebooks\n</code></pre> </li> <li> <p>Verify your setup</p> <pre><code>make test &amp;&amp; make check-all\n</code></pre> <p>If no errors are reported, you're ready to develop \ud83d\ude80</p> </li> </ol>"},{"location":"CONTRIBUTING/#making-changes","title":"Making Changes","text":"<ol> <li> <p>Create a feature branch</p> <pre><code>git checkout main\ngit pull upstream main\ngit checkout -b &lt;username&gt;/&lt;type-of-change&gt;/&lt;issue-number&gt;-&lt;short-description&gt;\n</code></pre> <p>Example types of change:</p> <ul> <li><code>feat</code> for new features</li> <li><code>fix</code> for bug fixes</li> <li><code>docs</code> for documentation updates</li> <li><code>test</code> for testing changes</li> <li><code>refactor</code> for code refactoring</li> <li><code>chore</code> for chore tasks</li> <li><code>style</code> for style changes</li> <li><code>perf</code> for performance improvements</li> </ul> <p>Example branch name:</p> <ul> <li><code>johnnygreco/feat/123-add-xyz-generator</code> for a new feature by @johnnygreco, addressing issue #123</li> </ul> </li> <li> <p>Develop your changes</p> <p>Please follow the patterns and conventions used throughout the codebase, as well as those outlined in AGENTS.md.</p> </li> <li> <p>Test and validate</p> <pre><code>make check-all-fix  # Format code and fix linting issues\nmake test           # Run all tests\nmake coverage       # Check test coverage (must be &gt;90%)\n</code></pre> <p>Writing tests: Place tests in tests/ mirroring the source structure. Use fixtures from tests/conftest.py, mock external services with <code>unittest.mock</code> or <code>pytest-httpx</code>, and test both success and failure cases. See AGENTS.md for patterns and examples.</p> </li> <li> <p>Commit your work</p> <p>Write clear, descriptive commit messages, optionally including a brief summary (50 characters or less) and reference issue numbers when applicable (e.g., \"Fixes #123\").</p> <pre><code>git commit -m \"Add XYZ generator for synthetic data\" -m \"Fixes #123\"\n</code></pre> </li> <li> <p>Stay up to date</p> <p>Regularly sync your branch with upstream changes:</p> <pre><code>git fetch upstream\ngit merge upstream/main\n</code></pre> </li> </ol>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting Changes","text":""},{"location":"CONTRIBUTING/#before-submitting","title":"Before Submitting","text":"<p>Ensure your changes meet the following criteria:</p> <ul> <li>All tests pass (<code>make test</code>)</li> <li>Code is formatted and linted (<code>make check-all-fix</code>)</li> <li>New functionality includes tests</li> <li>Documentation is updated (README, docstrings, examples)</li> <li>License headers are present on all new files</li> <li>Commit messages are clear and descriptive</li> </ul>"},{"location":"CONTRIBUTING/#creating-a-pull-request","title":"Creating a Pull Request","text":"<ol> <li> <p>Push your changes to your fork:</p> <pre><code>git push origin &lt;username&gt;/&lt;type-of-change&gt;/&lt;issue-number&gt;-&lt;short-description&gt;\n</code></pre> </li> <li> <p>Open a pull request on GitHub from your fork to the main repository</p> </li> <li> <p>Respond to review feedback update your PR as needed</p> </li> </ol>"},{"location":"CONTRIBUTING/#pull-request-review-process","title":"Pull Request Review Process","text":"<ul> <li>Maintainers will review your PR and may request changes</li> <li>Address feedback by pushing additional commits to your branch</li> <li>Reply to the feedback comment with a link to the commit that addresses it.</li> <li>Once approved, a maintainer will merge your PR</li> <li>Your contribution will be included in the next release!</li> </ul>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>Data Designer follows the Contributor Covenant Code of Conduct. We are committed to providing a welcoming and inclusive environment for all contributors.</p> <p>Please read our complete Code of Conduct for full details on our standards and expectations.</p>"},{"location":"CONTRIBUTING/#license-file-headers","title":"License File Headers","text":"<p>All code files that are added to this repository must include the appropriate NVIDIA copyright header:</p> <pre><code># SPDX-FileCopyrightText: Copyright (c) {YEAR} NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n</code></pre> <p>Use <code>make update-license-headers</code> to add headers automatically.</p>"},{"location":"CONTRIBUTING/#signing-off-on-your-work","title":"Signing off on your work","text":"<p>When contributing to this project, you must agree that you have authored 100% of the content, that you have the necessary rights to the content and that the content you contribute may be provided under the project license. All contributors are asked to sign the Data Designer Developer Certificate of Origin (DCO) when submitting their first pull request. The process is automated by a bot that will comment on the pull request. Our DCO is the same as the Linux Foundation requires its contributors to sign.</p> <p>Thank you for contributing to NeMo Data Designer! Your efforts help make synthetic data generation more accessible and powerful for everyone. \ud83c\udfa8\u2728</p>"},{"location":"installation/","title":"Installation","text":"<p>Installing Data Designer is as simple as:</p> pipuv <pre><code>pip install data-designer\n</code></pre> <pre><code>uv add data-designer\n</code></pre>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>To install the latest development version from the GitHub repository:</p> pipuv <pre><code>pip install 'git+https://github.com/NVIDIA-NeMo/DataDesigner@main'\n</code></pre> <pre><code>uv add 'git+https://github.com/NVIDIA-NeMo/DataDesigner@main'\n</code></pre>"},{"location":"quick-start/","title":"Quick Start","text":"<p>Get started with Data Designer using the default model providers and configurations. Data Designer ships with built-in model providers and configurations that make it easy to start generating synthetic data immediately.</p>"},{"location":"quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, you'll need an API key from one of the default providers:</p> <ul> <li>NVIDIA API Key: Get yours from build.nvidia.com</li> <li>OpenAI API Key (optional): Get yours from platform.openai.com</li> <li>OpenRouter API Key (optional): Get yours from openrouter.ai</li> </ul> <p>Set your API key as an environment variable:</p> <pre><code>export NVIDIA_API_KEY=\"your-api-key-here\"\n# Or for OpenAI\nexport OPENAI_API_KEY=\"your-openai-api-key-here\"\n# Or for OpenRouter\nexport OPENROUTER_API_KEY=\"your-openrouter-api-key-here\"\n</code></pre>"},{"location":"quick-start/#example","title":"Example","text":"<p>Below we'll construct a simple Data Designer workflow that generates multilingual greetings.</p> <pre><code>import os\n\nfrom data_designer.essentials import (\n    CategorySamplerParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    InfoType,\n    LLMTextColumnConfig,\n    SamplerColumnConfig,\n    SamplerType,\n)\n\n# Set your API key from build.nvidia.com\n# Skip this step if you've already exported your key to the environemnt variable\nos.environ[\"NVIDIA_API_KEY\"] = \"your-api-key-here\"\n\n# Create a DataDesigner instance\n# This automatically configures the default model providers\ndata_designer = DataDesigner()\n\n# Print out all the model providers available\ndata_designer.info.display(InfoType.MODEL_PROVIDERS)\n\n# Create a config builder\n# This automatically loads the default model configurations\nconfig_builder = DataDesignerConfigBuilder()\n\n# Print out all the model configurations available\nconfig_builder.info.display(InfoType.MODEL_CONFIGS)\n\n# Add a sampler column to randomly select a language\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"language\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\"English\", \"Spanish\", \"French\", \"German\", \"Italian\"],\n        ),\n    )\n)\n\n# Add an LLM text generation column\n# We'll use the built-in 'nvidia-text' model alias\nconfig_builder.add_column(\n    LLMTextColumnConfig(\n        name=\"greetings\",\n        model_alias=\"nvidia-text\",\n        prompt=\"\"\"Write a casual and formal greeting in '{{language}}' language.\"\"\",\n    )\n)\n\n# Run a preview to generate sample records\npreview_results = data_designer.preview(config_builder=config_builder)\n\n# Display a sample record\npreview_results.display_sample_record()\n</code></pre> <p>\ud83c\udf89 Congratulations, you successfully ran one iteration designing your synthetic data. Follow along to learn more.</p> <p>To learn more about the default providers and model configurations available, see the Default Model Settings guide.</p>"},{"location":"code_reference/analysis/","title":"Analysis","text":"<p>The <code>analysis</code> modules provide tools for profiling and analyzing generated datasets. It includes statistics tracking, column profiling, and reporting capabilities.</p>"},{"location":"code_reference/analysis/#column-statistics","title":"Column Statistics","text":"<p>Column statistics are automatically computed for every column after generation. They provide basic metrics specific to the column type. For example, LLM columns track token usage statistics, sampler columns track distribution information, and validation columns track validation success rates.</p> <p>The classes below are result objects that store the computed statistics for each column type and provide methods for formatting these results for display in reports.</p> <p>Classes:</p> Name Description <code>BaseColumnStatistics</code> <p>Abstract base class for all column statistics types.</p> <code>CategoricalDistribution</code> <p>Container for computed categorical distribution statistics.</p> <code>CategoricalHistogramData</code> <p>Container for categorical distribution histogram data.</p> <code>ExpressionColumnStatistics</code> <p>Container for statistics on expression-based derived columns.</p> <code>GeneralColumnStatistics</code> <p>Container for general statistics applicable to all column types.</p> <code>LLMCodeColumnStatistics</code> <p>Container for statistics on LLM-generated code columns.</p> <code>LLMJudgedColumnStatistics</code> <p>Container for statistics on LLM-as-a-judge quality assessment columns.</p> <code>LLMStructuredColumnStatistics</code> <p>Container for statistics on LLM-generated structured JSON columns.</p> <code>LLMTextColumnStatistics</code> <p>Container for statistics on LLM-generated text columns.</p> <code>NumericalDistribution</code> <p>Container for computed numerical distribution statistics.</p> <code>SamplerColumnStatistics</code> <p>Container for statistics on sampler-generated columns.</p> <code>SeedDatasetColumnStatistics</code> <p>Container for statistics on columns sourced from seed datasets.</p> <code>ValidationColumnStatistics</code> <p>Container for statistics on validation result columns.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.BaseColumnStatistics","title":"<code>BaseColumnStatistics</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract base class for all column statistics types.</p> <p>Serves as a container for computed statistics across different column types in Data-Designer-generated datasets. Subclasses hold column-specific statistical results and provide methods for formatting these results for display in reports.</p> <p>Methods:</p> Name Description <code>create_report_row_data</code> <p>Creates a formatted dictionary of statistics for display in reports.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.BaseColumnStatistics.create_report_row_data","title":"<code>create_report_row_data()</code>  <code>abstractmethod</code>","text":"<p>Creates a formatted dictionary of statistics for display in reports.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary mapping display labels to formatted statistic values.</p> Source code in <code>src/data_designer/config/analysis/column_statistics.py</code> <pre><code>@abstractmethod\ndef create_report_row_data(self) -&gt; dict[str, str]:\n    \"\"\"Creates a formatted dictionary of statistics for display in reports.\n\n    Returns:\n        Dictionary mapping display labels to formatted statistic values.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.CategoricalDistribution","title":"<code>CategoricalDistribution</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for computed categorical distribution statistics.</p> <p>Attributes:</p> Name Type Description <code>most_common_value</code> <code>str | int</code> <p>The category value that appears most frequently in the data.</p> <code>least_common_value</code> <code>str | int</code> <p>The category value that appears least frequently in the data.</p> <code>histogram</code> <code>CategoricalHistogramData</code> <p>Complete frequency distribution showing all categories and their counts.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.CategoricalHistogramData","title":"<code>CategoricalHistogramData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for categorical distribution histogram data.</p> <p>Stores the computed frequency distribution of categorical values.</p> <p>Attributes:</p> Name Type Description <code>categories</code> <code>list[float | int | str]</code> <p>List of unique category values that appear in the data.</p> <code>counts</code> <code>list[int]</code> <p>List of occurrence counts for each category.</p> <p>Methods:</p> Name Description <code>ensure_python_types</code> <p>Ensure numerical values are Python objects rather than Numpy types.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.CategoricalHistogramData.ensure_python_types","title":"<code>ensure_python_types()</code>","text":"<p>Ensure numerical values are Python objects rather than Numpy types.</p> Source code in <code>src/data_designer/config/analysis/column_statistics.py</code> <pre><code>@model_validator(mode=\"after\")\ndef ensure_python_types(self) -&gt; Self:\n    \"\"\"Ensure numerical values are Python objects rather than Numpy types.\"\"\"\n    self.categories = [(float(x) if is_float(x) else (int(x) if is_int(x) else str(x))) for x in self.categories]\n    self.counts = [int(i) for i in self.counts]\n    return self\n</code></pre>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.ExpressionColumnStatistics","title":"<code>ExpressionColumnStatistics</code>","text":"<p>               Bases: <code>GeneralColumnStatistics</code></p> <p>Container for statistics on expression-based derived columns.</p> <p>Inherits general statistics and stores statistics computed from columns that are derived from columns that are derived from Jinja2 expressions referencing other column values.</p> <p>Attributes:</p> Name Type Description <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"expression\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.GeneralColumnStatistics","title":"<code>GeneralColumnStatistics</code>","text":"<p>               Bases: <code>BaseColumnStatistics</code></p> <p>Container for general statistics applicable to all column types.</p> <p>Holds core statistical measures that apply universally across all column types, including null counts, unique values, and data type information. Serves as the base for more specialized column statistics classes that store additional column-specific metrics.</p> <p>Attributes:</p> Name Type Description <code>column_name</code> <code>str</code> <p>Name of the column being analyzed.</p> <code>num_records</code> <code>int | MissingValue</code> <p>Total number of records in the column.</p> <code>num_null</code> <code>int | MissingValue</code> <p>Number of null/missing values in the column.</p> <code>num_unique</code> <code>int | MissingValue</code> <p>Number of distinct values in the column. If a value is not hashable, it is converted to a string.</p> <code>pyarrow_dtype</code> <code>str</code> <p>PyArrow data type of the column as a string.</p> <code>simple_dtype</code> <code>str</code> <p>Simplified human-readable data type label.</p> <code>column_type</code> <code>Literal['general']</code> <p>Discriminator field, always \"general\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.LLMCodeColumnStatistics","title":"<code>LLMCodeColumnStatistics</code>","text":"<p>               Bases: <code>LLMTextColumnStatistics</code></p> <p>Container for statistics on LLM-generated code columns.</p> <p>Inherits all token usage metrics from LLMTextColumnStatistics. Stores statistics from columns that generate code snippets in specific programming languages.</p> <p>Attributes:</p> Name Type Description <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"llm-code\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.LLMJudgedColumnStatistics","title":"<code>LLMJudgedColumnStatistics</code>","text":"<p>               Bases: <code>LLMTextColumnStatistics</code></p> <p>Container for statistics on LLM-as-a-judge quality assessment columns.</p> <p>Inherits all token usage metrics from LLMTextColumnStatistics. Stores statistics from columns that evaluate and score other generated content based on defined criteria.</p> <p>Attributes:</p> Name Type Description <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"llm-judge\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.LLMStructuredColumnStatistics","title":"<code>LLMStructuredColumnStatistics</code>","text":"<p>               Bases: <code>LLMTextColumnStatistics</code></p> <p>Container for statistics on LLM-generated structured JSON columns.</p> <p>Inherits all token usage metrics from LLMTextColumnStatistics. Stores statistics from columns that generate structured data conforming to JSON schemas or Pydantic models.</p> <p>Attributes:</p> Name Type Description <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"llm-structured\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.LLMTextColumnStatistics","title":"<code>LLMTextColumnStatistics</code>","text":"<p>               Bases: <code>GeneralColumnStatistics</code></p> <p>Container for statistics on LLM-generated text columns.</p> <p>Inherits general statistics plus token usage metrics specific to LLM text generation. Stores both prompt and completion token consumption data.</p> <p>Attributes:</p> Name Type Description <code>output_tokens_mean</code> <code>float | MissingValue</code> <p>Mean number of output tokens generated per record.</p> <code>output_tokens_median</code> <code>float | MissingValue</code> <p>Median number of output tokens generated per record.</p> <code>output_tokens_stddev</code> <code>float | MissingValue</code> <p>Standard deviation of output tokens per record.</p> <code>input_tokens_mean</code> <code>float | MissingValue</code> <p>Mean number of input tokens used per record.</p> <code>input_tokens_median</code> <code>float | MissingValue</code> <p>Median number of input tokens used per record.</p> <code>input_tokens_stddev</code> <code>float | MissingValue</code> <p>Standard deviation of input tokens per record.</p> <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"llm-text\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.NumericalDistribution","title":"<code>NumericalDistribution</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for computed numerical distribution statistics.</p> <p>Attributes:</p> Name Type Description <code>min</code> <code>float | int</code> <p>Minimum value in the distribution.</p> <code>max</code> <code>float | int</code> <p>Maximum value in the distribution.</p> <code>mean</code> <code>float</code> <p>Arithmetic mean (average) of all values.</p> <code>stddev</code> <code>float</code> <p>Standard deviation measuring the spread of values around the mean.</p> <code>median</code> <code>float</code> <p>Median value of the distribution.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.SamplerColumnStatistics","title":"<code>SamplerColumnStatistics</code>","text":"<p>               Bases: <code>GeneralColumnStatistics</code></p> <p>Container for statistics on sampler-generated columns.</p> <p>Inherits general statistics plus sampler-specific information including the sampler type used and the empirical distribution of generated values. Stores both categorical and numerical distribution results.</p> <p>Attributes:</p> Name Type Description <code>sampler_type</code> <code>SamplerType</code> <p>Type of sampler used to generate this column (e.g., \"uniform\", \"category\", \"gaussian\", \"person\").</p> <code>distribution_type</code> <code>ColumnDistributionType</code> <p>Classification of the column's distribution (categorical, numerical, text, other, or unknown).</p> <code>distribution</code> <code>CategoricalDistribution | NumericalDistribution | MissingValue | None</code> <p>Empirical distribution statistics for the generated values. Can be CategoricalDistribution (for discrete values), NumericalDistribution (for continuous values), or MissingValue if distribution could not be computed.</p> <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"sampler\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.SeedDatasetColumnStatistics","title":"<code>SeedDatasetColumnStatistics</code>","text":"<p>               Bases: <code>GeneralColumnStatistics</code></p> <p>Container for statistics on columns sourced from seed datasets.</p> <p>Inherits general statistics and stores statistics computed from columns that originate from existing data provided via the seed dataset functionality.</p> <p>Attributes:</p> Name Type Description <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"seed-dataset\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.ValidationColumnStatistics","title":"<code>ValidationColumnStatistics</code>","text":"<p>               Bases: <code>GeneralColumnStatistics</code></p> <p>Container for statistics on validation result columns.</p> <p>Inherits general statistics plus validation-specific metrics including the count and percentage of records that passed validation. Stores results from validation logic (Python, SQL, or remote) executed against target columns.</p> <p>Attributes:</p> Name Type Description <code>num_valid_records</code> <code>int | MissingValue</code> <p>Number of records that passed validation.</p> <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"validation\" for this statistics type.</p>"},{"location":"code_reference/analysis/#column-profilers","title":"Column Profilers","text":"<p>Column profilers are optional analysis tools that provide deeper insights into specific column types. Currently, the only column profiler available is the Judge Score Profiler.</p> <p>The classes below are result objects that store the computed profiler results and provide methods for formatting these results for display in reports.</p> <p>Classes:</p> Name Description <code>ColumnProfilerResults</code> <p>Abstract base class for column profiler results.</p> <code>JudgeScoreDistributions</code> <p>Container for computed distributions across all judge score dimensions.</p> <code>JudgeScoreProfilerConfig</code> <p>Configuration for the LLM-as-a-judge score profiler.</p> <code>JudgeScoreProfilerResults</code> <p>Container for complete judge score profiler analysis results.</p> <code>JudgeScoreSample</code> <p>Container for a single judge score and its associated reasoning.</p> <code>JudgeScoreSummary</code> <p>Container for an LLM-generated summary of a judge score dimension.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_profilers.ColumnProfilerResults","title":"<code>ColumnProfilerResults</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract base class for column profiler results.</p> <p>Stores results from column profiling operations. Subclasses hold profiler-specific analysis results and provide methods for generating formatted report sections for display.</p> <p>Methods:</p> Name Description <code>create_report_section</code> <p>Creates a Rich Panel containing the formatted profiler results for display.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_profilers.ColumnProfilerResults.create_report_section","title":"<code>create_report_section()</code>","text":"<p>Creates a Rich Panel containing the formatted profiler results for display.</p> <p>Returns:</p> Type Description <code>Panel</code> <p>A Rich Panel containing the formatted profiler results. Default implementation</p> <code>Panel</code> <p>returns a \"Not Implemented\" message; subclasses should override to provide</p> <code>Panel</code> <p>specific formatting.</p> Source code in <code>src/data_designer/config/analysis/column_profilers.py</code> <pre><code>def create_report_section(self) -&gt; Panel:\n    \"\"\"Creates a Rich Panel containing the formatted profiler results for display.\n\n    Returns:\n        A Rich Panel containing the formatted profiler results. Default implementation\n        returns a \"Not Implemented\" message; subclasses should override to provide\n        specific formatting.\n    \"\"\"\n    return Panel(\n        f\"Report section generation not implemented for '{self.__class__.__name__}'.\",\n        title=\"Not Implemented\",\n        border_style=f\"bold {ColorPalette.YELLOW.value}\",\n        padding=(1, 2),\n    )\n</code></pre>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_profilers.JudgeScoreDistributions","title":"<code>JudgeScoreDistributions</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for computed distributions across all judge score dimensions.</p> <p>Stores the complete distribution analysis for all score dimensions in an LLM-as-a-judge column. Each score dimension (e.g., \"relevance\", \"fluency\") has its own distribution computed from the generated data.</p> <p>Attributes:</p> Name Type Description <code>scores</code> <code>dict[str, list[int | str]]</code> <p>Mapping of each score dimension name to its list of score values.</p> <code>reasoning</code> <code>dict[str, list[str]]</code> <p>Mapping of each score dimension name to its list of reasoning texts.</p> <code>distribution_types</code> <code>dict[str, ColumnDistributionType]</code> <p>Mapping of each score dimension name to its classification.</p> <code>distributions</code> <code>dict[str, CategoricalDistribution | NumericalDistribution | MissingValue]</code> <p>Mapping of each score dimension name to its computed distribution statistics.</p> <code>histograms</code> <code>dict[str, CategoricalHistogramData | MissingValue]</code> <p>Mapping of each score dimension name to its histogram data.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_profilers.JudgeScoreProfilerConfig","title":"<code>JudgeScoreProfilerConfig</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for the LLM-as-a-judge score profiler.</p> <p>Attributes:</p> Name Type Description <code>model_alias</code> <code>str</code> <p>Alias of the LLM model to use for generating score distribution summaries. Must match a model alias defined in the Data Designer configuration.</p> <code>summary_score_sample_size</code> <code>int | None</code> <p>Number of score samples to include when prompting the LLM to generate summaries. Larger sample sizes provide more context but increase token usage. Must be at least 1. Defaults to 20.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_profilers.JudgeScoreProfilerResults","title":"<code>JudgeScoreProfilerResults</code>","text":"<p>               Bases: <code>ColumnProfilerResults</code></p> <p>Container for complete judge score profiler analysis results.</p> <p>Attributes:</p> Name Type Description <code>column_name</code> <code>str</code> <p>Name of the judge column that was profiled.</p> <code>summaries</code> <code>dict[str, JudgeScoreSummary]</code> <p>Mapping of each score dimension name to its LLM-generated summary.</p> <code>score_distributions</code> <code>JudgeScoreDistributions | MissingValue</code> <p>Complete distribution analysis across all score dimensions.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_profilers.JudgeScoreSample","title":"<code>JudgeScoreSample</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for a single judge score and its associated reasoning.</p> <p>Stores a paired score-reasoning sample extracted from an LLM-as-a-judge column. Used when generating summaries to provide the LLM with examples of scoring patterns.</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>int | str</code> <p>The score value assigned by the judge. Can be numeric (int) or categorical (str).</p> <code>reasoning</code> <code>str</code> <p>The reasoning or explanation provided by the judge for this score.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_profilers.JudgeScoreSummary","title":"<code>JudgeScoreSummary</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for an LLM-generated summary of a judge score dimension.</p> <p>Stores the natural language summary and sample data for a single score dimension generated by the judge score profiler. The summary is created by an LLM analyzing the distribution and patterns in the score-reasoning pairs.</p> <p>Attributes:</p> Name Type Description <code>score_name</code> <code>str</code> <p>Name of the score dimension being summarized (e.g., \"relevance\", \"fluency\").</p> <code>summary</code> <code>str</code> <p>LLM-generated natural language summary describing the scoring patterns, distribution characteristics, and notable trends for this score dimension.</p> <code>score_samples</code> <code>list[JudgeScoreSample]</code> <p>List of score-reasoning pairs that were used to generate the summary. These are the examples of the scoring behavior that were used to generate the summary.</p>"},{"location":"code_reference/analysis/#dataset-profiler","title":"Dataset Profiler","text":"<p>The DatasetProfilerResults class contains complete profiling results for a generated dataset. It aggregates column-level statistics, metadata, and profiler results, and provides methods to:</p> <ul> <li>Compute dataset-level metrics (completion percentage, column type summary)</li> <li>Filter statistics by column type</li> <li>Generate formatted analysis reports via the <code>to_report()</code> method</li> </ul> <p>Reports can be displayed in the console or exported to HTML/SVG formats.</p> <p>Classes:</p> Name Description <code>DatasetProfilerResults</code> <p>Container for complete dataset profiling and analysis results.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.dataset_profiler.DatasetProfilerResults","title":"<code>DatasetProfilerResults</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for complete dataset profiling and analysis results.</p> <p>Stores profiling results for a generated dataset, including statistics for all columns, dataset-level metadata, and optional advanced profiler results. Provides methods for computing derived metrics and generating formatted reports.</p> <p>Attributes:</p> Name Type Description <code>num_records</code> <code>int</code> <p>Actual number of records successfully generated in the dataset.</p> <code>target_num_records</code> <code>int</code> <p>Target number of records that were requested to be generated.</p> <code>column_statistics</code> <code>list[Annotated[ColumnStatisticsT, Field(discriminator=column_type)]]</code> <p>List of statistics objects for all columns in the dataset. Each column has statistics appropriate to its type. Must contain at least one column.</p> <code>side_effect_column_names</code> <code>list[str] | None</code> <p>Column names that were generated as side effects of other columns.</p> <code>column_profiles</code> <code>list[ColumnProfilerResultsT] | None</code> <p>Column profiler results for specific columns when configured.</p> <p>Methods:</p> Name Description <code>get_column_statistics_by_type</code> <p>Filters column statistics to return only those of the specified type.</p> <code>to_report</code> <p>Generate and print an analysis report based on the dataset profiling results.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.dataset_profiler.DatasetProfilerResults.column_types","title":"<code>column_types</code>  <code>cached</code> <code>property</code>","text":"<p>Returns a sorted list of unique column types present in the dataset.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.dataset_profiler.DatasetProfilerResults.percent_complete","title":"<code>percent_complete</code>  <code>property</code>","text":"<p>Returns the completion percentage of the dataset.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.dataset_profiler.DatasetProfilerResults.get_column_statistics_by_type","title":"<code>get_column_statistics_by_type(column_type)</code>","text":"<p>Filters column statistics to return only those of the specified type.</p> Source code in <code>src/data_designer/config/analysis/dataset_profiler.py</code> <pre><code>def get_column_statistics_by_type(self, column_type: DataDesignerColumnType) -&gt; list[ColumnStatisticsT]:\n    \"\"\"Filters column statistics to return only those of the specified type.\"\"\"\n    return [c for c in self.column_statistics if c.column_type == column_type]\n</code></pre>"},{"location":"code_reference/analysis/#data_designer.config.analysis.dataset_profiler.DatasetProfilerResults.to_report","title":"<code>to_report(save_path=None, include_sections=None)</code>","text":"<p>Generate and print an analysis report based on the dataset profiling results.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str | Path | None</code> <p>Optional path to save the report. If provided, the report will be saved   as either HTML (.html) or SVG (.svg) format. If None, the report will   only be displayed in the console.</p> <code>None</code> <code>include_sections</code> <code>list[ReportSection | DataDesignerColumnType] | None</code> <p>Optional list of sections to include in the report. Choices are   any DataDesignerColumnType, \"overview\" (the dataset overview section),   and \"column_profilers\" (all column profilers in one section). If None,   all sections will be included.</p> <code>None</code> Source code in <code>src/data_designer/config/analysis/dataset_profiler.py</code> <pre><code>def to_report(\n    self,\n    save_path: str | Path | None = None,\n    include_sections: list[ReportSection | DataDesignerColumnType] | None = None,\n) -&gt; None:\n    \"\"\"Generate and print an analysis report based on the dataset profiling results.\n\n    Args:\n        save_path: Optional path to save the report. If provided, the report will be saved\n              as either HTML (.html) or SVG (.svg) format. If None, the report will\n              only be displayed in the console.\n        include_sections: Optional list of sections to include in the report. Choices are\n              any DataDesignerColumnType, \"overview\" (the dataset overview section),\n              and \"column_profilers\" (all column profilers in one section). If None,\n              all sections will be included.\n    \"\"\"\n    generate_analysis_report(self, save_path, include_sections=include_sections)\n</code></pre>"},{"location":"code_reference/column_configs/","title":"Column Configurations","text":"<p>The <code>column_configs</code> module defines configuration objects for all Data Designer column types. Each configuration inherits from SingleColumnConfig, which provides shared arguments like the column <code>name</code>, whether to <code>drop</code> the column after generation, and the <code>column_type</code>.</p> <p><code>column_type</code> is a discriminator field</p> <p>The <code>column_type</code> argument is used to identify column types when deserializing the Data Designer Config from JSON/YAML. It acts as the discriminator in a discriminated union, allowing Pydantic to automatically determine which column configuration class to instantiate.</p> <p>Classes:</p> Name Description <code>EmbeddingColumnConfig</code> <p>Configuration for embedding generation columns.</p> <code>ExpressionColumnConfig</code> <p>Configuration for derived columns using Jinja2 expressions.</p> <code>LLMCodeColumnConfig</code> <p>Configuration for code generation columns using Large Language Models.</p> <code>LLMJudgeColumnConfig</code> <p>Configuration for LLM-as-a-judge quality assessment and scoring columns.</p> <code>LLMStructuredColumnConfig</code> <p>Configuration for structured JSON generation columns using Large Language Models.</p> <code>LLMTextColumnConfig</code> <p>Configuration for text generation columns using Large Language Models.</p> <code>SamplerColumnConfig</code> <p>Configuration for columns generated using numerical samplers.</p> <code>Score</code> <p>Configuration for a \"score\" in an LLM judge evaluation.</p> <code>SeedDatasetColumnConfig</code> <p>Configuration for columns sourced from seed datasets.</p> <code>SingleColumnConfig</code> <p>Abstract base class for all single-column configuration types.</p> <code>ValidationColumnConfig</code> <p>Configuration for validation columns that validate existing columns.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.EmbeddingColumnConfig","title":"<code>EmbeddingColumnConfig</code>","text":"<p>               Bases: <code>SingleColumnConfig</code></p> <p>Configuration for embedding generation columns.</p> <p>Embedding columns generate embeddings for text input using a specified model.</p> <p>Attributes:</p> Name Type Description <code>target_column</code> <code>str</code> <p>The column to generate embeddings for. The column could be a single text string or a list of text strings in stringified JSON format. If it is a list of text strings in stringified JSON format, the embeddings will be generated for each text string.</p> <code>model_alias</code> <code>str</code> <p>The model to use for embedding generation.</p> <code>column_type</code> <code>Literal['embedding']</code> <p>Discriminator field, always \"embedding\" for this configuration type.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.ExpressionColumnConfig","title":"<code>ExpressionColumnConfig</code>","text":"<p>               Bases: <code>SingleColumnConfig</code></p> <p>Configuration for derived columns using Jinja2 expressions.</p> <p>Expression columns compute values by evaluating Jinja2 templates that reference other columns. Useful for transformations, concatenations, conditional logic, and derived features without requiring LLM generation. The expression is evaluated row-by-row.</p> <p>Attributes:</p> Name Type Description <code>expr</code> <code>str</code> <p>Jinja2 expression to evaluate. Can reference other column values using {{ column_name }} syntax. Supports filters, conditionals, and arithmetic. Must be a valid, non-empty Jinja2 template.</p> <code>dtype</code> <code>Literal['int', 'float', 'str', 'bool']</code> <p>Data type to cast the result to. Must be one of \"int\", \"float\", \"str\", or \"bool\". Defaults to \"str\". Type conversion is applied after expression evaluation.</p> <code>column_type</code> <code>Literal['expression']</code> <p>Discriminator field, always \"expression\" for this configuration type.</p> <p>Methods:</p> Name Description <code>assert_expression_valid_jinja</code> <p>Validate that the expression is a valid, non-empty Jinja2 template.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.ExpressionColumnConfig.required_columns","title":"<code>required_columns</code>  <code>property</code>","text":"<p>Returns the columns referenced in the expression template.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.ExpressionColumnConfig.assert_expression_valid_jinja","title":"<code>assert_expression_valid_jinja()</code>","text":"<p>Validate that the expression is a valid, non-empty Jinja2 template.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validated instance.</p> <p>Raises:</p> Type Description <code>InvalidConfigError</code> <p>If expression is empty or contains invalid Jinja2 syntax.</p> Source code in <code>src/data_designer/config/column_configs.py</code> <pre><code>@model_validator(mode=\"after\")\ndef assert_expression_valid_jinja(self) -&gt; Self:\n    \"\"\"Validate that the expression is a valid, non-empty Jinja2 template.\n\n    Returns:\n        The validated instance.\n\n    Raises:\n        InvalidConfigError: If expression is empty or contains invalid Jinja2 syntax.\n    \"\"\"\n    if not self.expr.strip():\n        raise InvalidConfigError(\n            f\"\ud83d\uded1 Expression column '{self.name}' has an empty or whitespace-only expression. \"\n            f\"Please provide a valid Jinja2 expression (e.g., '{{ column_name }}' or '{{ col1 }} + {{ col2 }}') \"\n            \"or remove this column if not needed.\"\n        )\n    assert_valid_jinja2_template(self.expr)\n    return self\n</code></pre>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMCodeColumnConfig","title":"<code>LLMCodeColumnConfig</code>","text":"<p>               Bases: <code>LLMTextColumnConfig</code></p> <p>Configuration for code generation columns using Large Language Models.</p> <p>Extends LLMTextColumnConfig to generate code snippets in specific programming languages or SQL dialects. The generated code is automatically extracted from markdown code blocks for the specified language. Inherits all prompt templating capabilities.</p> <p>Attributes:</p> Name Type Description <code>code_lang</code> <code>CodeLang</code> <p>Programming language or SQL dialect for code generation. Supported values include: \"python\", \"javascript\", \"typescript\", \"java\", \"kotlin\", \"go\", \"rust\", \"ruby\", \"scala\", \"swift\", \"sql:sqlite\", \"sql:postgres\", \"sql:mysql\", \"sql:tsql\", \"sql:bigquery\", \"sql:ansi\". See CodeLang enum for complete list.</p> <code>column_type</code> <code>Literal['llm-code']</code> <p>Discriminator field, always \"llm-code\" for this configuration type.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMJudgeColumnConfig","title":"<code>LLMJudgeColumnConfig</code>","text":"<p>               Bases: <code>LLMTextColumnConfig</code></p> <p>Configuration for LLM-as-a-judge quality assessment and scoring columns.</p> <p>Extends LLMTextColumnConfig to create judge columns that evaluate and score other generated content based on the defined criteria. Useful for quality assessment, preference ranking, and multi-dimensional evaluation of generated data.</p> <p>Attributes:</p> Name Type Description <code>scores</code> <code>list[Score]</code> <p>List of Score objects defining the evaluation dimensions. Each score represents a different aspect to evaluate (e.g., accuracy, relevance, fluency). Must contain at least one score.</p> <code>column_type</code> <code>Literal['llm-judge']</code> <p>Discriminator field, always \"llm-judge\" for this configuration type.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMStructuredColumnConfig","title":"<code>LLMStructuredColumnConfig</code>","text":"<p>               Bases: <code>LLMTextColumnConfig</code></p> <p>Configuration for structured JSON generation columns using Large Language Models.</p> <p>Extends LLMTextColumnConfig to generate structured data conforming to a specified schema. Uses JSON schema or Pydantic models to define the expected output structure, enabling type-safe and validated structured output generation. Inherits prompt templating capabilities.</p> <p>Attributes:</p> Name Type Description <code>output_format</code> <code>dict | type[BaseModel]</code> <p>The schema defining the expected output structure. Can be either: - A Pydantic BaseModel class (recommended) - A JSON schema dictionary</p> <code>column_type</code> <code>Literal['llm-structured']</code> <p>Discriminator field, always \"llm-structured\" for this configuration type.</p> <p>Methods:</p> Name Description <code>validate_output_format</code> <p>Convert Pydantic model to JSON schema if needed.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMStructuredColumnConfig.validate_output_format","title":"<code>validate_output_format()</code>","text":"<p>Convert Pydantic model to JSON schema if needed.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validated instance with output_format as a JSON schema dict.</p> Source code in <code>src/data_designer/config/column_configs.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_output_format(self) -&gt; Self:\n    \"\"\"Convert Pydantic model to JSON schema if needed.\n\n    Returns:\n        The validated instance with output_format as a JSON schema dict.\n    \"\"\"\n    if not isinstance(self.output_format, dict) and issubclass(self.output_format, BaseModel):\n        self.output_format = self.output_format.model_json_schema()\n    return self\n</code></pre>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMTextColumnConfig","title":"<code>LLMTextColumnConfig</code>","text":"<p>               Bases: <code>SingleColumnConfig</code></p> <p>Configuration for text generation columns using Large Language Models.</p> <p>LLM text columns generate free-form text content using language models via LiteLLM. Prompts support Jinja2 templating to reference values from other columns, enabling context-aware generation. The generated text can optionally include reasoning traces when models support extended thinking.</p> <p>Attributes:</p> Name Type Description <code>prompt</code> <code>str</code> <p>Prompt template for text generation. Supports Jinja2 syntax to reference other columns (e.g., \"Write a story about {{ character_name }}\"). Must be a valid Jinja2 template.</p> <code>model_alias</code> <code>str</code> <p>Alias of the model configuration to use for generation. Must match a model alias defined when initializing the DataDesignerConfigBuilder.</p> <code>system_prompt</code> <code>str | None</code> <p>Optional system prompt to set model behavior and constraints. Also supports Jinja2 templating. If provided, must be a valid Jinja2 template. Do not put any output parsing instructions in the system prompt. Instead, use the appropriate column type for the output you want to generate - e.g., <code>LLMStructuredColumnConfig</code> for structured output, <code>LLMCodeColumnConfig</code> for code.</p> <code>multi_modal_context</code> <code>list[ImageContext] | None</code> <p>Optional list of image contexts for multi-modal generation. Enables vision-capable models to generate text based on image inputs.</p> <code>column_type</code> <code>Literal['llm-text']</code> <p>Discriminator field, always \"llm-text\" for this configuration type.</p> <p>Methods:</p> Name Description <code>assert_prompt_valid_jinja</code> <p>Validate that prompt and system_prompt are valid Jinja2 templates.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMTextColumnConfig.required_columns","title":"<code>required_columns</code>  <code>property</code>","text":"<p>Get columns referenced in the prompt and system_prompt templates.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of unique column names referenced in Jinja2 templates.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMTextColumnConfig.side_effect_columns","title":"<code>side_effect_columns</code>  <code>property</code>","text":"<p>Returns the reasoning trace column, which may be generated alongside the main column.</p> <p>Reasoning traces are only returned if the served model parses and returns reasoning content.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List containing the reasoning trace column name.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMTextColumnConfig.assert_prompt_valid_jinja","title":"<code>assert_prompt_valid_jinja()</code>","text":"<p>Validate that prompt and system_prompt are valid Jinja2 templates.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validated instance.</p> <p>Raises:</p> Type Description <code>InvalidConfigError</code> <p>If prompt or system_prompt contains invalid Jinja2 syntax.</p> Source code in <code>src/data_designer/config/column_configs.py</code> <pre><code>@model_validator(mode=\"after\")\ndef assert_prompt_valid_jinja(self) -&gt; Self:\n    \"\"\"Validate that prompt and system_prompt are valid Jinja2 templates.\n\n    Returns:\n        The validated instance.\n\n    Raises:\n        InvalidConfigError: If prompt or system_prompt contains invalid Jinja2 syntax.\n    \"\"\"\n    assert_valid_jinja2_template(self.prompt)\n    if self.system_prompt:\n        assert_valid_jinja2_template(self.system_prompt)\n    return self\n</code></pre>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.SamplerColumnConfig","title":"<code>SamplerColumnConfig</code>","text":"<p>               Bases: <code>SingleColumnConfig</code></p> <p>Configuration for columns generated using numerical samplers.</p> <p>Sampler columns provide efficient data generation using numerical samplers for common data types and distributions. Supported samplers include UUID generation, datetime/timedelta sampling, person generation, category / subcategory sampling, and various statistical distributions (uniform, gaussian, binomial, poisson, scipy).</p> <p>Attributes:</p> Name Type Description <code>sampler_type</code> <code>SamplerType</code> <p>Type of sampler to use. Available types include: \"uuid\", \"category\", \"subcategory\", \"uniform\", \"gaussian\", \"bernoulli\", \"bernoulli_mixture\", \"binomial\", \"poisson\", \"scipy\", \"person\", \"datetime\", \"timedelta\".</p> <code>params</code> <code>Annotated[SamplerParamsT, Discriminator(sampler_type)]</code> <p>Parameters specific to the chosen sampler type. Type varies based on the <code>sampler_type</code> (e.g., <code>CategorySamplerParams</code>, <code>UniformSamplerParams</code>, <code>PersonSamplerParams</code>).</p> <code>conditional_params</code> <code>dict[str, Annotated[SamplerParamsT, Discriminator(sampler_type)]]</code> <p>Optional dictionary for conditional parameters. The dict keys are the conditions that must be met (e.g., \"age &gt; 21\") for the conditional parameters to be used. The values of dict are the parameters to use when the condition is met.</p> <code>convert_to</code> <code>str | None</code> <p>Optional type conversion to apply after sampling. Must be one of \"float\", \"int\", or \"str\". Useful for converting numerical samples to strings or other types.</p> <code>column_type</code> <code>Literal['sampler']</code> <p>Discriminator field, always \"sampler\" for this configuration type.</p> <p>Displaying available samplers and their parameters</p> <p>The config builder has an <code>info</code> attribute that can be used to display the available samplers and their parameters: <pre><code>config_builder.info.display(\"samplers\")\n</code></pre></p> <p>Methods:</p> Name Description <code>inject_sampler_type_into_params</code> <p>Inject sampler_type into params dict to enable discriminated union resolution.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.SamplerColumnConfig.inject_sampler_type_into_params","title":"<code>inject_sampler_type_into_params(data)</code>  <code>classmethod</code>","text":"<p>Inject sampler_type into params dict to enable discriminated union resolution.</p> <p>This allows users to pass params as a simple dict without the sampler_type field, which will be automatically added based on the outer sampler_type field.</p> Source code in <code>src/data_designer/config/column_configs.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef inject_sampler_type_into_params(cls, data: dict) -&gt; dict:\n    \"\"\"Inject sampler_type into params dict to enable discriminated union resolution.\n\n    This allows users to pass params as a simple dict without the sampler_type field,\n    which will be automatically added based on the outer sampler_type field.\n    \"\"\"\n    if isinstance(data, dict):\n        sampler_type = data.get(\"sampler_type\")\n        params = data.get(\"params\")\n\n        # If params is a dict and doesn't have sampler_type, inject it\n        if sampler_type and isinstance(params, dict) and \"sampler_type\" not in params:\n            data[\"params\"] = {\"sampler_type\": sampler_type, **params}\n\n        # Handle conditional_params similarly\n        conditional_params = data.get(\"conditional_params\")\n        if conditional_params and isinstance(conditional_params, dict):\n            for condition, cond_params in conditional_params.items():\n                if isinstance(cond_params, dict) and \"sampler_type\" not in cond_params:\n                    data[\"conditional_params\"][condition] = {\"sampler_type\": sampler_type, **cond_params}\n\n    return data\n</code></pre>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.Score","title":"<code>Score</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for a \"score\" in an LLM judge evaluation.</p> <p>Defines a single scoring criterion with its possible values and descriptions. Multiple Score objects can be combined in an LLMJudgeColumnConfig to create multi-dimensional quality assessments.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>A clear, concise name for this scoring dimension (e.g., \"Relevance\", \"Fluency\").</p> <code>description</code> <code>str</code> <p>An informative and detailed assessment guide explaining how to evaluate this dimension. Should provide clear criteria for scoring.</p> <code>options</code> <code>dict[int | str, str]</code> <p>Dictionary mapping score values to their descriptions. Keys can be integers (e.g., 1-5 scale) or strings (e.g., \"Poor\", \"Good\", \"Excellent\"). Values are descriptions explaining what each score level means.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.SeedDatasetColumnConfig","title":"<code>SeedDatasetColumnConfig</code>","text":"<p>               Bases: <code>SingleColumnConfig</code></p> <p>Configuration for columns sourced from seed datasets.</p> <p>This config marks columns that come from seed data. It is typically created automatically when calling <code>with_seed_dataset()</code> on the builder, rather than being instantiated directly by users.</p> <p>Attributes:</p> Name Type Description <code>column_type</code> <code>Literal['seed-dataset']</code> <p>Discriminator field, always \"seed-dataset\" for this configuration type.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.SingleColumnConfig","title":"<code>SingleColumnConfig</code>","text":"<p>               Bases: <code>ConfigBase</code>, <code>ABC</code></p> <p>Abstract base class for all single-column configuration types.</p> <p>This class serves as the foundation for all column configurations in DataDesigner, defining shared fields and properties across all column types.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique name of the column to be generated.</p> <code>drop</code> <code>bool</code> <p>If True, the column will be generated but removed from the final dataset. Useful for intermediate columns that are dependencies for other columns.</p> <code>column_type</code> <code>str</code> <p>Discriminator field that identifies the specific column type. Subclasses must override this field to specify the column type with a <code>Literal</code> value.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.SingleColumnConfig.required_columns","title":"<code>required_columns</code>  <code>property</code>","text":"<p>Returns a list of column names that must exist before this column can be generated.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of column names that this column depends on. Empty list indicates</p> <code>list[str]</code> <p>no dependencies. Override in subclasses to specify dependencies.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.SingleColumnConfig.side_effect_columns","title":"<code>side_effect_columns</code>  <code>property</code>","text":"<p>Returns a list of additional columns that this column will create as a side effect.</p> <p>Some column types generate additional metadata or auxiliary columns alongside the primary column (e.g., reasoning traces for LLM columns).</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of column names that this column will create as a side effect. Empty list</p> <code>list[str]</code> <p>indicates no side effect columns. Override in subclasses to specify side effects.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.ValidationColumnConfig","title":"<code>ValidationColumnConfig</code>","text":"<p>               Bases: <code>SingleColumnConfig</code></p> <p>Configuration for validation columns that validate existing columns.</p> <p>Validation columns execute validation logic against specified target columns and return structured results indicating pass/fail status with validation details. Supports multiple validation strategies: code execution (Python/SQL), local callable functions (library only), and remote HTTP endpoints.</p> <p>Attributes:</p> Name Type Description <code>target_columns</code> <code>list[str]</code> <p>List of column names to validate. These columns are passed to the validator for validation. All target columns must exist in the dataset before validation runs.</p> <code>validator_type</code> <code>ValidatorType</code> <p>The type of validator to use. Options: - \"code\": Execute code (Python or SQL) for validation. The code receives a   DataFrame with target columns and must return a DataFrame with validation results. - \"local_callable\": Call a local Python function with the data. Only supported   when running DataDesigner locally. - \"remote\": Send data to a remote HTTP endpoint for validation. Useful for</p> <code>validator_params</code> <code>ValidatorParamsT</code> <p>Parameters specific to the validator type. Type varies by validator: - CodeValidatorParams: Specifies code language (python or SQL dialect like   \"sql:postgres\", \"sql:mysql\"). - LocalCallableValidatorParams: Provides validation function (Callable[[pd.DataFrame],   pd.DataFrame]) and optional output schema for validation results. - RemoteValidatorParams: Configures endpoint URL, HTTP timeout, retry behavior   (max_retries, retry_backoff), and parallel request limits (max_parallel_requests).</p> <code>batch_size</code> <code>int</code> <p>Number of records to process in each validation batch. Defaults to 10. Larger batches are more efficient but use more memory. Adjust based on validator complexity and available resources.</p> <code>column_type</code> <code>Literal['validation']</code> <p>Discriminator field, always \"validation\" for this configuration type.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.ValidationColumnConfig.required_columns","title":"<code>required_columns</code>  <code>property</code>","text":"<p>Returns the columns that need to be validated.</p>"},{"location":"code_reference/config_builder/","title":"Data Designer's Config Builder","text":"<p>The <code>config_builder</code> module provides a high-level interface for constructing Data Designer configurations through the DataDesignerConfigBuilder class, enabling programmatic creation of DataDesignerConfig objects by incrementally adding column configurations, constraints, processors, and profilers.</p> <p>You can use the builder to create Data Designer configurations from scratch or from existing configurations stored in YAML/JSON files via <code>from_config()</code>. The builder includes validation capabilities to catch configuration errors early and can work with seed datasets from local sources or external datastores. Once configured, use <code>build()</code> to generate the final configuration object or <code>write_config()</code> to serialize it to disk.</p> <p>Model configs are required</p> <p>DataDesignerConfigBuilder requires a list of model configurations at initialization. This tells the builder which model aliases can be referenced by LLM-generated columns (such as <code>LLMTextColumnConfig</code>, <code>LLMCodeColumnConfig</code>, <code>LLMStructuredColumnConfig</code>, and <code>LLMJudgeColumnConfig</code>). Each model configuration specifies the model alias, model provider, model ID, and inference parameters that will be used during data generation.</p> <p>Classes:</p> Name Description <code>BuilderConfig</code> <p>Configuration container for Data Designer builder.</p> <code>DataDesignerConfigBuilder</code> <p>Config builder for Data Designer configurations.</p>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.BuilderConfig","title":"<code>BuilderConfig</code>","text":"<p>               Bases: <code>ExportableConfigBase</code></p> <p>Configuration container for Data Designer builder.</p> <p>This class holds the main Data Designer configuration along with optional datastore settings needed for seed dataset operations.</p> <p>Attributes:</p> Name Type Description <code>data_designer</code> <code>DataDesignerConfig</code> <p>The main Data Designer configuration containing columns, constraints, profilers, and other settings.</p>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder","title":"<code>DataDesignerConfigBuilder(model_configs=None)</code>","text":"<p>Config builder for Data Designer configurations.</p> <p>This class provides a high-level interface for building Data Designer configurations.</p> <p>Initialize a new DataDesignerConfigBuilder instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_configs</code> <code>list[ModelConfig] | str | Path | None</code> <p>Model configurations. Can be: - None to use default model configurations in local mode - A list of ModelConfig objects - A string or Path to a model configuration file</p> <code>None</code> <p>Methods:</p> Name Description <code>add_column</code> <p>Add a Data Designer column configuration to the current Data Designer configuration.</p> <code>add_constraint</code> <p>Add a constraint to the current Data Designer configuration.</p> <code>add_model_config</code> <p>Add a model configuration to the current Data Designer configuration.</p> <code>add_processor</code> <p>Add a processor to the current Data Designer configuration.</p> <code>add_profiler</code> <p>Add a profiler to the current Data Designer configuration.</p> <code>build</code> <p>Build a DataDesignerConfig instance based on the current builder configuration.</p> <code>delete_column</code> <p>Delete the column with the given name.</p> <code>delete_constraints</code> <p>Delete all constraints for the given target column.</p> <code>delete_model_config</code> <p>Delete a model configuration from the current Data Designer configuration by alias.</p> <code>from_config</code> <p>Create a DataDesignerConfigBuilder from an existing configuration.</p> <code>get_builder_config</code> <p>Get the builder config for the current Data Designer configuration.</p> <code>get_column_config</code> <p>Get a column configuration by name.</p> <code>get_column_configs</code> <p>Get all column configurations.</p> <code>get_columns_excluding_type</code> <p>Get all column configurations excluding the specified type.</p> <code>get_columns_of_type</code> <p>Get all column configurations of the specified type.</p> <code>get_constraints</code> <p>Get all constraints for the given target column.</p> <code>get_processor_configs</code> <p>Get processor configuration objects.</p> <code>get_profilers</code> <p>Get all profilers.</p> <code>get_seed_config</code> <p>Get the seed config for the current Data Designer configuration.</p> <code>num_columns_of_type</code> <p>Get the count of columns of the specified type.</p> <code>with_seed_dataset</code> <p>Add a seed dataset to the current Data Designer configuration.</p> <code>write_config</code> <p>Write the current configuration to a file.</p> <p>Attributes:</p> Name Type Description <code>allowed_references</code> <code>list[str]</code> <p>Get all referenceable variables allowed in prompt templates and expressions.</p> <code>info</code> <code>ConfigBuilderInfo</code> <p>Get the ConfigBuilderInfo object for this builder.</p> <code>model_configs</code> <code>list[ModelConfig]</code> <p>Get the model configurations for this builder.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def __init__(self, model_configs: list[ModelConfig] | str | Path | None = None):\n    \"\"\"Initialize a new DataDesignerConfigBuilder instance.\n\n    Args:\n        model_configs: Model configurations. Can be:\n            - None to use default model configurations in local mode\n            - A list of ModelConfig objects\n            - A string or Path to a model configuration file\n    \"\"\"\n    self._column_configs = {}\n    self._model_configs = _load_model_configs(model_configs)\n    self._processor_configs: list[ProcessorConfigT] = []\n    self._seed_config: SeedConfig | None = None\n    self._constraints: list[ColumnConstraintT] = []\n    self._profilers: list[ColumnProfilerConfigT] = []\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.allowed_references","title":"<code>allowed_references</code>  <code>property</code>","text":"<p>Get all referenceable variables allowed in prompt templates and expressions.</p> <p>This includes all column names and their side effect columns that can be referenced in prompt templates and expressions within the configuration.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of variable names that can be referenced in templates and expressions.</p>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.info","title":"<code>info</code>  <code>property</code>","text":"<p>Get the ConfigBuilderInfo object for this builder.</p> <p>Returns:</p> Type Description <code>ConfigBuilderInfo</code> <p>An object containing information about the configuration.</p>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.model_configs","title":"<code>model_configs</code>  <code>property</code>","text":"<p>Get the model configurations for this builder.</p> <p>Returns:</p> Type Description <code>list[ModelConfig]</code> <p>A list of ModelConfig objects used for data generation.</p>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.add_column","title":"<code>add_column(column_config=None, *, name=None, column_type=None, **kwargs)</code>","text":"<p>Add a Data Designer column configuration to the current Data Designer configuration.</p> <p>If no column config object is provided, you must provide the <code>name</code>, <code>column_type</code>, and any additional keyword arguments that are required by the column config constructor.</p> <p>Parameters:</p> Name Type Description Default <code>column_config</code> <code>ColumnConfigT | None</code> <p>Data Designer column config object to add.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Name of the column to add. This is only used if <code>column_config</code> is not provided.</p> <code>None</code> <code>column_type</code> <code>DataDesignerColumnType | None</code> <p>Column type to add. This is only used if <code>column_config</code> is not provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the column constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> <p>Raises:</p> Type Description <code>BuilderConfigurationError</code> <p>If the column name collides with an existing seed dataset column.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def add_column(\n    self,\n    column_config: ColumnConfigT | None = None,\n    *,\n    name: str | None = None,\n    column_type: DataDesignerColumnType | None = None,\n    **kwargs,\n) -&gt; Self:\n    \"\"\"Add a Data Designer column configuration to the current Data Designer configuration.\n\n    If no column config object is provided, you must provide the `name`, `column_type`, and any\n    additional keyword arguments that are required by the column config constructor.\n\n    Args:\n        column_config: Data Designer column config object to add.\n        name: Name of the column to add. This is only used if `column_config` is not provided.\n        column_type: Column type to add. This is only used if `column_config` is not provided.\n        **kwargs: Additional keyword arguments to pass to the column constructor.\n\n    Returns:\n        The current Data Designer config builder instance.\n\n    Raises:\n        BuilderConfigurationError: If the column name collides with an existing seed dataset column.\n    \"\"\"\n    if column_config is None:\n        if name is None or column_type is None:\n            raise BuilderConfigurationError(\n                \"\ud83d\uded1 You must provide either a 'column_config' object or 'name' *and* 'column_type' \"\n                f\"with additional keyword arguments. You provided {column_config=}, {name=}, and {column_type=}.\"\n            )\n        column_config = get_column_config_from_kwargs(name=name, column_type=column_type, **kwargs)\n\n    allowed_column_configs = ColumnConfigT.__args__\n    if not any(isinstance(column_config, t) for t in allowed_column_configs):\n        raise InvalidColumnTypeError(\n            f\"\ud83d\uded1 Invalid column config object: '{column_config}'. Valid column config options are: \"\n            f\"{', '.join([t.__name__ for t in allowed_column_configs])}\"\n        )\n\n    self._column_configs[column_config.name] = column_config\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.add_constraint","title":"<code>add_constraint(constraint=None, *, constraint_type=None, **kwargs)</code>","text":"<p>Add a constraint to the current Data Designer configuration.</p> <p>Currently, constraints are only supported for numerical samplers.</p> <p>You can either provide a constraint object directly, or provide a constraint type and additional keyword arguments to construct the constraint object. Valid constraint types are:     - \"scalar_inequality\": Constraint between a column and a scalar value.     - \"column_inequality\": Constraint between two columns.</p> <p>Parameters:</p> Name Type Description Default <code>constraint</code> <code>ColumnConstraintT | None</code> <p>Constraint object to add.</p> <code>None</code> <code>constraint_type</code> <code>ConstraintType | None</code> <p>Constraint type to add. Ignored when <code>constraint</code> is provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the constraint constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def add_constraint(\n    self,\n    constraint: ColumnConstraintT | None = None,\n    *,\n    constraint_type: ConstraintType | None = None,\n    **kwargs,\n) -&gt; Self:\n    \"\"\"Add a constraint to the current Data Designer configuration.\n\n    Currently, constraints are only supported for numerical samplers.\n\n    You can either provide a constraint object directly, or provide a constraint type and\n    additional keyword arguments to construct the constraint object. Valid constraint types are:\n        - \"scalar_inequality\": Constraint between a column and a scalar value.\n        - \"column_inequality\": Constraint between two columns.\n\n    Args:\n        constraint: Constraint object to add.\n        constraint_type: Constraint type to add. Ignored when `constraint` is provided.\n        **kwargs: Additional keyword arguments to pass to the constraint constructor.\n\n    Returns:\n        The current Data Designer config builder instance.\n    \"\"\"\n    if constraint is None:\n        if constraint_type is None:\n            raise BuilderConfigurationError(\n                \"\ud83d\uded1 You must provide either a 'constraint' object or 'constraint_type' \"\n                \"with additional keyword arguments.\"\n            )\n        try:\n            constraint_type = ConstraintType(constraint_type)\n        except Exception:\n            raise BuilderConfigurationError(\n                f\"\ud83d\uded1 Invalid constraint type: {constraint_type}. Valid options are: \"\n                f\"{', '.join([t.value for t in ConstraintType])}\"\n            )\n        if constraint_type == ConstraintType.SCALAR_INEQUALITY:\n            constraint = ScalarInequalityConstraint(**kwargs)\n        elif constraint_type == ConstraintType.COLUMN_INEQUALITY:\n            constraint = ColumnInequalityConstraint(**kwargs)\n\n    allowed_constraint_types = ColumnConstraintT.__args__\n    if not any(isinstance(constraint, t) for t in allowed_constraint_types):\n        raise BuilderConfigurationError(\n            \"\ud83d\uded1 Invalid constraint object. Valid constraint options are: \"\n            f\"{', '.join([t.__name__ for t in allowed_constraint_types])}\"\n        )\n\n    self._constraints.append(constraint)\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.add_model_config","title":"<code>add_model_config(model_config)</code>","text":"<p>Add a model configuration to the current Data Designer configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>ModelConfig</code> <p>The model configuration to add.</p> required Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def add_model_config(self, model_config: ModelConfig) -&gt; Self:\n    \"\"\"Add a model configuration to the current Data Designer configuration.\n\n    Args:\n        model_config: The model configuration to add.\n    \"\"\"\n    if model_config.alias in [mc.alias for mc in self._model_configs]:\n        raise BuilderConfigurationError(\n            f\"\ud83d\uded1 Model configuration with alias {model_config.alias} already exists. Please delete the existing model configuration or choose a different alias.\"\n        )\n    self._model_configs.append(model_config)\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.add_processor","title":"<code>add_processor(processor_config=None, *, processor_type=None, **kwargs)</code>","text":"<p>Add a processor to the current Data Designer configuration.</p> <p>You can either provide a processor config object directly, or provide a processor type and additional keyword arguments to construct the processor config object.</p> <p>Parameters:</p> Name Type Description Default <code>processor_config</code> <code>ProcessorConfigT | None</code> <p>The processor configuration object to add.</p> <code>None</code> <code>processor_type</code> <code>ProcessorType | None</code> <p>The type of processor to add.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the processor constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def add_processor(\n    self,\n    processor_config: ProcessorConfigT | None = None,\n    *,\n    processor_type: ProcessorType | None = None,\n    **kwargs,\n) -&gt; Self:\n    \"\"\"Add a processor to the current Data Designer configuration.\n\n    You can either provide a processor config object directly, or provide a processor type and\n    additional keyword arguments to construct the processor config object.\n\n    Args:\n        processor_config: The processor configuration object to add.\n        processor_type: The type of processor to add.\n        **kwargs: Additional keyword arguments to pass to the processor constructor.\n\n    Returns:\n        The current Data Designer config builder instance.\n    \"\"\"\n    if processor_config is None:\n        if processor_type is None:\n            raise BuilderConfigurationError(\n                \"\ud83d\uded1 You must provide either a 'processor_config' object or 'processor_type' \"\n                \"with additional keyword arguments.\"\n            )\n        processor_config = get_processor_config_from_kwargs(processor_type=processor_type, **kwargs)\n\n    # Checks elsewhere fail if DropColumnsProcessor drops a column but it is not marked for drop\n    if processor_config.processor_type == ProcessorType.DROP_COLUMNS:\n        for column in processor_config.column_names:\n            if column in self._column_configs:\n                self._column_configs[column].drop = True\n\n    self._processor_configs.append(processor_config)\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.add_profiler","title":"<code>add_profiler(profiler_config)</code>","text":"<p>Add a profiler to the current Data Designer configuration.</p> <p>Parameters:</p> Name Type Description Default <code>profiler_config</code> <code>ColumnProfilerConfigT</code> <p>The profiler configuration object to add.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> <p>Raises:</p> Type Description <code>BuilderConfigurationError</code> <p>If the profiler configuration is of an invalid type.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def add_profiler(self, profiler_config: ColumnProfilerConfigT) -&gt; Self:\n    \"\"\"Add a profiler to the current Data Designer configuration.\n\n    Args:\n        profiler_config: The profiler configuration object to add.\n\n    Returns:\n        The current Data Designer config builder instance.\n\n    Raises:\n        BuilderConfigurationError: If the profiler configuration is of an invalid type.\n    \"\"\"\n    if not isinstance(profiler_config, ColumnProfilerConfigT):\n        if hasattr(ColumnProfilerConfigT, \"__args__\"):\n            valid_options = \", \".join([t.__name__ for t in ColumnProfilerConfigT.__args__])\n        else:\n            valid_options = ColumnProfilerConfigT.__name__\n        raise BuilderConfigurationError(f\"\ud83d\uded1 Invalid profiler object. Valid profiler options are: {valid_options}\")\n    self._profilers.append(profiler_config)\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.build","title":"<code>build()</code>","text":"<p>Build a DataDesignerConfig instance based on the current builder configuration.</p> <p>Returns:</p> Type Description <code>DataDesignerConfig</code> <p>The current Data Designer config object.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def build(self) -&gt; DataDesignerConfig:\n    \"\"\"Build a DataDesignerConfig instance based on the current builder configuration.\n\n    Returns:\n        The current Data Designer config object.\n    \"\"\"\n    return DataDesignerConfig(\n        model_configs=self._model_configs,\n        seed_config=self._seed_config,\n        columns=list(self._column_configs.values()),\n        constraints=self._constraints or None,\n        profilers=self._profilers or None,\n        processors=self._processor_configs or None,\n    )\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.delete_column","title":"<code>delete_column(column_name)</code>","text":"<p>Delete the column with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>Name of the column to delete.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> <p>Raises:</p> Type Description <code>BuilderConfigurationError</code> <p>If trying to delete a seed dataset column.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def delete_column(self, column_name: str) -&gt; Self:\n    \"\"\"Delete the column with the given name.\n\n    Args:\n        column_name: Name of the column to delete.\n\n    Returns:\n        The current Data Designer config builder instance.\n\n    Raises:\n        BuilderConfigurationError: If trying to delete a seed dataset column.\n    \"\"\"\n    if isinstance(self._column_configs.get(column_name), SeedDatasetColumnConfig):\n        raise BuilderConfigurationError(\"Seed columns cannot be deleted. Please update the seed dataset instead.\")\n    self._column_configs.pop(column_name, None)\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.delete_constraints","title":"<code>delete_constraints(target_column)</code>","text":"<p>Delete all constraints for the given target column.</p> <p>Parameters:</p> Name Type Description Default <code>target_column</code> <code>str</code> <p>Name of the column to remove constraints for.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def delete_constraints(self, target_column: str) -&gt; Self:\n    \"\"\"Delete all constraints for the given target column.\n\n    Args:\n        target_column: Name of the column to remove constraints for.\n\n    Returns:\n        The current Data Designer config builder instance.\n    \"\"\"\n    self._constraints = [c for c in self._constraints if c.target_column != target_column]\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.delete_model_config","title":"<code>delete_model_config(alias)</code>","text":"<p>Delete a model configuration from the current Data Designer configuration by alias.</p> <p>Parameters:</p> Name Type Description Default <code>alias</code> <code>str</code> <p>The alias of the model configuration to delete.</p> required Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def delete_model_config(self, alias: str) -&gt; Self:\n    \"\"\"Delete a model configuration from the current Data Designer configuration by alias.\n\n    Args:\n        alias: The alias of the model configuration to delete.\n    \"\"\"\n    self._model_configs = [mc for mc in self._model_configs if mc.alias != alias]\n    if len(self._model_configs) == 0:\n        logger.warning(\n            f\"\u26a0\ufe0f No model configurations found after deleting model configuration with alias {alias}. Please add a model configuration before building the configuration.\"\n        )\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create a DataDesignerConfigBuilder from an existing configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict | str | Path | BuilderConfig</code> <p>Configuration source. Can be: - A dictionary containing the configuration - A string or Path to a YAML/JSON configuration file - A BuilderConfig object</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A new instance populated with the configuration from the provided source.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the config format is invalid.</p> <code>ValidationError</code> <p>If the builder config loaded from the config is invalid.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>@classmethod\ndef from_config(cls, config: dict | str | Path | BuilderConfig) -&gt; Self:\n    \"\"\"Create a DataDesignerConfigBuilder from an existing configuration.\n\n    Args:\n        config: Configuration source. Can be:\n            - A dictionary containing the configuration\n            - A string or Path to a YAML/JSON configuration file\n            - A BuilderConfig object\n\n    Returns:\n        A new instance populated with the configuration from the provided source.\n\n    Raises:\n        ValueError: If the config format is invalid.\n        ValidationError: If the builder config loaded from the config is invalid.\n    \"\"\"\n    if isinstance(config, BuilderConfig):\n        builder_config = config\n    else:\n        json_config = json.loads(serialize_data(smart_load_yaml(config)))\n        builder_config = BuilderConfig.model_validate(json_config)\n\n    builder = cls(model_configs=builder_config.data_designer.model_configs)\n    data_designer_config = builder_config.data_designer\n\n    for col in data_designer_config.columns:\n        builder.add_column(col)\n\n    for constraint in data_designer_config.constraints or []:\n        builder.add_constraint(constraint=constraint)\n\n    if (seed_config := data_designer_config.seed_config) is not None:\n        builder.with_seed_dataset(\n            seed_config.source,\n            sampling_strategy=seed_config.sampling_strategy,\n            selection_strategy=seed_config.selection_strategy,\n        )\n\n    return builder\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_builder_config","title":"<code>get_builder_config()</code>","text":"<p>Get the builder config for the current Data Designer configuration.</p> <p>Returns:</p> Type Description <code>BuilderConfig</code> <p>The builder config.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def get_builder_config(self) -&gt; BuilderConfig:\n    \"\"\"Get the builder config for the current Data Designer configuration.\n\n    Returns:\n        The builder config.\n    \"\"\"\n    return BuilderConfig(data_designer=self.build())\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_column_config","title":"<code>get_column_config(name)</code>","text":"<p>Get a column configuration by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the column to retrieve the config for.</p> required <p>Returns:</p> Type Description <code>ColumnConfigT</code> <p>The column configuration object.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no column with the given name exists.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def get_column_config(self, name: str) -&gt; ColumnConfigT:\n    \"\"\"Get a column configuration by name.\n\n    Args:\n        name: Name of the column to retrieve the config for.\n\n    Returns:\n        The column configuration object.\n\n    Raises:\n        KeyError: If no column with the given name exists.\n    \"\"\"\n    return self._column_configs[name]\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_column_configs","title":"<code>get_column_configs()</code>","text":"<p>Get all column configurations.</p> <p>Returns:</p> Type Description <code>list[ColumnConfigT]</code> <p>A list of all column configuration objects.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def get_column_configs(self) -&gt; list[ColumnConfigT]:\n    \"\"\"Get all column configurations.\n\n    Returns:\n        A list of all column configuration objects.\n    \"\"\"\n    return list(self._column_configs.values())\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_columns_excluding_type","title":"<code>get_columns_excluding_type(column_type)</code>","text":"<p>Get all column configurations excluding the specified type.</p> <p>Parameters:</p> Name Type Description Default <code>column_type</code> <code>DataDesignerColumnType</code> <p>The type of columns to exclude.</p> required <p>Returns:</p> Type Description <code>list[ColumnConfigT]</code> <p>A list of column configurations that do not match the specified type.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def get_columns_excluding_type(self, column_type: DataDesignerColumnType) -&gt; list[ColumnConfigT]:\n    \"\"\"Get all column configurations excluding the specified type.\n\n    Args:\n        column_type: The type of columns to exclude.\n\n    Returns:\n        A list of column configurations that do not match the specified type.\n    \"\"\"\n    column_type = resolve_string_enum(column_type, DataDesignerColumnType)\n    return [c for c in self._column_configs.values() if c.column_type != column_type]\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_columns_of_type","title":"<code>get_columns_of_type(column_type)</code>","text":"<p>Get all column configurations of the specified type.</p> <p>Parameters:</p> Name Type Description Default <code>column_type</code> <code>DataDesignerColumnType</code> <p>The type of columns to filter by.</p> required <p>Returns:</p> Type Description <code>list[ColumnConfigT]</code> <p>A list of column configurations matching the specified type.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def get_columns_of_type(self, column_type: DataDesignerColumnType) -&gt; list[ColumnConfigT]:\n    \"\"\"Get all column configurations of the specified type.\n\n    Args:\n        column_type: The type of columns to filter by.\n\n    Returns:\n        A list of column configurations matching the specified type.\n    \"\"\"\n    column_type = resolve_string_enum(column_type, DataDesignerColumnType)\n    return [c for c in self._column_configs.values() if c.column_type == column_type]\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_constraints","title":"<code>get_constraints(target_column)</code>","text":"<p>Get all constraints for the given target column.</p> <p>Parameters:</p> Name Type Description Default <code>target_column</code> <code>str</code> <p>Name of the column to get constraints for.</p> required <p>Returns:</p> Type Description <code>list[ColumnConstraintT]</code> <p>A list of constraint objects targeting the specified column.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def get_constraints(self, target_column: str) -&gt; list[ColumnConstraintT]:\n    \"\"\"Get all constraints for the given target column.\n\n    Args:\n        target_column: Name of the column to get constraints for.\n\n    Returns:\n        A list of constraint objects targeting the specified column.\n    \"\"\"\n    return [c for c in self._constraints if c.target_column == target_column]\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_processor_configs","title":"<code>get_processor_configs()</code>","text":"<p>Get processor configuration objects.</p> <p>Returns:</p> Type Description <code>dict[BuildStage, list[ProcessorConfigT]]</code> <p>A dictionary of processor configuration objects by dataset builder stage.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def get_processor_configs(self) -&gt; dict[BuildStage, list[ProcessorConfigT]]:\n    \"\"\"Get processor configuration objects.\n\n    Returns:\n        A dictionary of processor configuration objects by dataset builder stage.\n    \"\"\"\n    return self._processor_configs\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_profilers","title":"<code>get_profilers()</code>","text":"<p>Get all profilers.</p> <p>Returns:</p> Type Description <code>list[ColumnProfilerConfigT]</code> <p>A list of profiler configuration objects.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def get_profilers(self) -&gt; list[ColumnProfilerConfigT]:\n    \"\"\"Get all profilers.\n\n    Returns:\n        A list of profiler configuration objects.\n    \"\"\"\n    return self._profilers\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_seed_config","title":"<code>get_seed_config()</code>","text":"<p>Get the seed config for the current Data Designer configuration.</p> <p>Returns:</p> Type Description <code>SeedConfig | None</code> <p>The seed config if configured, None otherwise.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def get_seed_config(self) -&gt; SeedConfig | None:\n    \"\"\"Get the seed config for the current Data Designer configuration.\n\n    Returns:\n        The seed config if configured, None otherwise.\n    \"\"\"\n    return self._seed_config\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.num_columns_of_type","title":"<code>num_columns_of_type(column_type)</code>","text":"<p>Get the count of columns of the specified type.</p> <p>Parameters:</p> Name Type Description Default <code>column_type</code> <code>DataDesignerColumnType</code> <p>The type of columns to count.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of columns matching the specified type.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def num_columns_of_type(self, column_type: DataDesignerColumnType) -&gt; int:\n    \"\"\"Get the count of columns of the specified type.\n\n    Args:\n        column_type: The type of columns to count.\n\n    Returns:\n        The number of columns matching the specified type.\n    \"\"\"\n    return len(self.get_columns_of_type(column_type))\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.with_seed_dataset","title":"<code>with_seed_dataset(seed_source, *, sampling_strategy=SamplingStrategy.ORDERED, selection_strategy=None)</code>","text":"<p>Add a seed dataset to the current Data Designer configuration.</p> <p>This method sets the seed dataset for the configuration, but columns are not resolved until compilation (including validation) is performed by the engine using a SeedReader.</p> <p>Parameters:</p> Name Type Description Default <code>seed_source</code> <code>SeedSourceT</code> <p>The pointer to the seed dataset.</p> required <code>sampling_strategy</code> <code>SamplingStrategy</code> <p>The sampling strategy to use when generating data from the seed dataset. Defaults to ORDERED sampling.</p> <code>ORDERED</code> <code>selection_strategy</code> <code>IndexRange | PartitionBlock | None</code> <p>An optional selection strategy to use when generating data from the seed dataset. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def with_seed_dataset(\n    self,\n    seed_source: SeedSourceT,\n    *,\n    sampling_strategy: SamplingStrategy = SamplingStrategy.ORDERED,\n    selection_strategy: IndexRange | PartitionBlock | None = None,\n) -&gt; Self:\n    \"\"\"Add a seed dataset to the current Data Designer configuration.\n\n    This method sets the seed dataset for the configuration, but columns are not resolved until\n    compilation (including validation) is performed by the engine using a SeedReader.\n\n    Args:\n        seed_source: The pointer to the seed dataset.\n        sampling_strategy: The sampling strategy to use when generating data from the seed dataset.\n            Defaults to ORDERED sampling.\n        selection_strategy: An optional selection strategy to use when generating data from the seed dataset.\n            Defaults to None.\n\n    Returns:\n        The current Data Designer config builder instance.\n    \"\"\"\n    self._seed_config = SeedConfig(\n        source=seed_source,\n        sampling_strategy=sampling_strategy,\n        selection_strategy=selection_strategy,\n    )\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.write_config","title":"<code>write_config(path, indent=2, **kwargs)</code>","text":"<p>Write the current configuration to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file to write the configuration to.</p> required <code>indent</code> <code>int | None</code> <p>Indentation level for the output file (default: 2).</p> <code>2</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the serialization methods used.</p> <code>{}</code> <p>Raises:</p> Type Description <code>BuilderConfigurationError</code> <p>If the file format is unsupported.</p> <code>BuilderSerializationError</code> <p>If the configuration cannot be serialized.</p> Source code in <code>src/data_designer/config/config_builder.py</code> <pre><code>def write_config(self, path: str | Path, indent: int | None = 2, **kwargs) -&gt; None:\n    \"\"\"Write the current configuration to a file.\n\n    Args:\n        path: Path to the file to write the configuration to.\n        indent: Indentation level for the output file (default: 2).\n        **kwargs: Additional keyword arguments passed to the serialization methods used.\n\n    Raises:\n        BuilderConfigurationError: If the file format is unsupported.\n        BuilderSerializationError: If the configuration cannot be serialized.\n    \"\"\"\n    if (seed_config := self.get_seed_config()) is not None and isinstance(seed_config.source, DataFrameSeedSource):\n        raise BuilderSerializationError(\n            \"This builder was configured with a DataFrame seed dataset. \"\n            \"DataFrame seeds cannot be serialized to config files. \"\n            \"To serialize this configuration, change your seed dataset to a more persistent, serializable source format. \"\n            \"For example, you could make a local file seed source from the dataframe:\\n\\n\"\n            \"LocalFileSeedSource.from_dataframe(my_dataframe, '/path/to/data.parquet')\"\n        )\n\n    cfg = self.get_builder_config()\n    suffix = Path(path).suffix\n    if suffix in {\".yaml\", \".yml\"}:\n        cfg.to_yaml(path, indent=indent, **kwargs)\n    elif suffix == \".json\":\n        cfg.to_json(path, indent=indent, **kwargs)\n    else:\n        raise BuilderConfigurationError(f\"\ud83d\uded1 Unsupported file type: {suffix}. Must be `.yaml`, `.yml` or `.json`.\")\n</code></pre>"},{"location":"code_reference/data_designer_config/","title":"Data Designer Configuration","text":"<p>DataDesignerConfig is the main configuration object for builder datasets with Data Designer. It is a declarative configuration for defining the dataset you want to generate column-by-column, including options for dataset post-processing, validation, and profiling.</p> <p>Generally, you should use the DataDesignerConfigBuilder to build your configuration, but you can also build it manually by instantiating the DataDesignerConfig class directly.</p> <p>Classes:</p> Name Description <code>DataDesignerConfig</code> <p>Configuration for NeMo Data Designer.</p>"},{"location":"code_reference/data_designer_config/#data_designer.config.data_designer_config.DataDesignerConfig","title":"<code>DataDesignerConfig</code>","text":"<p>               Bases: <code>ExportableConfigBase</code></p> <p>Configuration for NeMo Data Designer.</p> <p>This class defines the main configuration structure for NeMo Data Designer, which orchestrates the generation of synthetic data.</p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>list[Annotated[ColumnConfigT, Field(discriminator='column_type')]]</code> <p>Required list of column configurations defining how each column should be generated. Must contain at least one column.</p> <code>model_configs</code> <code>list[ModelConfig] | None</code> <p>Optional list of model configurations for LLM-based generation. Each model config defines the model, provider, and inference parameters.</p> <code>seed_config</code> <code>SeedConfig | None</code> <p>Optional seed dataset settings to use for generation.</p> <code>constraints</code> <code>list[ColumnConstraintT] | None</code> <p>Optional list of column constraints.</p> <code>profilers</code> <code>list[ColumnProfilerConfigT] | None</code> <p>Optional list of column profilers for analyzing generated data characteristics.</p>"},{"location":"code_reference/models/","title":"Models","text":"<p>The <code>models</code> module defines configuration objects for model-based generation. ModelProvider, specifies connection and authentication details for custom providers. ModelConfig encapsulates model details including the model alias, identifier, and inference parameters. Inference Parameters controls model behavior through settings like <code>temperature</code>, <code>top_p</code>, and <code>max_tokens</code>, with support for both fixed values and distribution-based sampling. The module includes ImageContext for providing image inputs to multimodal models.</p> <p>For more information on how they are used, see below:</p> <ul> <li>Model Providers</li> <li>Model Configs</li> <li>Image Context</li> </ul> <p>Classes:</p> Name Description <code>BaseInferenceParams</code> <p>Base configuration for inference parameters.</p> <code>ChatCompletionInferenceParams</code> <p>Configuration for LLM inference parameters.</p> <code>DistributionType</code> <p>Types of distributions for sampling inference parameters.</p> <code>EmbeddingInferenceParams</code> <p>Configuration for embedding generation parameters.</p> <code>ImageContext</code> <p>Configuration for providing image context to multimodal models.</p> <code>ImageFormat</code> <p>Supported image formats for image modality.</p> <code>ManualDistribution</code> <p>Manual (discrete) distribution for sampling inference parameters.</p> <code>ManualDistributionParams</code> <p>Parameters for manual distribution sampling.</p> <code>Modality</code> <p>Supported modality types for multimodal model data.</p> <code>ModalityDataType</code> <p>Data type formats for multimodal data.</p> <code>ModelConfig</code> <p>Configuration for a model used for generation.</p> <code>ModelProvider</code> <p>Configuration for a custom model provider.</p> <code>UniformDistribution</code> <p>Uniform distribution for sampling inference parameters.</p> <code>UniformDistributionParams</code> <p>Parameters for uniform distribution sampling.</p>"},{"location":"code_reference/models/#data_designer.config.models.BaseInferenceParams","title":"<code>BaseInferenceParams</code>","text":"<p>               Bases: <code>ConfigBase</code>, <code>ABC</code></p> <p>Base configuration for inference parameters.</p> <p>Attributes:</p> Name Type Description <code>generation_type</code> <code>GenerationType</code> <p>Type of generation (chat-completion or embedding). Acts as discriminator.</p> <code>max_parallel_requests</code> <code>int</code> <p>Maximum number of parallel requests to the model API.</p> <code>timeout</code> <code>int | None</code> <p>Timeout in seconds for each request.</p> <code>extra_body</code> <code>dict[str, Any] | None</code> <p>Additional parameters to pass to the model API.</p> <p>Methods:</p> Name Description <code>format_for_display</code> <p>Format inference parameters for display.</p>"},{"location":"code_reference/models/#data_designer.config.models.BaseInferenceParams.generate_kwargs","title":"<code>generate_kwargs</code>  <code>property</code>","text":"<p>Get the generate kwargs for the inference parameters.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary of the generate kwargs.</p>"},{"location":"code_reference/models/#data_designer.config.models.BaseInferenceParams.format_for_display","title":"<code>format_for_display()</code>","text":"<p>Format inference parameters for display.</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted string of inference parameters</p> Source code in <code>src/data_designer/config/models.py</code> <pre><code>def format_for_display(self) -&gt; str:\n    \"\"\"Format inference parameters for display.\n\n    Returns:\n        Formatted string of inference parameters\n    \"\"\"\n    params_dict = self.model_dump(exclude_none=True, mode=\"json\")\n\n    if not params_dict:\n        return \"(none)\"\n\n    parts = []\n    for key, value in params_dict.items():\n        formatted_value = self._format_value(key, value)\n        parts.append(f\"{key}={formatted_value}\")\n    return \", \".join(parts)\n</code></pre>"},{"location":"code_reference/models/#data_designer.config.models.ChatCompletionInferenceParams","title":"<code>ChatCompletionInferenceParams</code>","text":"<p>               Bases: <code>BaseInferenceParams</code></p> <p>Configuration for LLM inference parameters.</p> <p>Attributes:</p> Name Type Description <code>generation_type</code> <code>Literal[CHAT_COMPLETION]</code> <p>Type of generation, always \"chat-completion\" for this class.</p> <code>temperature</code> <code>float | DistributionT | None</code> <p>Sampling temperature (0.0-2.0). Can be a fixed value or a distribution for dynamic sampling.</p> <code>top_p</code> <code>float | DistributionT | None</code> <p>Nucleus sampling probability (0.0-1.0). Can be a fixed value or a distribution for dynamic sampling.</p> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate in the response.</p>"},{"location":"code_reference/models/#data_designer.config.models.DistributionType","title":"<code>DistributionType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of distributions for sampling inference parameters.</p>"},{"location":"code_reference/models/#data_designer.config.models.EmbeddingInferenceParams","title":"<code>EmbeddingInferenceParams</code>","text":"<p>               Bases: <code>BaseInferenceParams</code></p> <p>Configuration for embedding generation parameters.</p> <p>Attributes:</p> Name Type Description <code>generation_type</code> <code>Literal[EMBEDDING]</code> <p>Type of generation, always \"embedding\" for this class.</p> <code>encoding_format</code> <code>Literal['float', 'base64']</code> <p>Format of the embedding encoding (\"float\" or \"base64\").</p> <code>dimensions</code> <code>int | None</code> <p>Number of dimensions for the embedding.</p>"},{"location":"code_reference/models/#data_designer.config.models.ImageContext","title":"<code>ImageContext</code>","text":"<p>               Bases: <code>ModalityContext</code></p> <p>Configuration for providing image context to multimodal models.</p> <p>Attributes:</p> Name Type Description <code>modality</code> <code>Modality</code> <p>The modality type (always \"image\").</p> <code>column_name</code> <code>str</code> <p>Name of the column containing image data.</p> <code>data_type</code> <code>ModalityDataType</code> <p>Format of the image data (\"url\" or \"base64\").</p> <code>image_format</code> <code>ImageFormat | None</code> <p>Image format (required for base64 data).</p> <p>Methods:</p> Name Description <code>get_context</code> <p>Get the context for the image modality.</p>"},{"location":"code_reference/models/#data_designer.config.models.ImageContext.get_context","title":"<code>get_context(record)</code>","text":"<p>Get the context for the image modality.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>dict</code> <p>The record containing the image data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The context for the image modality.</p> Source code in <code>src/data_designer/config/models.py</code> <pre><code>def get_context(self, record: dict) -&gt; dict[str, Any]:\n    \"\"\"Get the context for the image modality.\n\n    Args:\n        record: The record containing the image data.\n\n    Returns:\n        The context for the image modality.\n    \"\"\"\n    context = dict(type=\"image_url\")\n    context_value = record[self.column_name]\n    if self.data_type == ModalityDataType.URL:\n        context[\"image_url\"] = context_value\n    else:\n        context[\"image_url\"] = {\n            \"url\": f\"data:image/{self.image_format.value};base64,{context_value}\",\n            \"format\": self.image_format.value,\n        }\n    return context\n</code></pre>"},{"location":"code_reference/models/#data_designer.config.models.ImageFormat","title":"<code>ImageFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported image formats for image modality.</p>"},{"location":"code_reference/models/#data_designer.config.models.ManualDistribution","title":"<code>ManualDistribution</code>","text":"<p>               Bases: <code>Distribution[ManualDistributionParams]</code></p> <p>Manual (discrete) distribution for sampling inference parameters.</p> <p>Samples from a discrete set of values with optional weights. Useful for testing specific values or creating custom probability distributions for temperature or top_p.</p> <p>Attributes:</p> Name Type Description <code>distribution_type</code> <code>DistributionType | None</code> <p>Type of distribution (\"manual\").</p> <code>params</code> <code>ManualDistributionParams</code> <p>Distribution parameters (values, weights).</p> <p>Methods:</p> Name Description <code>sample</code> <p>Sample a value from the manual distribution.</p>"},{"location":"code_reference/models/#data_designer.config.models.ManualDistribution.sample","title":"<code>sample()</code>","text":"<p>Sample a value from the manual distribution.</p> <p>Returns:</p> Type Description <code>float</code> <p>A float value sampled from the manual distribution.</p> Source code in <code>src/data_designer/config/models.py</code> <pre><code>def sample(self) -&gt; float:\n    \"\"\"Sample a value from the manual distribution.\n\n    Returns:\n        A float value sampled from the manual distribution.\n    \"\"\"\n    return float(np.random.choice(self.params.values, p=self.params.weights))\n</code></pre>"},{"location":"code_reference/models/#data_designer.config.models.ManualDistributionParams","title":"<code>ManualDistributionParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for manual distribution sampling.</p> <p>Attributes:</p> Name Type Description <code>values</code> <code>list[float]</code> <p>List of possible values to sample from.</p> <code>weights</code> <code>list[float] | None</code> <p>Optional list of weights for each value. If not provided, all values have equal probability.</p>"},{"location":"code_reference/models/#data_designer.config.models.Modality","title":"<code>Modality</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported modality types for multimodal model data.</p>"},{"location":"code_reference/models/#data_designer.config.models.ModalityDataType","title":"<code>ModalityDataType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Data type formats for multimodal data.</p>"},{"location":"code_reference/models/#data_designer.config.models.ModelConfig","title":"<code>ModelConfig</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for a model used for generation.</p> <p>Attributes:</p> Name Type Description <code>alias</code> <code>str</code> <p>User-defined alias to reference in column configurations.</p> <code>model</code> <code>str</code> <p>Model identifier (e.g., from build.nvidia.com or other providers).</p> <code>inference_parameters</code> <code>InferenceParamsT</code> <p>Inference parameters for the model (temperature, top_p, max_tokens, etc.). The generation_type is determined by the type of inference_parameters.</p> <code>provider</code> <code>str | None</code> <p>Optional model provider name if using custom providers.</p>"},{"location":"code_reference/models/#data_designer.config.models.ModelConfig.generation_type","title":"<code>generation_type</code>  <code>property</code>","text":"<p>Get the generation type from the inference parameters.</p>"},{"location":"code_reference/models/#data_designer.config.models.ModelProvider","title":"<code>ModelProvider</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for a custom model provider.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the model provider.</p> <code>endpoint</code> <code>str</code> <p>API endpoint URL for the provider.</p> <code>provider_type</code> <code>str</code> <p>Provider type (default: \"openai\"). Determines the API format to use.</p> <code>api_key</code> <code>str | None</code> <p>Optional API key for authentication.</p> <code>extra_body</code> <code>dict[str, Any] | None</code> <p>Additional parameters to pass in API requests.</p> <code>extra_headers</code> <code>dict[str, str] | None</code> <p>Additional headers to pass in API requests.</p>"},{"location":"code_reference/models/#data_designer.config.models.UniformDistribution","title":"<code>UniformDistribution</code>","text":"<p>               Bases: <code>Distribution[UniformDistributionParams]</code></p> <p>Uniform distribution for sampling inference parameters.</p> <p>Samples values uniformly between low and high bounds. Useful for exploring a continuous range of values for temperature or top_p.</p> <p>Attributes:</p> Name Type Description <code>distribution_type</code> <code>DistributionType | None</code> <p>Type of distribution (\"uniform\").</p> <code>params</code> <code>UniformDistributionParams</code> <p>Distribution parameters (low, high).</p> <p>Methods:</p> Name Description <code>sample</code> <p>Sample a value from the uniform distribution.</p>"},{"location":"code_reference/models/#data_designer.config.models.UniformDistribution.sample","title":"<code>sample()</code>","text":"<p>Sample a value from the uniform distribution.</p> <p>Returns:</p> Type Description <code>float</code> <p>A float value sampled from the uniform distribution.</p> Source code in <code>src/data_designer/config/models.py</code> <pre><code>def sample(self) -&gt; float:\n    \"\"\"Sample a value from the uniform distribution.\n\n    Returns:\n        A float value sampled from the uniform distribution.\n    \"\"\"\n    return float(np.random.uniform(low=self.params.low, high=self.params.high, size=1)[0])\n</code></pre>"},{"location":"code_reference/models/#data_designer.config.models.UniformDistributionParams","title":"<code>UniformDistributionParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for uniform distribution sampling.</p> <p>Attributes:</p> Name Type Description <code>low</code> <code>float</code> <p>Lower bound (inclusive).</p> <code>high</code> <code>float</code> <p>Upper bound (exclusive).</p>"},{"location":"code_reference/processors/","title":"Processors","text":"<p>The <code>processors</code> module defines configuration objects for post-generation data transformations. Processors run after column generation and can modify the dataset schema or content before output.</p> <p>Classes:</p> Name Description <code>DropColumnsProcessorConfig</code> <p>Configuration for dropping columns from the output dataset.</p> <code>ProcessorConfig</code> <p>Abstract base class for all processor configuration types.</p> <code>ProcessorType</code> <p>Enumeration of available processor types.</p> <code>SchemaTransformProcessorConfig</code> <p>Configuration for transforming the dataset schema using Jinja2 templates.</p> <p>Functions:</p> Name Description <code>get_processor_config_from_kwargs</code> <p>Create a processor configuration from a processor type and keyword arguments.</p>"},{"location":"code_reference/processors/#data_designer.config.processors.DropColumnsProcessorConfig","title":"<code>DropColumnsProcessorConfig</code>","text":"<p>               Bases: <code>ProcessorConfig</code></p> <p>Configuration for dropping columns from the output dataset.</p> <p>This processor removes specified columns from the generated dataset. The dropped columns are saved separately in a <code>dropped-columns</code> directory for reference. When this processor is added via the config builder, the corresponding column configs are automatically marked with <code>drop = True</code>.</p> <p>Alternatively, you can set <code>drop = True</code> when configuring a column.</p> <p>Attributes:</p> Name Type Description <code>column_names</code> <code>list[str]</code> <p>List of column names to remove from the output dataset.</p> <code>processor_type</code> <code>Literal[DROP_COLUMNS]</code> <p>Discriminator field, always <code>ProcessorType.DROP_COLUMNS</code> for this configuration type.</p>"},{"location":"code_reference/processors/#data_designer.config.processors.ProcessorConfig","title":"<code>ProcessorConfig</code>","text":"<p>               Bases: <code>ConfigBase</code>, <code>ABC</code></p> <p>Abstract base class for all processor configuration types.</p> <p>Processors are transformations that run before or after columns are generated. They can modify, reshape, or augment the dataset before it's saved.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique name of the processor, used to identify the processor in results and to name output artifacts on disk.</p> <code>build_stage</code> <code>BuildStage</code> <p>The stage at which the processor runs. Currently only <code>POST_BATCH</code> is supported, meaning processors run after each batch of columns is generated.</p>"},{"location":"code_reference/processors/#data_designer.config.processors.ProcessorType","title":"<code>ProcessorType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of available processor types.</p> <p>Attributes:</p> Name Type Description <code>DROP_COLUMNS</code> <p>Processor that removes specified columns from the output dataset.</p> <code>SCHEMA_TRANSFORM</code> <p>Processor that creates a new dataset with a transformed schema using Jinja2 templates.</p>"},{"location":"code_reference/processors/#data_designer.config.processors.SchemaTransformProcessorConfig","title":"<code>SchemaTransformProcessorConfig</code>","text":"<p>               Bases: <code>ProcessorConfig</code></p> <p>Configuration for transforming the dataset schema using Jinja2 templates.</p> <p>This processor creates a new dataset with a transformed schema. Each key in the template becomes a column in the output, and values are Jinja2 templates that can reference any column in the batch. The transformed dataset is written to a <code>processors-outputs/{processor_name}/</code> directory alongside the main dataset.</p> <p>Attributes:</p> Name Type Description <code>template</code> <code>dict[str, Any]</code> <p>Dictionary defining the output schema. Keys are new column names, values are Jinja2 templates (strings, lists, or nested structures). Must be JSON-serializable.</p> <code>processor_type</code> <code>Literal[SCHEMA_TRANSFORM]</code> <p>Discriminator field, always <code>ProcessorType.SCHEMA_TRANSFORM</code> for this configuration type.</p>"},{"location":"code_reference/processors/#data_designer.config.processors.get_processor_config_from_kwargs","title":"<code>get_processor_config_from_kwargs(processor_type, **kwargs)</code>","text":"<p>Create a processor configuration from a processor type and keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>processor_type</code> <code>ProcessorType</code> <p>The type of processor to create.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the processor constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ProcessorConfig</code> <p>A processor configuration object of the specified type.</p> Source code in <code>src/data_designer/config/processors.py</code> <pre><code>def get_processor_config_from_kwargs(processor_type: ProcessorType, **kwargs: Any) -&gt; ProcessorConfig:\n    \"\"\"Create a processor configuration from a processor type and keyword arguments.\n\n    Args:\n        processor_type: The type of processor to create.\n        **kwargs: Additional keyword arguments passed to the processor constructor.\n\n    Returns:\n        A processor configuration object of the specified type.\n    \"\"\"\n    if processor_type == ProcessorType.DROP_COLUMNS:\n        return DropColumnsProcessorConfig(**kwargs)\n    elif processor_type == ProcessorType.SCHEMA_TRANSFORM:\n        return SchemaTransformProcessorConfig(**kwargs)\n</code></pre>"},{"location":"code_reference/sampler_params/","title":"Sampler Parameters","text":"<p>The <code>sampler_params</code> module defines parameter configuration objects for all Data Designer sampler types. Sampler parameters are used within the SamplerColumnConfig to specify how values should be generated for sampled columns.</p> <p>Displaying available samplers and their parameters</p> <p>The config builder has an <code>info</code> attribute that can be used to display the available sampler types and their parameters: <pre><code>config_builder.info.display(\"samplers\")\n</code></pre></p> <p>Classes:</p> Name Description <code>BernoulliMixtureSamplerParams</code> <p>Parameters for sampling from a Bernoulli mixture distribution.</p> <code>BernoulliSamplerParams</code> <p>Parameters for sampling from a Bernoulli distribution.</p> <code>BinomialSamplerParams</code> <p>Parameters for sampling from a Binomial distribution.</p> <code>CategorySamplerParams</code> <p>Parameters for categorical sampling with optional probability weighting.</p> <code>DatetimeSamplerParams</code> <p>Parameters for uniform datetime sampling within a specified range.</p> <code>GaussianSamplerParams</code> <p>Parameters for sampling from a Gaussian (Normal) distribution.</p> <code>PersonFromFakerSamplerParams</code> <p>Parameters for sampling synthetic person data with demographic attributes from Faker.</p> <code>PersonSamplerParams</code> <p>Parameters for sampling synthetic person data with demographic attributes.</p> <code>PoissonSamplerParams</code> <p>Parameters for sampling from a Poisson distribution.</p> <code>ScipySamplerParams</code> <p>Parameters for sampling from any scipy.stats continuous or discrete distribution.</p> <code>SubcategorySamplerParams</code> <p>Parameters for subcategory sampling conditioned on a parent category column.</p> <code>TimeDeltaSamplerParams</code> <p>Parameters for sampling time deltas relative to a reference datetime column.</p> <code>UUIDSamplerParams</code> <p>Parameters for generating UUID (Universally Unique Identifier) values.</p> <code>UniformSamplerParams</code> <p>Parameters for sampling from a continuous Uniform distribution.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.BernoulliMixtureSamplerParams","title":"<code>BernoulliMixtureSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling from a Bernoulli mixture distribution.</p> <p>Combines a Bernoulli distribution with another continuous distribution, creating a mixture where values are either 0 (with probability 1-p) or sampled from the specified distribution (with probability p). This is useful for modeling scenarios with many zero values mixed with a continuous distribution of non-zero values.</p> <p>Common use cases include modeling sparse events, zero-inflated data, or situations where an outcome either doesn't occur (0) or follows a specific distribution when it does occur.</p> <p>Attributes:</p> Name Type Description <code>p</code> <code>float</code> <p>Probability of sampling from the mixture distribution (non-zero outcome). Must be between 0.0 and 1.0 (inclusive). With probability 1-p, the sample is 0.</p> <code>dist_name</code> <code>str</code> <p>Name of the scipy.stats distribution to sample from when outcome is non-zero. Must be a valid scipy.stats distribution name (e.g., \"norm\", \"gamma\", \"expon\").</p> <code>dist_params</code> <code>dict</code> <p>Parameters for the specified scipy.stats distribution.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.BernoulliSamplerParams","title":"<code>BernoulliSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling from a Bernoulli distribution.</p> <p>Samples binary values (0 or 1) representing the outcome of a single trial with a fixed probability of success. This is the simplest discrete probability distribution, useful for modeling binary outcomes like success/failure, yes/no, or true/false.</p> <p>Attributes:</p> Name Type Description <code>p</code> <code>float</code> <p>Probability of success (sampling 1). Must be between 0.0 and 1.0 (inclusive). The probability of failure (sampling 0) is automatically 1 - p.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.BinomialSamplerParams","title":"<code>BinomialSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling from a Binomial distribution.</p> <p>Samples integer values representing the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success. Commonly used to model the number of successful outcomes in repeated experiments.</p> <p>Attributes:</p> Name Type Description <code>n</code> <code>int</code> <p>Number of independent trials. Must be a positive integer.</p> <code>p</code> <code>float</code> <p>Probability of success on each trial. Must be between 0.0 and 1.0 (inclusive).</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.CategorySamplerParams","title":"<code>CategorySamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for categorical sampling with optional probability weighting.</p> <p>Samples values from a discrete set of categories. When weights are provided, values are sampled according to their assigned probabilities. Without weights, uniform sampling is used.</p> <p>Attributes:</p> Name Type Description <code>values</code> <code>list[str | int | float]</code> <p>List of possible categorical values to sample from. Can contain strings, integers, or floats. Must contain at least one value.</p> <code>weights</code> <code>list[float] | None</code> <p>Optional unnormalized probability weights for each value. If provided, must be the same length as <code>values</code>. Weights are automatically normalized to sum to 1.0. Larger weights result in higher sampling probability for the corresponding value.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.DatetimeSamplerParams","title":"<code>DatetimeSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for uniform datetime sampling within a specified range.</p> <p>Samples datetime values uniformly between a start and end date with a specified granularity. The sampling unit determines the smallest possible time interval between consecutive samples.</p> <p>Attributes:</p> Name Type Description <code>start</code> <code>str</code> <p>Earliest possible datetime for the sampling range (inclusive). Must be a valid datetime string parseable by pandas.to_datetime().</p> <code>end</code> <code>str</code> <p>Latest possible datetime for the sampling range (inclusive). Must be a valid datetime string parseable by pandas.to_datetime().</p> <code>unit</code> <code>Literal['Y', 'M', 'D', 'h', 'm', 's']</code> <p>Time unit for sampling granularity. Options: - \"Y\": Years - \"M\": Months - \"D\": Days (default) - \"h\": Hours - \"m\": Minutes - \"s\": Seconds</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.GaussianSamplerParams","title":"<code>GaussianSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling from a Gaussian (Normal) distribution.</p> <p>Samples continuous values from a normal distribution characterized by its mean and standard deviation. The Gaussian distribution is one of the most commonly used probability distributions, appearing naturally in many real-world phenomena due to the Central Limit Theorem.</p> <p>Attributes:</p> Name Type Description <code>mean</code> <code>float</code> <p>Mean (center) of the Gaussian distribution. This is the expected value and the location of the distribution's peak.</p> <code>stddev</code> <code>float</code> <p>Standard deviation of the Gaussian distribution. Controls the spread or width of the distribution. Must be positive.</p> <code>decimal_places</code> <code>int | None</code> <p>Optional number of decimal places to round sampled values to. If None, values are not rounded.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.PersonFromFakerSamplerParams","title":"<code>PersonFromFakerSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling synthetic person data with demographic attributes from Faker.</p> <p>Uses the Faker library to generate random personal information. The data is basic and not demographically accurate, but is useful for quick testing, prototyping, or when realistic demographic distributions are not relevant for your use case. For demographically accurate person data, use the <code>PersonSamplerParams</code> sampler.</p> <p>Attributes:</p> Name Type Description <code>locale</code> <code>str</code> <p>Locale string determining the language and geographic region for synthetic people. Can be any locale supported by Faker.</p> <code>sex</code> <code>SexT | None</code> <p>If specified, filters to only sample people of the specified sex. Options: \"Male\" or \"Female\". If None, samples both sexes.</p> <code>city</code> <code>str | list[str] | None</code> <p>If specified, filters to only sample people from the specified city or cities. Can be a single city name (string) or a list of city names.</p> <code>age_range</code> <code>list[int]</code> <p>Two-element list [min_age, max_age] specifying the age range to sample from (inclusive). Defaults to a standard age range. Both values must be between the minimum and maximum allowed ages.</p> <code>sampler_type</code> <code>Literal[PERSON_FROM_FAKER]</code> <p>Discriminator for the sampler type. Must be <code>SamplerType.PERSON_FROM_FAKER</code>.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.PersonFromFakerSamplerParams.generator_kwargs","title":"<code>generator_kwargs</code>  <code>property</code>","text":"<p>Keyword arguments to pass to the person generator.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.PersonSamplerParams","title":"<code>PersonSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling synthetic person data with demographic attributes.</p> <p>Generates realistic synthetic person data including names, addresses, phone numbers, and other demographic information. Data can be sampled from managed datasets (when available) or generated using Faker. The sampler supports filtering by locale, sex, age, geographic location, and can optionally include synthetic persona descriptions.</p> <p>Attributes:</p> Name Type Description <code>locale</code> <code>str</code> <p>Locale string determining the language and geographic region for synthetic people. Must be a locale supported by a managed Nemotron Personas dataset. The dataset must be downloaded and available in the managed assets directory.</p> <code>sex</code> <code>SexT | None</code> <p>If specified, filters to only sample people of the specified sex. Options: \"Male\" or \"Female\". If None, samples both sexes.</p> <code>city</code> <code>str | list[str] | None</code> <p>If specified, filters to only sample people from the specified city or cities. Can be a single city name (string) or a list of city names.</p> <code>age_range</code> <code>list[int]</code> <p>Two-element list [min_age, max_age] specifying the age range to sample from (inclusive). Defaults to a standard age range. Both values must be between minimum and maximum allowed ages.</p> <code>with_synthetic_personas</code> <code>bool</code> <p>If True, appends additional synthetic persona columns including personality traits, interests, and background descriptions. Only supported for certain locales with managed datasets.</p> <code>sample_dataset_when_available</code> <code>bool</code> <p>If True, samples from curated managed datasets when available for the specified locale. If False or unavailable, falls back to Faker-generated data. Managed datasets typically provide more realistic and diverse synthetic people.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.PersonSamplerParams.generator_kwargs","title":"<code>generator_kwargs</code>  <code>property</code>","text":"<p>Keyword arguments to pass to the person generator.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.PoissonSamplerParams","title":"<code>PoissonSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling from a Poisson distribution.</p> <p>Samples non-negative integer values representing the number of events occurring in a fixed interval of time or space. The Poisson distribution is commonly used to model count data like the number of arrivals, occurrences, or events per time period.</p> <p>The distribution is characterized by a single parameter (mean/rate), and both the mean and variance equal this parameter value.</p> <p>Attributes:</p> Name Type Description <code>mean</code> <code>float</code> <p>Mean number of events in the fixed interval (also called rate parameter \u03bb). Must be positive. This represents both the expected value and the variance of the distribution.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.ScipySamplerParams","title":"<code>ScipySamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling from any scipy.stats continuous or discrete distribution.</p> <p>Provides a flexible interface to sample from the wide range of probability distributions available in scipy.stats. This enables advanced statistical sampling beyond the built-in distribution types (Gaussian, Uniform, etc.).</p> <p>See: scipy.stats documentation</p> <p>Attributes:</p> Name Type Description <code>dist_name</code> <code>str</code> <p>Name of the scipy.stats distribution to sample from (e.g., \"beta\", \"gamma\", \"lognorm\", \"expon\"). Must be a valid distribution name from scipy.stats.</p> <code>dist_params</code> <code>dict</code> <p>Dictionary of parameters for the specified distribution. Parameter names and values must match the scipy.stats distribution specification (e.g., {\"a\": 2, \"b\": 5} for beta distribution, {\"scale\": 1.5} for exponential).</p> <code>decimal_places</code> <code>int | None</code> <p>Optional number of decimal places to round sampled values to. If None, values are not rounded.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.SubcategorySamplerParams","title":"<code>SubcategorySamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for subcategory sampling conditioned on a parent category column.</p> <p>Samples subcategory values based on the value of a parent category column. Each parent category value maps to its own list of possible subcategory values, enabling hierarchical or conditional sampling patterns.</p> <p>Attributes:</p> Name Type Description <code>category</code> <code>str</code> <p>Name of the parent category column that this subcategory depends on. The parent column must be generated before this subcategory column.</p> <code>values</code> <code>dict[str, list[str | int | float]]</code> <p>Mapping from each parent category value to a list of possible subcategory values. Each key must correspond to a value that appears in the parent category column.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.TimeDeltaSamplerParams","title":"<code>TimeDeltaSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling time deltas relative to a reference datetime column.</p> <p>Samples time offsets within a specified range and adds them to values from a reference datetime column. This is useful for generating related datetime columns like order dates and delivery dates, or event start times and end times.</p> Note <p>Years and months are not supported as timedelta units because they have variable lengths. See: pandas timedelta documentation</p> <p>Attributes:</p> Name Type Description <code>dt_min</code> <code>int</code> <p>Minimum time-delta value (inclusive). Must be non-negative and less than <code>dt_max</code>. Specified in units defined by the <code>unit</code> parameter.</p> <code>dt_max</code> <code>int</code> <p>Maximum time-delta value (exclusive). Must be positive and greater than <code>dt_min</code>. Specified in units defined by the <code>unit</code> parameter.</p> <code>reference_column_name</code> <code>str</code> <p>Name of an existing datetime column to add the time-delta to. This column must be generated before the timedelta column.</p> <code>unit</code> <code>Literal['D', 'h', 'm', 's']</code> <p>Time unit for the delta values. Options: - \"D\": Days (default) - \"h\": Hours - \"m\": Minutes - \"s\": Seconds</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.UUIDSamplerParams","title":"<code>UUIDSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for generating UUID (Universally Unique Identifier) values.</p> <p>Generates UUID4 (random) identifiers with optional formatting options. UUIDs are useful for creating unique identifiers for records, entities, or transactions.</p> <p>Attributes:</p> Name Type Description <code>prefix</code> <code>str | None</code> <p>Optional string to prepend to each UUID. Useful for creating namespaced or typed identifiers (e.g., \"user-\", \"order-\", \"txn-\").</p> <code>short_form</code> <code>bool</code> <p>If True, truncates UUIDs to 8 characters (first segment only). Default is False for full 32-character UUIDs (excluding hyphens).</p> <code>uppercase</code> <code>bool</code> <p>If True, converts all hexadecimal letters to uppercase. Default is False for lowercase UUIDs.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.UniformSamplerParams","title":"<code>UniformSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling from a continuous Uniform distribution.</p> <p>Samples continuous values uniformly from a specified range, where every value in the range has equal probability of being sampled. This is useful when all values within a range are equally likely, such as random percentages, proportions, or unbiased measurements.</p> <p>Attributes:</p> Name Type Description <code>low</code> <code>float</code> <p>Lower bound of the uniform distribution (inclusive). Can be any real number.</p> <code>high</code> <code>float</code> <p>Upper bound of the uniform distribution (inclusive). Must be greater than <code>low</code>.</p> <code>decimal_places</code> <code>int | None</code> <p>Optional number of decimal places to round sampled values to. If None, values are not rounded and may have many decimal places.</p>"},{"location":"code_reference/validator_params/","title":"Validator Parameters","text":"<p>When creating a <code>ValidationColumnConfig</code>, two parameters are used to define the validator: <code>validator_type</code> and <code>validator_config</code>. The <code>validator_type</code> parameter can be set to either <code>code</code>, <code>local_callable</code> or <code>remote</code>. The <code>validator_config</code> accompanying each of these is, respectively:</p> <p>Classes:</p> Name Description <code>CodeValidatorParams</code> <p>Configuration for code validation. Supports Python and SQL code validation.</p> <code>LocalCallableValidatorParams</code> <p>Configuration for local callable validation. Expects a function to be passed that validates the data.</p> <code>RemoteValidatorParams</code> <p>Configuration for remote validation. Sends data to a remote endpoint for validation.</p>"},{"location":"code_reference/validator_params/#data_designer.config.validator_params.CodeValidatorParams","title":"<code>CodeValidatorParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for code validation. Supports Python and SQL code validation.</p> <p>Attributes:</p> Name Type Description <code>code_lang</code> <code>CodeLang</code> <p>The language of the code to validate. Supported values include: <code>python</code>, <code>sql:sqlite</code>, <code>sql:postgres</code>, <code>sql:mysql</code>, <code>sql:tsql</code>, <code>sql:bigquery</code>, <code>sql:ansi</code>.</p>"},{"location":"code_reference/validator_params/#data_designer.config.validator_params.LocalCallableValidatorParams","title":"<code>LocalCallableValidatorParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for local callable validation. Expects a function to be passed that validates the data.</p> <p>Attributes:</p> Name Type Description <code>validation_function</code> <code>Any</code> <p>Function (<code>Callable[[pd.DataFrame], pd.DataFrame]</code>) to validate the data. Output must contain a column <code>is_valid</code> of type <code>bool</code>.</p> <code>output_schema</code> <code>dict[str, Any] | None</code> <p>The JSON schema for the local callable validator's output. If not provided, the output will not be validated.</p>"},{"location":"code_reference/validator_params/#data_designer.config.validator_params.RemoteValidatorParams","title":"<code>RemoteValidatorParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for remote validation. Sends data to a remote endpoint for validation.</p> <p>Attributes:</p> Name Type Description <code>endpoint_url</code> <code>str</code> <p>The URL of the remote endpoint.</p> <code>output_schema</code> <code>dict[str, Any] | None</code> <p>The JSON schema for the remote validator's output. If not provided, the output will not be validated.</p> <code>timeout</code> <code>float</code> <p>The timeout for the HTTP request in seconds. Defaults to 30.0.</p> <code>max_retries</code> <code>int</code> <p>The maximum number of retry attempts. Defaults to 3.</p> <code>retry_backoff</code> <code>float</code> <p>The backoff factor for the retry delay in seconds. Defaults to 2.0.</p> <code>max_parallel_requests</code> <code>int</code> <p>The maximum number of parallel requests to make. Defaults to 4.</p>"},{"location":"colab_notebooks/1-the-basics/","title":"1 the basics","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install -U data-designer\n</pre> %%capture !pip install -U data-designer In\u00a0[\u00a0]: Copied! <pre>import getpass\nimport os\n\nfrom google.colab import userdata\n\ntry:\n    os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\nexcept userdata.SecretNotFoundError:\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")\n</pre> import getpass import os  from google.colab import userdata  try:     os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\") except userdata.SecretNotFoundError:     os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \") In\u00a0[\u00a0]: Copied! <pre>from data_designer.essentials import (\n    CategorySamplerParams,\n    ChatCompletionInferenceParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    LLMTextColumnConfig,\n    ModelConfig,\n    PersonFromFakerSamplerParams,\n    SamplerColumnConfig,\n    SamplerType,\n    SubcategorySamplerParams,\n    UniformSamplerParams,\n)\n</pre> from data_designer.essentials import (     CategorySamplerParams,     ChatCompletionInferenceParams,     DataDesigner,     DataDesignerConfigBuilder,     LLMTextColumnConfig,     ModelConfig,     PersonFromFakerSamplerParams,     SamplerColumnConfig,     SamplerType,     SubcategorySamplerParams,     UniformSamplerParams, ) In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[\u00a0]: Copied! <pre>config_builder = DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre>config_builder.info.display(\"samplers\")\n</pre> config_builder.info.display(\"samplers\") <p>Let's start designing our product review dataset by adding product category and subcategory columns.</p> In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    SamplerColumnConfig(\n        name=\"product_category\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\n                \"Electronics\",\n                \"Clothing\",\n                \"Home &amp; Kitchen\",\n                \"Books\",\n                \"Home Office\",\n            ],\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"product_subcategory\",\n        sampler_type=SamplerType.SUBCATEGORY,\n        params=SubcategorySamplerParams(\n            category=\"product_category\",\n            values={\n                \"Electronics\": [\n                    \"Smartphones\",\n                    \"Laptops\",\n                    \"Headphones\",\n                    \"Cameras\",\n                    \"Accessories\",\n                ],\n                \"Clothing\": [\n                    \"Men's Clothing\",\n                    \"Women's Clothing\",\n                    \"Winter Coats\",\n                    \"Activewear\",\n                    \"Accessories\",\n                ],\n                \"Home &amp; Kitchen\": [\n                    \"Appliances\",\n                    \"Cookware\",\n                    \"Furniture\",\n                    \"Decor\",\n                    \"Organization\",\n                ],\n                \"Books\": [\n                    \"Fiction\",\n                    \"Non-Fiction\",\n                    \"Self-Help\",\n                    \"Textbooks\",\n                    \"Classics\",\n                ],\n                \"Home Office\": [\n                    \"Desks\",\n                    \"Chairs\",\n                    \"Storage\",\n                    \"Office Supplies\",\n                    \"Lighting\",\n                ],\n            },\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"target_age_range\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),\n    )\n)\n\n# Optionally validate that the columns are configured correctly.\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     SamplerColumnConfig(         name=\"product_category\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(             values=[                 \"Electronics\",                 \"Clothing\",                 \"Home &amp; Kitchen\",                 \"Books\",                 \"Home Office\",             ],         ),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"product_subcategory\",         sampler_type=SamplerType.SUBCATEGORY,         params=SubcategorySamplerParams(             category=\"product_category\",             values={                 \"Electronics\": [                     \"Smartphones\",                     \"Laptops\",                     \"Headphones\",                     \"Cameras\",                     \"Accessories\",                 ],                 \"Clothing\": [                     \"Men's Clothing\",                     \"Women's Clothing\",                     \"Winter Coats\",                     \"Activewear\",                     \"Accessories\",                 ],                 \"Home &amp; Kitchen\": [                     \"Appliances\",                     \"Cookware\",                     \"Furniture\",                     \"Decor\",                     \"Organization\",                 ],                 \"Books\": [                     \"Fiction\",                     \"Non-Fiction\",                     \"Self-Help\",                     \"Textbooks\",                     \"Classics\",                 ],                 \"Home Office\": [                     \"Desks\",                     \"Chairs\",                     \"Storage\",                     \"Office Supplies\",                     \"Lighting\",                 ],             },         ),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"target_age_range\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),     ) )  # Optionally validate that the columns are configured correctly. data_designer.validate(config_builder) <p>Next, let's add samplers to generate data related to the customer and their review.</p> In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=SamplerType.PERSON_FROM_FAKER,\n        params=PersonFromFakerSamplerParams(age_range=[18, 70], locale=\"en_US\"),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"number_of_stars\",\n        sampler_type=SamplerType.UNIFORM,\n        params=UniformSamplerParams(low=1, high=5),\n        convert_to=\"int\",  # Convert the sampled float to an integer.\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"review_style\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],\n            weights=[1, 2, 2, 1],\n        ),\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     SamplerColumnConfig(         name=\"customer\",         sampler_type=SamplerType.PERSON_FROM_FAKER,         params=PersonFromFakerSamplerParams(age_range=[18, 70], locale=\"en_US\"),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"number_of_stars\",         sampler_type=SamplerType.UNIFORM,         params=UniformSamplerParams(low=1, high=5),         convert_to=\"int\",  # Convert the sampled float to an integer.     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"review_style\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(             values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],             weights=[1, 2, 2, 1],         ),     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    LLMTextColumnConfig(\n        name=\"product_name\",\n        prompt=(\n            \"You are a helpful assistant that generates product names. DO NOT add quotes around the product name.\\n\\n\"\n            \"Come up with a creative product name for a product in the '{{ product_category }}' category, focusing \"\n            \"on products related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"\n            \"{{ target_age_range }} years old. Respond with only the product name, no other text.\"\n        ),\n        model_alias=MODEL_ALIAS,\n    )\n)\n\nconfig_builder.add_column(\n    LLMTextColumnConfig(\n        name=\"customer_review\",\n        prompt=(\n            \"You are a customer named {{ customer.first_name }} from {{ customer.city }}, {{ customer.state }}. \"\n            \"You are {{ customer.age }} years old and recently purchased a product called {{ product_name }}. \"\n            \"Write a review of this product, which you gave a rating of {{ number_of_stars }} stars. \"\n            \"The style of the review should be '{{ review_style }}'. \"\n            \"Respond with only the review, no other text.\"\n        ),\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     LLMTextColumnConfig(         name=\"product_name\",         prompt=(             \"You are a helpful assistant that generates product names. DO NOT add quotes around the product name.\\n\\n\"             \"Come up with a creative product name for a product in the '{{ product_category }}' category, focusing \"             \"on products related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"             \"{{ target_age_range }} years old. Respond with only the product name, no other text.\"         ),         model_alias=MODEL_ALIAS,     ) )  config_builder.add_column(     LLMTextColumnConfig(         name=\"customer_review\",         prompt=(             \"You are a customer named {{ customer.first_name }} from {{ customer.city }}, {{ customer.state }}. \"             \"You are {{ customer.age }} years old and recently purchased a product called {{ product_name }}. \"             \"Write a review of this product, which you gave a rating of {{ number_of_stars }} stars. \"             \"The style of the review should be '{{ review_style }}'. \"             \"Respond with only the review, no other text.\"         ),         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-1\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-1\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"colab_notebooks/1-the-basics/#data-designer-tutorial-the-basics","title":"\ud83c\udfa8 Data Designer Tutorial: The Basics\u00b6","text":""},{"location":"colab_notebooks/1-the-basics/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>This notebook demonstrates the basics of Data Designer by generating a simple product review dataset.</p>"},{"location":"colab_notebooks/1-the-basics/#colab-setup","title":"\u26a1 Colab Setup\u00b6","text":"<p>Run the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from build.nvidia.com.</p>"},{"location":"colab_notebooks/1-the-basics/#import-the-essentials","title":"\ud83d\udce6 Import the essentials\u00b6","text":"<ul> <li>The <code>essentials</code> module provides quick access to the most commonly used objects.</li> </ul>"},{"location":"colab_notebooks/1-the-basics/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object is responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"colab_notebooks/1-the-basics/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"colab_notebooks/1-the-basics/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"colab_notebooks/1-the-basics/#getting-started-with-sampler-columns","title":"\ud83c\udfb2 Getting started with sampler columns\u00b6","text":"<ul> <li><p>Sampler columns offer non-LLM based generation of synthetic data.</p> </li> <li><p>They are particularly useful for steering the diversity of the generated data, as we demonstrate below.</p> </li> </ul> <p>You can view available samplers using the config builder's <code>info</code> property:</p>"},{"location":"colab_notebooks/1-the-basics/#llm-generated-columns","title":"\ud83e\udd9c LLM-generated columns\u00b6","text":"<ul> <li><p>The real power of Data Designer comes from leveraging LLMs to generate text, code, and structured data.</p> </li> <li><p>When prompting the LLM, we can use Jinja templating to reference other columns in the dataset.</p> </li> <li><p>As we see below, nested json fields can be accessed using dot notation.</p> </li> </ul>"},{"location":"colab_notebooks/1-the-basics/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"colab_notebooks/1-the-basics/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"colab_notebooks/1-the-basics/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"colab_notebooks/1-the-basics/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Now that you've seen the basics of Data Designer, check out the following notebooks to learn more about:</p> <ul> <li><p>Structured outputs and jinja expressions</p> </li> <li><p>Seeding synthetic data generation with an external dataset</p> </li> <li><p>Providing images as context</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/","title":"2 structured outputs and jinja expressions","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install -U data-designer\n</pre> %%capture !pip install -U data-designer In\u00a0[\u00a0]: Copied! <pre>import getpass\nimport os\n\nfrom google.colab import userdata\n\ntry:\n    os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\nexcept userdata.SecretNotFoundError:\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")\n</pre> import getpass import os  from google.colab import userdata  try:     os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\") except userdata.SecretNotFoundError:     os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \") In\u00a0[\u00a0]: Copied! <pre>from data_designer.essentials import (\n    CategorySamplerParams,\n    ChatCompletionInferenceParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    ExpressionColumnConfig,\n    LLMStructuredColumnConfig,\n    ModelConfig,\n    PersonFromFakerSamplerParams,\n    SamplerColumnConfig,\n    SamplerType,\n    SubcategorySamplerParams,\n)\n</pre> from data_designer.essentials import (     CategorySamplerParams,     ChatCompletionInferenceParams,     DataDesigner,     DataDesignerConfigBuilder,     ExpressionColumnConfig,     LLMStructuredColumnConfig,     ModelConfig,     PersonFromFakerSamplerParams,     SamplerColumnConfig,     SamplerType,     SubcategorySamplerParams, ) In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[\u00a0]: Copied! <pre>config_builder = DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre>from decimal import Decimal\nfrom typing import Literal\n\nfrom pydantic import BaseModel, Field\n\n\n# We define a Product schema so that the name, description, and price are generated\n# in one go, with the types and constraints specified.\nclass Product(BaseModel):\n    name: str = Field(description=\"The name of the product\")\n    description: str = Field(description=\"A description of the product\")\n    price: Decimal = Field(description=\"The price of the product\", ge=10, le=1000, decimal_places=2)\n\n\nclass ProductReview(BaseModel):\n    rating: int = Field(description=\"The rating of the product\", ge=1, le=5)\n    customer_mood: Literal[\"irritated\", \"mad\", \"happy\", \"neutral\", \"excited\"] = Field(\n        description=\"The mood of the customer\"\n    )\n    review: str = Field(description=\"A review of the product\")\n</pre> from decimal import Decimal from typing import Literal  from pydantic import BaseModel, Field   # We define a Product schema so that the name, description, and price are generated # in one go, with the types and constraints specified. class Product(BaseModel):     name: str = Field(description=\"The name of the product\")     description: str = Field(description=\"A description of the product\")     price: Decimal = Field(description=\"The price of the product\", ge=10, le=1000, decimal_places=2)   class ProductReview(BaseModel):     rating: int = Field(description=\"The rating of the product\", ge=1, le=5)     customer_mood: Literal[\"irritated\", \"mad\", \"happy\", \"neutral\", \"excited\"] = Field(         description=\"The mood of the customer\"     )     review: str = Field(description=\"A review of the product\") <p>Next, let's design our product review dataset using a few more tricks compared to the previous notebook.</p> In\u00a0[\u00a0]: Copied! <pre># Since we often only want a few attributes from Person objects, we can\n# set drop=True in the column config to drop the column from the final dataset.\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=SamplerType.PERSON_FROM_FAKER,\n        params=PersonFromFakerSamplerParams(),\n        drop=True,\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"product_category\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\n                \"Electronics\",\n                \"Clothing\",\n                \"Home &amp; Kitchen\",\n                \"Books\",\n                \"Home Office\",\n            ],\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"product_subcategory\",\n        sampler_type=SamplerType.SUBCATEGORY,\n        params=SubcategorySamplerParams(\n            category=\"product_category\",\n            values={\n                \"Electronics\": [\n                    \"Smartphones\",\n                    \"Laptops\",\n                    \"Headphones\",\n                    \"Cameras\",\n                    \"Accessories\",\n                ],\n                \"Clothing\": [\n                    \"Men's Clothing\",\n                    \"Women's Clothing\",\n                    \"Winter Coats\",\n                    \"Activewear\",\n                    \"Accessories\",\n                ],\n                \"Home &amp; Kitchen\": [\n                    \"Appliances\",\n                    \"Cookware\",\n                    \"Furniture\",\n                    \"Decor\",\n                    \"Organization\",\n                ],\n                \"Books\": [\n                    \"Fiction\",\n                    \"Non-Fiction\",\n                    \"Self-Help\",\n                    \"Textbooks\",\n                    \"Classics\",\n                ],\n                \"Home Office\": [\n                    \"Desks\",\n                    \"Chairs\",\n                    \"Storage\",\n                    \"Office Supplies\",\n                    \"Lighting\",\n                ],\n            },\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"target_age_range\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),\n    )\n)\n\n# Sampler columns support conditional params, which are used if the condition is met.\n# In this example, we set the review style to rambling if the target age range is 18-25.\n# Note conditional parameters are only supported for Sampler column types.\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"review_style\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],\n            weights=[1, 2, 2, 1],\n        ),\n        conditional_params={\n            \"target_age_range == '18-25'\": CategorySamplerParams(values=[\"rambling\"]),\n        },\n    )\n)\n\n# Optionally validate that the columns are configured correctly.\ndata_designer.validate(config_builder)\n</pre> # Since we often only want a few attributes from Person objects, we can # set drop=True in the column config to drop the column from the final dataset. config_builder.add_column(     SamplerColumnConfig(         name=\"customer\",         sampler_type=SamplerType.PERSON_FROM_FAKER,         params=PersonFromFakerSamplerParams(),         drop=True,     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"product_category\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(             values=[                 \"Electronics\",                 \"Clothing\",                 \"Home &amp; Kitchen\",                 \"Books\",                 \"Home Office\",             ],         ),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"product_subcategory\",         sampler_type=SamplerType.SUBCATEGORY,         params=SubcategorySamplerParams(             category=\"product_category\",             values={                 \"Electronics\": [                     \"Smartphones\",                     \"Laptops\",                     \"Headphones\",                     \"Cameras\",                     \"Accessories\",                 ],                 \"Clothing\": [                     \"Men's Clothing\",                     \"Women's Clothing\",                     \"Winter Coats\",                     \"Activewear\",                     \"Accessories\",                 ],                 \"Home &amp; Kitchen\": [                     \"Appliances\",                     \"Cookware\",                     \"Furniture\",                     \"Decor\",                     \"Organization\",                 ],                 \"Books\": [                     \"Fiction\",                     \"Non-Fiction\",                     \"Self-Help\",                     \"Textbooks\",                     \"Classics\",                 ],                 \"Home Office\": [                     \"Desks\",                     \"Chairs\",                     \"Storage\",                     \"Office Supplies\",                     \"Lighting\",                 ],             },         ),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"target_age_range\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),     ) )  # Sampler columns support conditional params, which are used if the condition is met. # In this example, we set the review style to rambling if the target age range is 18-25. # Note conditional parameters are only supported for Sampler column types. config_builder.add_column(     SamplerColumnConfig(         name=\"review_style\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(             values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],             weights=[1, 2, 2, 1],         ),         conditional_params={             \"target_age_range == '18-25'\": CategorySamplerParams(values=[\"rambling\"]),         },     ) )  # Optionally validate that the columns are configured correctly. data_designer.validate(config_builder) <p>Next, we will use more advanced Jinja expressions to create new columns.</p> <p>Jinja expressions let you:</p> <ul> <li><p>Access nested attributes: <code>{{ customer.first_name }}</code></p> </li> <li><p>Combine values: <code>{{ customer.first_name }} {{ customer.last_name }}</code></p> </li> <li><p>Use conditional logic: <code>{% if condition %}...{% endif %}</code></p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre># We can create new columns using Jinja expressions that reference\n# existing columns, including attributes of nested objects.\nconfig_builder.add_column(\n    ExpressionColumnConfig(name=\"customer_name\", expr=\"{{ customer.first_name }} {{ customer.last_name }}\")\n)\n\nconfig_builder.add_column(ExpressionColumnConfig(name=\"customer_age\", expr=\"{{ customer.age }}\"))\n\nconfig_builder.add_column(\n    LLMStructuredColumnConfig(\n        name=\"product\",\n        prompt=(\n            \"Create a product in the '{{ product_category }}' category, focusing on products  \"\n            \"related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"\n            \"{{ target_age_range }} years old. The product should be priced between $10 and $1000.\"\n        ),\n        output_format=Product,\n        model_alias=MODEL_ALIAS,\n    )\n)\n\n# We can even use if/else logic in our Jinja expressions to create more complex prompt patterns.\nconfig_builder.add_column(\n    LLMStructuredColumnConfig(\n        name=\"customer_review\",\n        prompt=(\n            \"Your task is to write a review for the following product:\\n\\n\"\n            \"Product Name: {{ product.name }}\\n\"\n            \"Product Description: {{ product.description }}\\n\"\n            \"Price: {{ product.price }}\\n\\n\"\n            \"Imagine your name is {{ customer_name }} and you are from {{ customer.city }}, {{ customer.state }}. \"\n            \"Write the review in a style that is '{{ review_style }}'.\"\n            \"{% if target_age_range == '18-25' %}\"\n            \"Make sure the review is more informal and conversational.\\n\"\n            \"{% else %}\"\n            \"Make sure the review is more formal and structured.\\n\"\n            \"{% endif %}\"\n            \"The review field should contain only the review, no other text.\"\n        ),\n        output_format=ProductReview,\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> # We can create new columns using Jinja expressions that reference # existing columns, including attributes of nested objects. config_builder.add_column(     ExpressionColumnConfig(name=\"customer_name\", expr=\"{{ customer.first_name }} {{ customer.last_name }}\") )  config_builder.add_column(ExpressionColumnConfig(name=\"customer_age\", expr=\"{{ customer.age }}\"))  config_builder.add_column(     LLMStructuredColumnConfig(         name=\"product\",         prompt=(             \"Create a product in the '{{ product_category }}' category, focusing on products  \"             \"related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"             \"{{ target_age_range }} years old. The product should be priced between $10 and $1000.\"         ),         output_format=Product,         model_alias=MODEL_ALIAS,     ) )  # We can even use if/else logic in our Jinja expressions to create more complex prompt patterns. config_builder.add_column(     LLMStructuredColumnConfig(         name=\"customer_review\",         prompt=(             \"Your task is to write a review for the following product:\\n\\n\"             \"Product Name: {{ product.name }}\\n\"             \"Product Description: {{ product.description }}\\n\"             \"Price: {{ product.price }}\\n\\n\"             \"Imagine your name is {{ customer_name }} and you are from {{ customer.city }}, {{ customer.state }}. \"             \"Write the review in a style that is '{{ review_style }}'.\"             \"{% if target_age_range == '18-25' %}\"             \"Make sure the review is more informal and conversational.\\n\"             \"{% else %}\"             \"Make sure the review is more formal and structured.\\n\"             \"{% endif %}\"             \"The review field should contain only the review, no other text.\"         ),         output_format=ProductReview,         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-2\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-2\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#data-designer-tutorial-structured-outputs-and-jinja-expressions","title":"\ud83c\udfa8 Data Designer Tutorial: Structured Outputs and Jinja Expressions\u00b6","text":""},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>In this notebook, we will continue our exploration of Data Designer, demonstrating more advanced data generation using structured outputs and Jinja expressions.</p> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#colab-setup","title":"\u26a1 Colab Setup\u00b6","text":"<p>Run the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from build.nvidia.com.</p>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#import-the-essentials","title":"\ud83d\udce6 Import the essentials\u00b6","text":"<ul> <li>The <code>essentials</code> module provides quick access to the most commonly used objects.</li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object that is used to interface with the library.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#designing-our-data","title":"\ud83e\uddd1\u200d\ud83c\udfa8 Designing our data\u00b6","text":"<ul> <li><p>We will again create a product review dataset, but this time we will use structured outputs and Jinja expressions.</p> </li> <li><p>Structured outputs let you specify the exact schema of the data you want to generate.</p> </li> <li><p>Data Designer supports schemas specified using either json schema or Pydantic data models (recommended).</p> </li> </ul> <p>We'll define our structured outputs using Pydantic data models</p> <p>\ud83d\udca1 Why Pydantic?</p> <ul> <li><p>Pydantic models provide better IDE support and type validation.</p> </li> <li><p>They are more Pythonic than raw JSON schemas.</p> </li> <li><p>They integrate seamlessly with Data Designer's structured output system.</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Check out the following notebook to learn more about:</p> <ul> <li><p>Seeding synthetic data generation with an external dataset</p> </li> <li><p>Providing images as contextA</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/","title":"3 seeding with a dataset","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install -U data-designer\n</pre> %%capture !pip install -U data-designer In\u00a0[\u00a0]: Copied! <pre>import getpass\nimport os\n\nfrom google.colab import userdata\n\ntry:\n    os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\nexcept userdata.SecretNotFoundError:\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")\n</pre> import getpass import os  from google.colab import userdata  try:     os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\") except userdata.SecretNotFoundError:     os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \") In\u00a0[\u00a0]: Copied! <pre>from data_designer.essentials import (\n    ChatCompletionInferenceParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    LocalFileSeedSource,\n    ModelConfig,\n)\n</pre> from data_designer.essentials import (     ChatCompletionInferenceParams,     DataDesigner,     DataDesignerConfigBuilder,     LocalFileSeedSource,     ModelConfig, ) In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[\u00a0]: Copied! <pre>config_builder = DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre># Download sample dataset from Github\nimport urllib.request\n\nurl = \"https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/refs/heads/main/nemo/NeMo-Data-Designer/data/gretelai_symptom_to_diagnosis.csv\"\nlocal_filename, _ = urllib.request.urlretrieve(url, \"gretelai_symptom_to_diagnosis.csv\")\n\n# Seed datasets are passed as reference objects to the config builder.\nseed_source = LocalFileSeedSource(path=local_filename)\n\nconfig_builder.with_seed_dataset(seed_source)\n</pre> # Download sample dataset from Github import urllib.request  url = \"https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/refs/heads/main/nemo/NeMo-Data-Designer/data/gretelai_symptom_to_diagnosis.csv\" local_filename, _ = urllib.request.urlretrieve(url, \"gretelai_symptom_to_diagnosis.csv\")  # Seed datasets are passed as reference objects to the config builder. seed_source = LocalFileSeedSource(path=local_filename)  config_builder.with_seed_dataset(seed_source) In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    name=\"patient_sampler\",\n    column_type=\"sampler\",\n    sampler_type=\"person_from_faker\",\n)\n\nconfig_builder.add_column(\n    name=\"doctor_sampler\",\n    column_type=\"sampler\",\n    sampler_type=\"person_from_faker\",\n)\n\nconfig_builder.add_column(\n    name=\"patient_id\",\n    column_type=\"sampler\",\n    sampler_type=\"uuid\",\n    params={\n        \"prefix\": \"PT-\",\n        \"short_form\": True,\n        \"uppercase\": True,\n    },\n)\n\nconfig_builder.add_column(\n    name=\"first_name\",\n    column_type=\"expression\",\n    expr=\"{{ patient_sampler.first_name}}\",\n)\n\nconfig_builder.add_column(\n    name=\"last_name\",\n    column_type=\"expression\",\n    expr=\"{{ patient_sampler.last_name }}\",\n)\n\n\nconfig_builder.add_column(\n    name=\"dob\",\n    column_type=\"expression\",\n    expr=\"{{ patient_sampler.birth_date }}\",\n)\n\nconfig_builder.add_column(\n    name=\"symptom_onset_date\",\n    column_type=\"sampler\",\n    sampler_type=\"datetime\",\n    params={\"start\": \"2024-01-01\", \"end\": \"2024-12-31\"},\n)\n\nconfig_builder.add_column(\n    name=\"date_of_visit\",\n    column_type=\"sampler\",\n    sampler_type=\"timedelta\",\n    params={\"dt_min\": 1, \"dt_max\": 30, \"reference_column_name\": \"symptom_onset_date\"},\n)\n\nconfig_builder.add_column(\n    name=\"physician\",\n    column_type=\"expression\",\n    expr=\"Dr. {{ doctor_sampler.last_name }}\",\n)\n\nconfig_builder.add_column(\n    name=\"physician_notes\",\n    column_type=\"llm-text\",\n    prompt=\"\"\"\\\nYou are a primary-care physician who just had an appointment with {{ first_name }} {{ last_name }},\nwho has been struggling with symptoms from {{ diagnosis }} since {{ symptom_onset_date }}.\nThe date of today's visit is {{ date_of_visit }}.\n\n{{ patient_summary }}\n\nWrite careful notes about your visit with {{ first_name }},\nas Dr. {{ doctor_sampler.first_name }} {{ doctor_sampler.last_name }}.\n\nFormat the notes as a busy doctor might.\nRespond with only the notes, no other text.\n\"\"\",\n    model_alias=MODEL_ALIAS,\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     name=\"patient_sampler\",     column_type=\"sampler\",     sampler_type=\"person_from_faker\", )  config_builder.add_column(     name=\"doctor_sampler\",     column_type=\"sampler\",     sampler_type=\"person_from_faker\", )  config_builder.add_column(     name=\"patient_id\",     column_type=\"sampler\",     sampler_type=\"uuid\",     params={         \"prefix\": \"PT-\",         \"short_form\": True,         \"uppercase\": True,     }, )  config_builder.add_column(     name=\"first_name\",     column_type=\"expression\",     expr=\"{{ patient_sampler.first_name}}\", )  config_builder.add_column(     name=\"last_name\",     column_type=\"expression\",     expr=\"{{ patient_sampler.last_name }}\", )   config_builder.add_column(     name=\"dob\",     column_type=\"expression\",     expr=\"{{ patient_sampler.birth_date }}\", )  config_builder.add_column(     name=\"symptom_onset_date\",     column_type=\"sampler\",     sampler_type=\"datetime\",     params={\"start\": \"2024-01-01\", \"end\": \"2024-12-31\"}, )  config_builder.add_column(     name=\"date_of_visit\",     column_type=\"sampler\",     sampler_type=\"timedelta\",     params={\"dt_min\": 1, \"dt_max\": 30, \"reference_column_name\": \"symptom_onset_date\"}, )  config_builder.add_column(     name=\"physician\",     column_type=\"expression\",     expr=\"Dr. {{ doctor_sampler.last_name }}\", )  config_builder.add_column(     name=\"physician_notes\",     column_type=\"llm-text\",     prompt=\"\"\"\\ You are a primary-care physician who just had an appointment with {{ first_name }} {{ last_name }}, who has been struggling with symptoms from {{ diagnosis }} since {{ symptom_onset_date }}. The date of today's visit is {{ date_of_visit }}.  {{ patient_summary }}  Write careful notes about your visit with {{ first_name }}, as Dr. {{ doctor_sampler.first_name }} {{ doctor_sampler.last_name }}.  Format the notes as a busy doctor might. Respond with only the notes, no other text. \"\"\",     model_alias=MODEL_ALIAS, )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-3\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-3\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#data-designer-tutorial-seeding-synthetic-data-generation-with-an-external-dataset","title":"\ud83c\udfa8 Data Designer Tutorial: Seeding Synthetic Data Generation with an External Dataset\u00b6","text":""},{"location":"colab_notebooks/3-seeding-with-a-dataset/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>In this notebook, we will demonstrate how to seed synthetic data generation in Data Designer with an external dataset.</p> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#colab-setup","title":"\u26a1 Colab Setup\u00b6","text":"<p>Run the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from build.nvidia.com.</p>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#import-the-essentials","title":"\ud83d\udce6 Import the essentials\u00b6","text":"<ul> <li>The <code>essentials</code> module provides quick access to the most commonly used objects.</li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object is responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#prepare-a-seed-dataset","title":"\ud83c\udfe5 Prepare a seed dataset\u00b6","text":"<ul> <li><p>For this notebook, we'll create a synthetic dataset of patient notes.</p> </li> <li><p>We will seed the generation process with a symptom-to-diagnosis dataset.</p> </li> <li><p>We already have the dataset downloaded in the data directory of this repository.</p> </li> </ul> <p>\ud83c\udf31 Why use a seed dataset?</p> <ul> <li><p>Seed datasets let you steer the generation process by providing context that is specific to your use case.</p> </li> <li><p>Seed datasets are also an excellent way to inject real-world diversity into your synthetic data.</p> </li> <li><p>During generation, prompt templates can reference any of the seed dataset fields.</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#designing-our-synthetic-patient-notes-dataset","title":"\ud83c\udfa8 Designing our synthetic patient notes dataset\u00b6","text":"<ul> <li><p>Here we use <code>add_column</code> with keyword arguments (rather than imported config objects).</p> </li> <li><p>Generally, we recommend using concrete objects, but this is a convenient shorthand.</p> </li> <li><p>Note: The prompt template can reference fields from our seed dataset:</p> <ul> <li><code>{{ diagnosis }}</code> - the medical diagnosis from the seed data</li> <li><code>{{ patient_summary }}</code> - the symptom description from the seed data</li> </ul> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Check out the following notebook to learn more about:</p> <ul> <li>Providing images as context</li> </ul>"},{"location":"colab_notebooks/4-providing-images-as-context/","title":"4 providing images as context","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install -U data-designer pillow&gt;=12.0.0\n</pre> %%capture !pip install -U data-designer pillow&gt;=12.0.0 In\u00a0[\u00a0]: Copied! <pre>import getpass\nimport os\n\nfrom google.colab import userdata\n\ntry:\n    os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\nexcept userdata.SecretNotFoundError:\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")\n</pre> import getpass import os  from google.colab import userdata  try:     os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\") except userdata.SecretNotFoundError:     os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \") In\u00a0[\u00a0]: Copied! <pre># Standard library imports\nimport base64\nimport io\nimport uuid\n\n# Third-party imports\nimport pandas as pd\nimport rich\nfrom datasets import load_dataset\nfrom IPython.display import display\nfrom rich.panel import Panel\n\n# Data Designer imports\nfrom data_designer.essentials import (\n    ChatCompletionInferenceParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    DataFrameSeedSource,\n    ImageContext,\n    ImageFormat,\n    LLMTextColumnConfig,\n    ModalityDataType,\n    ModelConfig,\n)\n</pre> # Standard library imports import base64 import io import uuid  # Third-party imports import pandas as pd import rich from datasets import load_dataset from IPython.display import display from rich.panel import Panel  # Data Designer imports from data_designer.essentials import (     ChatCompletionInferenceParams,     DataDesigner,     DataDesignerConfigBuilder,     DataFrameSeedSource,     ImageContext,     ImageFormat,     LLMTextColumnConfig,     ModalityDataType,     ModelConfig, ) In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\nmodel_configs = [\n    ModelConfig(\n        alias=\"vision\",\n        model=\"meta/llama-4-scout-17b-16e-instruct\",\n        provider=MODEL_PROVIDER,\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=0.60,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  model_configs = [     ModelConfig(         alias=\"vision\",         model=\"meta/llama-4-scout-17b-16e-instruct\",         provider=MODEL_PROVIDER,         inference_parameters=ChatCompletionInferenceParams(             temperature=0.60,             top_p=0.95,             max_tokens=2048,         ),     ), ] In\u00a0[\u00a0]: Copied! <pre>config_builder = DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre># Dataset processing configuration\nIMG_COUNT = 512  # Number of images to process\nBASE64_IMAGE_HEIGHT = 512  # Standardized height for model input\n\n# Load ColPali dataset for visual documents\nimg_dataset_cfg = {\"path\": \"vidore/colpali_train_set\", \"split\": \"train\", \"streaming\": True}\n</pre> # Dataset processing configuration IMG_COUNT = 512  # Number of images to process BASE64_IMAGE_HEIGHT = 512  # Standardized height for model input  # Load ColPali dataset for visual documents img_dataset_cfg = {\"path\": \"vidore/colpali_train_set\", \"split\": \"train\", \"streaming\": True} In\u00a0[\u00a0]: Copied! <pre>def resize_image(image, height: int):\n    \"\"\"\n    Resize image while maintaining aspect ratio.\n\n    Args:\n        image: PIL Image object\n        height: Target height in pixels\n\n    Returns:\n        Resized PIL Image object\n    \"\"\"\n    original_width, original_height = image.size\n    width = int(original_width * (height / original_height))\n    return image.resize((width, height))\n\n\ndef convert_image_to_chat_format(record, height: int) -&gt; dict:\n    \"\"\"\n    Convert PIL image to base64 format for chat template usage.\n\n    Args:\n        record: Dataset record containing image and metadata\n        height: Target height for image resizing\n\n    Returns:\n        Updated record with base64_image and uuid fields\n    \"\"\"\n    # Resize image for consistent processing\n    image = resize_image(record[\"image\"], height)\n\n    # Convert to base64 string\n    img_buffer = io.BytesIO()\n    image.save(img_buffer, format=\"PNG\")\n    byte_data = img_buffer.getvalue()\n    base64_encoded_data = base64.b64encode(byte_data)\n    base64_string = base64_encoded_data.decode(\"utf-8\")\n\n    # Return updated record\n    return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())}\n</pre> def resize_image(image, height: int):     \"\"\"     Resize image while maintaining aspect ratio.      Args:         image: PIL Image object         height: Target height in pixels      Returns:         Resized PIL Image object     \"\"\"     original_width, original_height = image.size     width = int(original_width * (height / original_height))     return image.resize((width, height))   def convert_image_to_chat_format(record, height: int) -&gt; dict:     \"\"\"     Convert PIL image to base64 format for chat template usage.      Args:         record: Dataset record containing image and metadata         height: Target height for image resizing      Returns:         Updated record with base64_image and uuid fields     \"\"\"     # Resize image for consistent processing     image = resize_image(record[\"image\"], height)      # Convert to base64 string     img_buffer = io.BytesIO()     image.save(img_buffer, format=\"PNG\")     byte_data = img_buffer.getvalue()     base64_encoded_data = base64.b64encode(byte_data)     base64_string = base64_encoded_data.decode(\"utf-8\")      # Return updated record     return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())} In\u00a0[\u00a0]: Copied! <pre># Load and process the visual document dataset\nprint(\"\ud83d\udce5 Loading and processing document images...\")\n\nimg_dataset_iter = iter(\n    load_dataset(**img_dataset_cfg).map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT})\n)\nimg_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])\n\nprint(f\"\u2705 Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\")\n</pre> # Load and process the visual document dataset print(\"\ud83d\udce5 Loading and processing document images...\")  img_dataset_iter = iter(     load_dataset(**img_dataset_cfg).map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT}) ) img_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])  print(f\"\u2705 Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\") In\u00a0[\u00a0]: Copied! <pre>img_dataset.head()\n</pre> img_dataset.head() In\u00a0[\u00a0]: Copied! <pre># Add the seed dataset containing our processed images\ndf_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]]\nconfig_builder.with_seed_dataset(DataFrameSeedSource(df=df_seed))\n</pre> # Add the seed dataset containing our processed images df_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]] config_builder.with_seed_dataset(DataFrameSeedSource(df=df_seed)) In\u00a0[\u00a0]: Copied! <pre># Add a column to generate detailed document summaries\nconfig_builder.add_column(\n    LLMTextColumnConfig(\n        name=\"summary\",\n        model_alias=\"vision\",\n        prompt=(\n            \"Provide a detailed summary of the content in this image in Markdown format. \"\n            \"Start from the top of the image and then describe it from top to bottom. \"\n            \"Place a summary at the bottom.\"\n        ),\n        multi_modal_context=[\n            ImageContext(\n                column_name=\"base64_image\",\n                data_type=ModalityDataType.BASE64,\n                image_format=ImageFormat.PNG,\n            )\n        ],\n    )\n)\n</pre> # Add a column to generate detailed document summaries config_builder.add_column(     LLMTextColumnConfig(         name=\"summary\",         model_alias=\"vision\",         prompt=(             \"Provide a detailed summary of the content in this image in Markdown format. \"             \"Start from the top of the image and then describe it from top to bottom. \"             \"Place a summary at the bottom.\"         ),         multi_modal_context=[             ImageContext(                 column_name=\"base64_image\",                 data_type=ModalityDataType.BASE64,                 image_format=ImageFormat.PNG,             )         ],     ) ) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre># Compare original document with generated summary\nindex = 0  # Change this to view different examples\n\n# Merge preview data with original images for comparison\ncomparison_dataset = preview.dataset.merge(pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\")\n\n# Extract the record for display\nrecord = comparison_dataset.iloc[index]\n\nprint(\"\ud83d\udcc4 Original Document Image:\")\ndisplay(resize_image(record.image, BASE64_IMAGE_HEIGHT))\n\nprint(\"\\n\ud83d\udcdd Generated Summary:\")\nrich.print(Panel(record.summary, title=\"Document Summary\", title_align=\"left\"))\n</pre> # Compare original document with generated summary index = 0  # Change this to view different examples  # Merge preview data with original images for comparison comparison_dataset = preview.dataset.merge(pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\")  # Extract the record for display record = comparison_dataset.iloc[index]  print(\"\ud83d\udcc4 Original Document Image:\") display(resize_image(record.image, BASE64_IMAGE_HEIGHT))  print(\"\\n\ud83d\udcdd Generated Summary:\") rich.print(Panel(record.summary, title=\"Document Summary\", title_align=\"left\")) In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-4\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-4\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"colab_notebooks/4-providing-images-as-context/#data-designer-tutorial-providing-images-as-context-for-vision-based-data-generation","title":"\ud83c\udfa8 Data Designer Tutorial: Providing Images as Context for Vision-Based Data Generation\u00b6","text":""},{"location":"colab_notebooks/4-providing-images-as-context/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>This notebook demonstrates how to provide images as context to generate text descriptions using vision-language models.</p> <ul> <li>\u2728 Visual Document Processing: Converting images to chat-ready format for model consumption</li> <li>\ud83d\udd0d Vision-Language Generation: Using vision models to generate detailed summaries from images</li> </ul> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"colab_notebooks/4-providing-images-as-context/#colab-setup","title":"\u26a1 Colab Setup\u00b6","text":"<p>Run the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from build.nvidia.com.</p>"},{"location":"colab_notebooks/4-providing-images-as-context/#import-the-essentials","title":"\ud83d\udce6 Import the essentials\u00b6","text":"<ul> <li>The <code>essentials</code> module provides quick access to the most commonly used objects.</li> </ul>"},{"location":"colab_notebooks/4-providing-images-as-context/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object is responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"colab_notebooks/4-providing-images-as-context/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"colab_notebooks/4-providing-images-as-context/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"colab_notebooks/4-providing-images-as-context/#seed-dataset-creation","title":"\ud83c\udf31 Seed Dataset Creation\u00b6","text":"<p>In this section, we'll prepare our visual documents as a seed dataset for summarization:</p> <ul> <li>Loading Visual Documents: We use the ColPali dataset containing document images</li> <li>Image Processing: Convert images to base64 format for vision model consumption</li> <li>Metadata Extraction: Preserve relevant document information (filename, page number, source, etc.)</li> </ul> <p>The seed dataset will be used to generate detailed text summaries of each document image.</p>"},{"location":"colab_notebooks/4-providing-images-as-context/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013 preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"colab_notebooks/4-providing-images-as-context/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"colab_notebooks/4-providing-images-as-context/#visual-inspection","title":"\ud83d\udd0e Visual Inspection\u00b6","text":"<p>Let's compare the original document image with the generated summary to validate quality:</p>"},{"location":"colab_notebooks/4-providing-images-as-context/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"colab_notebooks/4-providing-images-as-context/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Now that you've learned how to use visual context for image summarization in Data Designer, explore more:</p> <ul> <li>Experiment with different vision models for specific document types</li> <li>Try different prompt variations to generate specialized descriptions (e.g., technical details, key findings)</li> <li>Combine vision-based summaries with other column types for multi-modal workflows</li> <li>Apply this pattern to other vision tasks like image captioning, OCR validation, or visual question answering</li> </ul>"},{"location":"concepts/columns/","title":"Columns","text":"<p>Columns are the fundamental building blocks in Data Designer. Each column represents a field in your dataset and defines how to generate it\u2014whether that's sampling from a distribution, calling an LLM, or applying a transformation.</p> <p>The Declarative Approach</p> <p>Columns are declarative specifications. You describe what you want, and the framework handles how to generate it\u2014managing execution order, batching, parallelization, and resources automatically.</p>"},{"location":"concepts/columns/#column-types","title":"Column Types","text":"<p>Data Designer provides nine built-in column types, each optimized for different generation scenarios.</p>"},{"location":"concepts/columns/#sampler-columns","title":"\ud83c\udfb2 Sampler Columns","text":"<p>Sampler columns generate data using numerical sampling\u2014fast, deterministic, and ideal for numerical and categorical dataset fields. They're significantly faster than LLMs and can produce data following specific distributions (Poisson for event counts, Gaussian for measurements, etc.).</p> <p>Available sampler types:</p> <ul> <li>UUID: Unique identifiers</li> <li>Category: Categorical values with optional probability weights</li> <li>Subcategory: Hierarchical categorical data (states within countries, models within brands)</li> <li>Uniform: Evenly distributed numbers (integers or floats)</li> <li>Gaussian: Normally distributed values with configurable mean and standard deviation</li> <li>Bernoulli: Binary outcomes with specified success probability</li> <li>Bernoulli Mixture: Binary outcomes from multiple probability components</li> <li>Binomial: Count of successes in repeated trials</li> <li>Poisson: Count data and event frequencies</li> <li>Scipy: Access to the full scipy.stats distribution library</li> <li>Person: Realistic synthetic individuals with names, demographics, and attributes</li> <li>Datetime: Timestamps within specified ranges</li> <li>Timedelta: Time duration values</li> </ul> <p>Conditional Sampling</p> <p>Samplers support conditional parameters that change behavior based on other columns. Want age distributions that vary by country? Income ranges that depend on occupation? Just define conditions on existing column values.</p>"},{"location":"concepts/columns/#llm-text-columns","title":"\ud83d\udcdd LLM-Text Columns","text":"<p>LLM-Text columns generate natural language text: product descriptions, customer reviews, narrative summaries, email threads, or anything requiring semantic understanding and creativity.</p> <p>Use Jinja2 templating in prompts to reference other columns. Data Designer automatically manages dependencies and injects the referenced column values into the prompt.</p> <p>Reasoning Traces</p> <p>Models that support extended thinking (chain-of-thought reasoning) can capture their reasoning process in a separate <code>{column_name}__reasoning_trace</code> column\u2013useful for understanding why the model generated specific content. This column is automatically added to the dataset if the model and service provider parse and return reasoning content.</p>"},{"location":"concepts/columns/#llm-code-columns","title":"\ud83d\udcbb LLM-Code Columns","text":"<p>LLM-Code columns generate code in specific programming languages. They handle the prompting and parsing necessary to extract clean code from the LLM's response\u2014automatically detecting and extracting code from markdown blocks. You provide the prompt and choose the model; the column handles the extraction.</p> <p>Supported languages: Python, JavaScript, TypeScript, Java, Kotlin, Go, Rust, Ruby, Scala, Swift, plus SQL dialects (SQLite, PostgreSQL, MySQL, T-SQL, BigQuery, ANSI SQL).</p>"},{"location":"concepts/columns/#llm-structured-columns","title":"\ud83d\uddc2\ufe0f LLM-Structured Columns","text":"<p>LLM-Structured columns generate JSON with a guaranteed schema. Define your structure using a Pydantic model or JSON schema, and Data Designer ensures the LLM output conforms\u2014no parsing errors, no schema drift.</p> <p>Use for complex nested structures: API responses, configuration files, database records with multiple related fields, or any structured data where type safety matters. Schemas can be arbitrarily complex with nested objects, arrays, enums, and validation constraints, but success depends on the model's capabilities.</p> <p>Schema Complexity and Model Choice</p> <p>Flat schemas with simple fields are easier and more robustly produced across models. Deeply nested schemas with complex validation constraints are more sensitive to model choice\u2014stronger models handle complexity better. If you're experiencing schema conformance issues, try simplifying the schema or switching to a more capable model.</p>"},{"location":"concepts/columns/#llm-judge-columns","title":"\u2696\ufe0f LLM-Judge Columns","text":"<p>LLM-Judge columns score generated content across multiple quality dimensions using LLMs as evaluators.</p> <p>Define scoring rubrics (relevance, accuracy, fluency, helpfulness) and the judge model evaluates each record. Score rubrics specify criteria and scoring options (1-5 scales, categorical grades, etc.), producing quantified quality metrics for every data point.</p> <p>Use judge columns for data quality filtering (e.g., keep only 4+ rated responses), A/B testing generation strategies, and quality monitoring over time.</p>"},{"location":"concepts/columns/#embedding-columns","title":"\ud83e\uddec Embedding Columns","text":"<p>Embedding columns generate vector embeddings (numerical representations) for text content using embedding models. These embeddings capture semantic meaning, enabling similarity search, clustering, and semantic analysis.</p> <p>Specify a <code>target_column</code> containing text, and Data Designer generates embeddings for that content. The target column can contain either a single text string or a list of text strings in stringified JSON format. In the latter case, embeddings are generated for each text string in the list.</p> <p>Common use cases:</p> <ul> <li>Semantic search: Generate embeddings for documents, then find similar content by vector similarity</li> <li>Clustering: Group similar texts based on embedding proximity</li> <li>Recommendation systems: Match content by semantic similarity</li> <li>Anomaly detection: Identify outliers in embedding space</li> </ul> <p>Embedding Models</p> <p>Embedding columns require an embedding model configured with <code>EmbeddingInferenceParams</code>. These models differ from chat completion models\u2014they output vectors rather than text. The generation type is automatically determined by the inference parameters type.</p>"},{"location":"concepts/columns/#expression-columns","title":"\ud83e\udde9 Expression Columns","text":"<p>Expression columns handle simple transformations using Jinja2 templates\u2014concatenate first and last names, calculate numerical totals, format date strings. No LLM overhead needed.</p> <p>Template capabilities:</p> <ul> <li>Variable substitution: Pull values from any existing column</li> <li>String filters: Uppercase, lowercase, strip whitespace, replace patterns</li> <li>Conditional logic: if/elif/else support</li> <li>Arithmetic: Add, subtract, multiply, divide</li> </ul>"},{"location":"concepts/columns/#validation-columns","title":"\ud83d\udd0d Validation Columns","text":"<p>Validation columns check generated content against rules and return structured pass/fail results.</p> <p>Built-in validation types:</p> <p>Code validation runs Python or SQL code through a linter to validate the code.</p> <p>Local callable validation accepts a Python function directly when using Data Designer as a library.</p> <p>Remote validation sends data to HTTP endpoints for validation-as-a-service. Useful for linters, security scanners, or proprietary systems.</p>"},{"location":"concepts/columns/#seed-dataset-columns","title":"\ud83c\udf31 Seed Dataset Columns","text":"<p>Seed dataset columns bootstrap generation from existing data. Provide a real dataset, and those columns become available as context for generating new synthetic data.</p> <p>Typical pattern: use seed data for one part of your schema (real product names and categories), then generate synthetic fields around it (customer reviews, purchase histories, ratings). The seed data provides realism and constraints; generated columns add volume and variation.</p>"},{"location":"concepts/columns/#shared-column-properties","title":"Shared Column Properties","text":"<p>Every column configuration inherits from <code>SingleColumnConfig</code> with these standard properties:</p>"},{"location":"concepts/columns/#name","title":"<code>name</code>","text":"<p>The column's identifier\u2014unique within your configuration, used in Jinja2 references, and becomes the column name in the output DataFrame. Choose descriptive names: <code>user_review</code> &gt; <code>col_17</code>.</p>"},{"location":"concepts/columns/#drop","title":"<code>drop</code>","text":"<p>Boolean flag (default: <code>False</code>) controlling whether the column appears in final output. Setting <code>drop=True</code> generates the column (available as a dependency) but excludes it from final output.</p> <p>When to drop columns:</p> <ul> <li>Intermediate calculations that feed expressions but aren't meaningful standalone</li> <li>Context columns used only for LLM prompt templates</li> <li>Validation results during development unwanted in production</li> </ul> <p>Dropped columns participate fully in generation and the dependency graph\u2014just filtered out at the end.</p>"},{"location":"concepts/columns/#column_type","title":"<code>column_type</code>","text":"<p>Literal string identifying the column type: <code>\"sampler\"</code>, <code>\"llm-text\"</code>, <code>\"expression\"</code>, etc. Set automatically by each configuration class and serves as Pydantic's discriminator for deserialization.</p> <p>You rarely set this manually\u2014instantiating <code>LLMTextColumnConfig</code> automatically sets <code>column_type=\"llm-text\"</code>. Serialization is reversible: save to YAML, load later, and Pydantic reconstructs the exact objects.</p>"},{"location":"concepts/columns/#required_columns","title":"<code>required_columns</code>","text":"<p>Computed property listing columns that must be generated before this one. The framework derives this automatically:</p> <ul> <li>For LLM/Expression columns: extracted from Jinja2 template <code>{{ variables }}</code></li> <li>For Validation columns: explicitly listed target columns</li> <li>For Sampler columns with conditional parameters: columns referenced in conditions</li> </ul> <p>You read this property for introspection but never set it\u2014always computed from configuration details.</p>"},{"location":"concepts/columns/#side_effect_columns","title":"<code>side_effect_columns</code>","text":"<p>Computed property listing columns created implicitly alongside the primary column. Currently, only LLM columns produce side effects (reasoning trace columns like <code>{name}__reasoning_trace</code> when models use extended thinking).</p> <p>For detailed information on each column type, refer to the column configuration code reference.</p>"},{"location":"concepts/person_sampling/","title":"Person Sampling in Data Designer","text":"<p>Person sampling in Data Designer allows you to generate synthetic person data for your datasets. There are two distinct approaches, each with different capabilities and use cases.</p>"},{"location":"concepts/person_sampling/#overview","title":"Overview","text":"<p>Data Designer provides two ways to generate synthetic people:</p> <ol> <li>Faker-based sampling - Quick, basic PII generation for testing or when realistic demographic distributions are not relevant for your use case</li> <li>Nemotron-Personas datasets - Demographically accurate, rich persona data</li> </ol>"},{"location":"concepts/person_sampling/#approach-1-faker-based-sampling","title":"Approach 1: Faker-Based Sampling","text":""},{"location":"concepts/person_sampling/#what-it-does","title":"What It Does","text":"<p>Uses the Faker library to generate random personal information. The data is basic and not demographically accurate, but is useful for quick testing, prototyping, or when realistic demographic distributions are not relevant for your use case.</p>"},{"location":"concepts/person_sampling/#features","title":"Features","text":"<ul> <li>Gives you access to person attributes that Faker exposes</li> <li>Quick to set up with no additional downloads</li> <li>Generates random names, emails, addresses, phone numbers, etc.</li> <li>Supports all Faker-supported locales</li> <li>Not demographically grounded - data patterns don't reflect real-world demographics</li> </ul>"},{"location":"concepts/person_sampling/#usage-example","title":"Usage Example","text":"<pre><code>from data_designer.essentials import (\n    SamplerColumnConfig,\n    SamplerType,\n    PersonFromFakerSamplerParams,\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=SamplerType.PERSON_FROM_FAKER,\n        params=PersonFromFakerSamplerParams(\n            locale=\"en_US\",\n            age_range=[25, 65],\n            sex=\"Female\",\n        ),\n    )\n)\n</code></pre> <p>For mor details, see the documentation for <code>SamplerColumnConfig</code> and <code>PersonFromFakerSamplerParams</code>.</p>"},{"location":"concepts/person_sampling/#approach-2-nemotron-personas-datasets","title":"Approach 2: Nemotron-Personas Datasets","text":""},{"location":"concepts/person_sampling/#what-it-does_1","title":"What It Does","text":"<p>Uses curated Nemotron-Personas datasets from NVIDIA GPU Cloud (NGC) to generate demographically accurate person data with rich personality profiles and behavioral characteristics.</p> <p>The NGC datasets are extended versions of the open-source Nemotron-Personas datasets on HuggingFace, with additional fields and enhanced data quality.</p> <p>Supported locales:</p> <ul> <li><code>en_US</code>: United States</li> <li><code>ja_JP</code>: Japan</li> <li><code>en_IN</code>: India</li> <li><code>hi_Deva_IN</code>: India (Devanagari script)</li> <li><code>hi_Latn_IN</code>: India (Latin script)</li> </ul>"},{"location":"concepts/person_sampling/#features_1","title":"Features","text":"<ul> <li>Demographically accurate personal details: Names, ages, sex, marital status, education, occupation based on census data</li> <li>Rich persona details: Comprehensive behavioral profiles including:</li> <li>Big Five personality traits with scores</li> <li>Cultural backgrounds and narratives</li> <li>Skills and hobbies</li> <li>Career goals and aspirations</li> <li>Context-specific personas (professional, financial, healthcare, sports, arts, travel, culinary, etc.)</li> <li>Consistent, referenceable attributes across your dataset</li> <li>Grounded in real-world demographic distributions</li> </ul>"},{"location":"concepts/person_sampling/#prerequisites","title":"Prerequisites","text":"<p>To use the extended Nemotron-Personas datasets with Data Designer, you need to download them from NGC and move them to the Data Designer managed assets directory.</p> <p>See below for step-by-step instructions.</p>"},{"location":"concepts/person_sampling/#nemotron-personas-datasets-setup-instructions","title":"Nemotron-Personas Datasets Setup Instructions","text":""},{"location":"concepts/person_sampling/#step-0-obtain-an-ngc-api-key-and-install-the-ngc-cli","title":"Step 0: Obtain an NGC API Key and install the NGC CLI","text":"<p>To download the Nemotron-Personas datasets from NGC, you will need to obtain an NGC API key and install the NGC CLI.</p> <ol> <li>NGC API Key: Obtain from NVIDIA GPU Cloud</li> <li>NGC CLI: NGC CLI</li> </ol>"},{"location":"concepts/person_sampling/#step-1-set-your-ngc-api-key","title":"Step 1: Set Your NGC API Key","text":"<pre><code>export NGC_API_KEY=\"your-ngc-api-key-here\"\n</code></pre>"},{"location":"concepts/person_sampling/#step-2-option-1-download-nemotron-personas-datasets-via-the-data-designer-cli","title":"Step 2 (option 1): Download Nemotron-Personas Datasets via the Data Designer CLI","text":"<p>Once you have the NGC CLI and your NGC API key set up, you can download the datasets via the Data Designer CLI.</p> <p>You can pass the locales you want to download as arguments to the CLI command: <pre><code>data-designer download personas --locale en_US --locale ja_JP\n</code></pre></p> <p>Or you can use the interactive mode to select the locales you want to download: <pre><code>data-designer download personas\n</code></pre></p>"},{"location":"concepts/person_sampling/#step-2-option-2-download-nemotron-personas-datasets-directly","title":"Step 2 (option 2): Download Nemotron-Personas Datasets Directly","text":"<p>Use the NGC CLI to download the datasets: <pre><code># For Nemotron-Personas USA\nngc registry resource download-version \"nvidia/nemotron-personas/nemotron-personas-dataset-en_us\"\n\n# For Nemotron-Personas IN\nngc registry resource download-version \"nvidia/nemotron-personas/nemotron-personas-dataset-hi_deva_in\"\nngc registry resource download-version \"nvidia/nemotron-personas/nemotron-personas-dataset-hi_latn_in\"\nngc registry resource download-version \"nvidia/nemotron-personas/nemotron-personas-dataset-en_in\"\n\n# For Nemotron-Personas JP\nngc registry resource download-version \"nvidia/nemotron-personas/nemotron-personas-dataset-ja_jp\"\n</code></pre></p> <p>Then move the downloaded dataset to the Data Designer managed assets directory: <pre><code>mkdir -p ~/.data-designer/managed-assets/datasets/\nmv nemotron-personas-dataset-*/*.parquet ~/.data-designer/managed-assets/datasets/\n</code></pre></p>"},{"location":"concepts/person_sampling/#step-3-use-personsampler-in-your-code","title":"Step 3: Use PersonSampler in Your Code","text":"<pre><code>from data_designer.essentials import (\n    SamplerColumnConfig,\n    SamplerType,\n    PersonSamplerParams,\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=SamplerType.PERSON,\n        params=PersonSamplerParams(\n            locale=\"en_US\",\n            sex=\"Female\",\n            age_range=[25, 45],\n            with_synthetic_personas=True,\n        ),\n    )\n)\n</code></pre> <p>For more details, see the documentation for <code>SamplerColumnConfig</code> and <code>PersonSamplerParams</code>.</p>"},{"location":"concepts/person_sampling/#available-data-fields","title":"Available Data Fields","text":"<p>Core Fields (all locales):</p> Field Type Notes <code>uuid</code> UUID Unique identifier <code>first_name</code> string <code>middle_name</code> string <code>last_name</code> string <code>sex</code> enum \"Male\" or \"Female\" <code>birth_date</code> date Derived: year, month, day <code>street_number</code> int <code>street_name</code> string <code>unit</code> string Address line 2 <code>city</code> string <code>region</code> string Alias: state <code>district</code> string Alias: county <code>postcode</code> string Alias: zipcode <code>country</code> string <code>phone_number</code> PhoneNumber Derived: area_code, country_code, prefix, line_number <code>marital_status</code> string Values: never_married, married_present, separated, widowed, divorced <code>education_level</code> string or None <code>bachelors_field</code> string or None <code>occupation</code> string or None <code>email_address</code> string <code>national_id</code> string <p>Japan-Specific Fields (<code>ja_JP</code>):</p> <ul> <li><code>area</code></li> </ul> <p>India-Specific Fields (<code>en_IN</code>, <code>hi_IN</code>, <code>hi_Deva_IN</code>, <code>hi_Latn_IN</code>):</p> <ul> <li><code>religion</code> - Census-reported religion</li> <li><code>education_degree</code> - Census-reported education degree</li> <li><code>first_language</code> - Native language</li> <li><code>second_language</code> - Second language (if applicable)</li> <li><code>third_language</code> - Third language (if applicable)</li> <li><code>zone</code> - Urban vs rural</li> </ul> <p>With Synthetic Personas Enabled:</p> <ul> <li>Big Five personality traits (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) with t-scores and labels</li> <li>Cultural background narratives</li> <li>Skills and competencies</li> <li>Hobbies and interests</li> <li>Career goals</li> <li>Context-specific personas (professional, financial, healthcare, sports, arts &amp; entertainment, travel, culinary, etc.)</li> </ul>"},{"location":"concepts/person_sampling/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Description <code>locale</code> str Language/region code - must be one of: \"en_US\", \"ja_JP\", \"en_IN\", \"hi_Deva_IN\", \"hi_Latn_IN\" <code>sex</code> str (optional) Filter by \"Male\" or \"Female\" <code>city</code> str or list[str] (optional) Filter by specific city or cities within locale <code>age_range</code> list[int] (optional) Two-element list [min_age, max_age] (default: [18, 114]) <code>with_synthetic_personas</code> bool (optional) Include rich personality profiles (default: False) <code>select_field_values</code> dict (optional) Custom field-based filtering (e.g., {\"state\": [\"NY\", \"CA\"], \"education_level\": [\"bachelors\"]})"},{"location":"concepts/processors/","title":"Processors","text":"<p>Processors are transformations that modify your dataset before or after columns are generated. They run at different stages and can reshape, filter, or augment the data.</p> <p>When to Use Processors</p> <p>Processors handle transformations that don't fit the \"column\" model: restructuring the schema for a specific output format, dropping intermediate columns in bulk, or applying batch-wide operations.</p>"},{"location":"concepts/processors/#overview","title":"Overview","text":"<p>Each processor:</p> <ul> <li>Receives the complete batch DataFrame</li> <li>Applies its transformation</li> <li>Passes the result to the next processor (or to output)</li> </ul> <p>Currently, processors run only at the <code>POST_BATCH</code> stage, i.e., after column generation completes for each batch.</p>"},{"location":"concepts/processors/#processor-types","title":"Processor Types","text":""},{"location":"concepts/processors/#drop-columns-processor","title":"\ud83d\uddd1\ufe0f Drop Columns Processor","text":"<p>Removes specified columns from the output dataset. Dropped columns are saved separately in the <code>dropped-columns</code> directory for reference.</p> <p>Dropping Columns is More Easily Achieved via <code>drop = True</code></p> <p>The Drop Columns Processor is different from others in the sense that it does not need to be explicitly added: setting <code>drop = True</code> when configuring a column will accomplish the same.</p> <p>Configuration:</p> <pre><code>from data_designer.essentials import DropColumnsProcessorConfig\n\nprocessor = DropColumnsProcessorConfig(\n    name=\"remove_intermediate\",\n    column_names=[\"temp_calculation\", \"raw_input\", \"debug_info\"],\n)\n</code></pre> <p>Behavior:</p> <ul> <li>Columns specified in <code>column_names</code> are removed from the output</li> <li>Original values are preserved in a separate parquet file</li> <li>Missing columns produce a warning but don't fail the build</li> <li>Column configs are automatically marked with <code>drop=True</code> when this processor is added</li> </ul> <p>Use Cases:</p> <ul> <li>Removing intermediate columns used only for LLM context</li> <li>Cleaning up debug or validation columns before final output</li> <li>Separating sensitive data from the main dataset</li> </ul>"},{"location":"concepts/processors/#schema-transform-processor","title":"\ud83d\udd04 Schema Transform Processor","text":"<p>Creates an additional dataset with a transformed schema using Jinja2 templates. The output is written to a separate directory alongside the main dataset.</p> <p>Configuration:</p> <pre><code>from data_designer.essentials import SchemaTransformProcessorConfig\n\nprocessor = SchemaTransformProcessorConfig(\n    name=\"chat_format\",\n    template={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"{{ question }}\"},\n            {\"role\": \"assistant\", \"content\": \"{{ answer }}\"},\n        ],\n        \"metadata\": \"{{ category | upper }}\",\n    },\n)\n</code></pre> <p>Behavior:</p> <ul> <li>Each key in <code>template</code> becomes a column in the transformed dataset</li> <li>Values are Jinja2 templates with access to all columns in the batch</li> <li>Complex structures (lists, nested dicts) are supported</li> <li>Output is saved to the <code>processors-outputs/{name}/</code> directory</li> <li>The original dataset passes through unchanged</li> </ul> <p>Template Capabilities:</p> <ul> <li>Variable substitution: <code>{{ column_name }}</code></li> <li>Filters: <code>{{ text | upper }}</code>, <code>{{ text | lower }}</code>, <code>{{ text | trim }}</code></li> <li>Nested structures: Arbitrarily deep JSON structures</li> <li>Lists: <code>[\"{{ col1 }}\", \"{{ col2 }}\"]</code></li> </ul> <p>Use Cases:</p> <ul> <li>Converting flat columns to chat message format</li> <li>Restructuring data for specific model training formats</li> <li>Creating derived views without modifying the source dataset</li> </ul>"},{"location":"concepts/processors/#using-processors","title":"Using Processors","text":"<p>Add processors to your configuration using the builder's <code>add_processor</code> method:</p> <pre><code>from data_designer.essentials import (\n    DataDesignerConfigBuilder,\n    DropColumnsProcessorConfig,\n    SchemaTransformProcessorConfig,\n)\n\nbuilder = DataDesignerConfigBuilder()\n\n# ... add columns ...\n\n# Drop intermediate columns\nbuilder.add_processor(\n    DropColumnsProcessorConfig(\n        name=\"cleanup\",\n        column_names=[\"scratch_work\", \"raw_context\"],\n    )\n)\n\n# Transform to chat format\nbuilder.add_processor(\n    SchemaTransformProcessorConfig(\n        name=\"chat_format\",\n        template={\n            \"messages\": [\n                {\"role\": \"user\", \"content\": \"{{ question }}\"},\n                {\"role\": \"assistant\", \"content\": \"{{ answer }}\"},\n            ],\n        },\n    )\n)\n</code></pre>"},{"location":"concepts/processors/#execution-order","title":"Execution Order","text":"<p>Processors execute in the order they're added. Plan accordingly when one processor's output affects another.</p>"},{"location":"concepts/processors/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"concepts/processors/#common-parameters","title":"Common Parameters","text":"Parameter Type Description <code>name</code> str Identifier for the processor, used in output directory names <code>build_stage</code> BuildStage When to run (default: <code>POST_BATCH</code>)"},{"location":"concepts/processors/#dropcolumnsprocessorconfig","title":"DropColumnsProcessorConfig","text":"Parameter Type Description <code>column_names</code> list[str] Columns to remove from output"},{"location":"concepts/processors/#schematransformprocessorconfig","title":"SchemaTransformProcessorConfig","text":"Parameter Type Description <code>template</code> dict[str, Any] Jinja2 template defining the output schema. Must be JSON-serializable."},{"location":"concepts/validators/","title":"Validators","text":"<p>Validators are quality assurance mechanisms in Data Designer that check generated content against rules and return structured pass/fail results. They enable automated verification of data for correctness, code quality, and adherence to specifications.</p> <p>Quality Gates for Generated Data</p> <p>Validators act as quality gates in your generation pipeline. Use them to filter invalid records, score code quality, verify format compliance, or integrate with external validation services.</p>"},{"location":"concepts/validators/#overview","title":"Overview","text":"<p>Validation columns execute validation logic against target columns and produce structured results indicating:</p> <ul> <li><code>is_valid</code>: Boolean pass/fail status</li> <li>Additional metadata: Error messages, scores, severity levels, and custom fields</li> </ul> <p>Validators currently support three execution strategies:</p> <ol> <li>Code validation: Lint and check Python or SQL code using industry-standard tools</li> <li>Local callable validation: Execute custom Python functions for flexible validation logic</li> <li>Remote validation: Send data to HTTP endpoints for external validation services</li> </ol>"},{"location":"concepts/validators/#validator-types","title":"Validator Types","text":""},{"location":"concepts/validators/#python-code-validator","title":"\ud83d\udc0d Python Code Validator","text":"<p>The Python code validator runs generated Python code through Ruff, a fast Python linter that checks for syntax errors, undefined variables, and code quality issues.</p> <p>Configuration:</p> <pre><code>from data_designer.essentials import CodeLang, CodeValidatorParams\n\nvalidator_params = CodeValidatorParams(code_lang=CodeLang.PYTHON)\n</code></pre> <p>Validation Output:</p> <p>Each validated record returns:</p> <ul> <li><code>is_valid</code>: <code>True</code> if no fatal or error-level issues found</li> <li><code>python_linter_score</code>: Quality score from 0-10 (based on pylint formula)</li> <li><code>python_linter_severity</code>: Highest severity level found (<code>\"none\"</code>, <code>\"convention\"</code>, <code>\"refactor\"</code>, <code>\"warning\"</code>, <code>\"error\"</code>, <code>\"fatal\"</code>)</li> <li><code>python_linter_messages</code>: List of linter messages with line numbers, columns, and descriptions</li> </ul> <p>Severity Levels:</p> <ul> <li>Fatal: Syntax errors preventing code execution</li> <li>Error: Undefined names, invalid syntax</li> <li>Warning: Code smells and potential issues</li> <li>Refactor: Simplification opportunities</li> <li>Convention: Style guide violations</li> </ul> <p>A record is marked valid if it has no messages or only messages at warning/convention/refactor levels.</p> <p>Example Validation Result:</p> <pre><code>{\n    \"is_valid\": False,\n    \"python_linter_score\": 0,\n    \"python_linter_severity\": \"error\",\n    \"python_linter_messages\": [\n        {\n            \"type\": \"error\",\n            \"symbol\": \"F821\",\n            \"line\": 1,\n            \"column\": 7,\n            \"message\": \"Undefined name `it`\"\n        }\n    ]\n}\n</code></pre>"},{"location":"concepts/validators/#sql-code-validator","title":"\ud83d\uddc4\ufe0f SQL Code Validator","text":"<p>The SQL code validator uses SQLFluff, a dialect-aware SQL linter that checks query syntax and structure.</p> <p>Configuration:</p> <pre><code>from data_designer.essentials import CodeLang, CodeValidatorParams\n\nvalidator_params = CodeValidatorParams(code_lang=CodeLang.SQL_POSTGRES)\n</code></pre> <p>Multiple Dialects</p> <p>The SQL code validator supports multiple dialects: <code>SQL_POSTGRES</code>, <code>SQL_ANSI</code>, <code>SQL_MYSQL</code>, <code>SQL_SQLITE</code>, <code>SQL_TSQL</code> and <code>SQL_BIGQUERY</code>.</p> <p>Validation Output:</p> <p>Each validated record returns:</p> <ul> <li><code>is_valid</code>: <code>True</code> if no parsing errors found</li> <li><code>error_messages</code>: Concatenated error descriptions (empty string if valid)</li> </ul> <p>The validator focuses on parsing errors (PRS codes) that indicate malformed SQL. It also checks for common pitfalls like <code>DECIMAL</code> definitions without scale parameters.</p> <p>Example Validation Result:</p> <pre><code># Valid SQL\n{\n    \"is_valid\": True,\n    \"error_messages\": \"\"\n}\n\n# Invalid SQL\n{\n    \"is_valid\": False,\n    \"error_messages\": \"PRS: Line 1, Position 1: Found unparsable section: 'NOT SQL'\"\n}\n</code></pre>"},{"location":"concepts/validators/#local-callable-validator","title":"\ud83d\udd27 Local Callable Validator","text":"<p>The local callable validator executes custom Python functions for flexible validation logic.</p> <p>Configuration:</p> <pre><code>import pandas as pd\n\nfrom data_designer.essentials import LocalCallableValidatorParams\n\ndef my_validation_function(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Validate that values are positive.\n\n    Args:\n        df: DataFrame with target columns\n\n    Returns:\n        DataFrame with is_valid column and optional metadata\n    \"\"\"\n    result = pd.DataFrame()\n    result[\"is_valid\"] = df[\"price\"] &gt; 0\n    result[\"error_message\"] = result[\"is_valid\"].apply(\n        lambda valid: \"\" if valid else \"Price must be positive\"\n    )\n    return result\n\nvalidator_params = LocalCallableValidatorParams(\n    validation_function=my_validation_function,\n    output_schema={  # Optional: enforce output schema\n        \"type\": \"object\",\n        \"properties\": {\n            \"data\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"is_valid\": {\"type\": [\"boolean\", \"null\"]},\n                        \"error_message\": {\"type\": \"string\"}\n                    },\n                    \"required\": [\"is_valid\"]\n                }\n            }\n        }\n    }\n)\n</code></pre> <p>Function Requirements:</p> <ul> <li>Input: DataFrame with target columns</li> <li>Output: DataFrame with <code>is_valid</code> column (boolean or null)</li> <li>Extra fields: Any additional columns become validation metadata</li> </ul> <p>The <code>output_schema</code> parameter is optional but recommended\u2014it validates the function's output against a JSON schema, catching unexpected return formats.</p>"},{"location":"concepts/validators/#remote-validator","title":"\ud83c\udf10 Remote Validator","text":"<p>The remote validator sends data to HTTP endpoints for validation-as-a-service. This is useful for when you have validation software that needs to run on external compute and you can expose it through a service. Some examples are:</p> <ul> <li>External linting services</li> <li>Security scanners</li> <li>Domain-specific validators</li> <li>Proprietary validation systems</li> </ul> <p>Authentication</p> <p>Currently, the remote validator is only able to perform unauthenticated API calls. When implementing your own service, you can rely on network isolation for security. If you need to reach a service that requires authentication, you should implement a local proxy.</p> <p>Configuration:</p> <pre><code>from data_designer.essentials import RemoteValidatorParams\n\nvalidator_params = RemoteValidatorParams(\n    endpoint_url=\"https://api.example.com/validate\",\n    timeout=30.0,  # Request timeout in seconds\n    max_retries=3,  # Retry attempts on failure\n    retry_backoff=2.0,  # Exponential backoff factor\n    max_parallel_requests=4,  # Concurrent request limit\n    output_schema={  # Optional: enforce response schema\n        \"type\": \"object\",\n        \"properties\": {\n            \"data\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"is_valid\": {\"type\": [\"boolean\", \"null\"]},\n                        \"confidence\": {\"type\": \"string\"}\n                    }\n                }\n            }\n        }\n    }\n)\n</code></pre> <p>Request Format:</p> <p>The validator sends POST requests with this structure:</p> <pre><code>{\n    \"data\": [\n        {\"column1\": \"value1\", \"column2\": \"value2\"},\n        {\"column1\": \"value3\", \"column2\": \"value4\"}\n    ]\n}\n</code></pre> <p>Expected Response Format:</p> <p>The endpoint must return:</p> <pre><code>{\n    \"data\": [\n        {\n            \"is_valid\": true,\n            \"custom_field\": \"any additional metadata\"\n        },\n        {\n            \"is_valid\": false,\n            \"custom_field\": \"more metadata\"\n        }\n    ]\n}\n</code></pre> <p>Retry Behavior:</p> <p>The validator automatically retries on:</p> <ul> <li>Network errors</li> <li>HTTP status codes: 429 (rate limit), 500, 502, 503, 504</li> </ul> <p>Failed requests use exponential backoff: <code>delay = retry_backoff^attempt</code>.</p> <p>Parallelization:</p> <p>Set <code>max_parallel_requests</code> to control concurrency. Higher values improve throughput but increase server load. The validator batches requests according to the <code>batch_size</code> parameter in the validation column configuration.</p>"},{"location":"concepts/validators/#using-validators-in-columns","title":"Using Validators in Columns","text":"<p>Add validation columns to your configuration using the builder's <code>add_column</code> method:</p> <pre><code>from data_designer.essentials import (\n    CodeValidatorParams,\n    CodeLang,\n    DataDesignerConfigBuilder,\n    LLMCodeColumnConfig,\n    ValidationColumnConfig,\n)\n\nbuilder = DataDesignerConfigBuilder()\n\n# Generate Python code\nbuilder.add_column(\n    LLMCodeColumnConfig(\n        name=\"sorting_algorithm\",\n        prompt=\"Write a Python function to sort a list using bubble sort.\",\n        code_lang=\"python\",\n        model_alias=\"my-model\"\n    )\n)\n\n# Validate the generated code\nbuilder.add_column(\n    ValidationColumnConfig(\n        name=\"code_validation\",\n        target_columns=[\"sorting_algorithm\"],\n        validator_type=\"code\",\n        validator_params=CodeValidatorParams(code_lang=CodeLang.PYTHON),\n        batch_size=10,\n        drop=False,\n    )\n)\n</code></pre> <p>The <code>target_columns</code> parameter specifies which columns to validate. All target columns are passed to the validator together (except for code validators, which process each column separately).</p>"},{"location":"concepts/validators/#configuration-parameters","title":"Configuration Parameters","text":"<p>See more about parameters used to instantiate <code>ValidationColumnConfig</code> in the code reference.</p>"},{"location":"concepts/validators/#batch-size-considerations","title":"Batch Size Considerations","text":"<p>Larger batch sizes improve efficiency but consume more memory:</p> <ul> <li>Code validators: 5-20 records (file I/O overhead)</li> <li>Local callable: 10-50 records (depends on function complexity)</li> <li>Remote validators: 1-10 records (network latency, server capacity)</li> </ul> <p>Adjust based on:</p> <ul> <li>Validator computational cost</li> <li>Available memory</li> <li>Network bandwidth (for remote validators)</li> <li>Server rate limits</li> </ul> <p>If the validation logic uses information from other samples, only samples in the batch will be considered.</p>"},{"location":"concepts/validators/#multiple-column-validation","title":"Multiple Column Validation","text":"<p>Validate multiple columns simultaneously:</p> <pre><code>from data_designer.essentials import RemoteValidatorParams, ValidationColumnConfig\n\nbuilder.add_column(\n    ValidationColumnConfig(\n        name=\"multi_column_validation\",\n        target_columns=[\"column_a\", \"column_b\", \"column_c\"],\n        validator_type=\"remote\",\n        validator_params=RemoteValidatorParams(\n            endpoint_url=\"https://api.example.com/validate\"\n        )\n    )\n)\n</code></pre> <p>Note: Code validators always process each target column separately, even when multiple columns are specified. Local callable and remote validators receive all target columns together.</p>"},{"location":"concepts/validators/#see-also","title":"See Also","text":"<ul> <li>Validator Parameters Reference: Configuration object schemas</li> </ul>"},{"location":"concepts/models/configure-model-settings-with-the-cli/","title":"Configuring Model Settings Using The CLI","text":"<p>The Data Designer CLI provides an interactive interface for creating and managing default model providers and model configurations stored in your Data Designer home directory (default: <code>~/.data-designer/</code>).</p>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#configuration-files","title":"Configuration Files","text":"<p>The CLI manages two YAML configuration files:</p> <ul> <li><code>model_providers.yaml</code>: Model provider configurations</li> <li><code>model_configs.yaml</code>: Model configurations</li> </ul> <p>Automatic Configuration</p> <p>If these configuration files don't already exist, the Data Designer library automatically creates them with default settings at runtime when first initialized.</p> <p>Custom Directory</p> <p>You can customize the configuration directory location with the <code>DATA_DESIGNER_HOME</code> environment variable: <pre><code>export DATA_DESIGNER_HOME=\"/path/to/your/custom/directory\"\n</code></pre></p>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#cli-commands","title":"CLI Commands","text":"<p>The Data Designer CLI provides four main configuration commands:</p> <pre><code># Configure model providers\ndata-designer config providers\n\n# Configure models\ndata-designer config models\n\n# List current configurations\ndata-designer config list\n\n# Reset all configurations\ndata-designer config reset\n</code></pre> <p>Getting help</p> <p>See available commands <pre><code>data-designer --help\n</code></pre></p> <p>See available sub-commands <pre><code>data-designer config --help\n</code></pre></p>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#managing-model-providers","title":"Managing Model Providers","text":"<p>Run the interactive provider configuration command:</p> <pre><code>data-designer config providers\n</code></pre>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#available-operations","title":"Available Operations","text":"<p>Add a new provider: Define a new provider by entering its name, endpoint URL, provider type, and optionally an API key (as plain text or as an environment variable name).</p> <p>Update an existing provider: Modify an existing provider's settings. All fields are pre-filled with current values.</p> <p>Delete a provider: Remove a provider and its associated models.</p> <p>Delete all providers: Remove all providers and their associated models.</p> <p>Change default provider: Set which provider is used by default. This option is only available when multiple providers are configured.</p>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#managing-model-configurations","title":"Managing Model Configurations","text":"<p>Run the interactive model configuration command:</p> <pre><code>data-designer config models\n</code></pre> <p>Provider Required</p> <p>You need at least one provider configured before adding models. Run <code>data-designer config providers</code> first if none exist.</p>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#available-operations_1","title":"Available Operations","text":"<p>Add a new model configuration</p> <p>Create a new model configuration with the following fields:</p> <ul> <li>Alias: A unique name for referencing this model in a column configuration.</li> <li>Model ID: The model identifier (e.g., <code>nvidia/nemotron-3-nano-30b-a3b</code>)</li> <li>Provider: Select from available providers (if multiple exist)</li> <li>Temperature: Sampling temperature (0.0 to 2.0)</li> <li>Top P: Nucleus sampling parameter (0.0 to 1.0)</li> <li>Max Tokens: Maximum output length (1 to 100000)</li> </ul> <p>Additional Settings</p> <p>To configure additional inference parameter settings or use distribution-based inference parameters, edit the <code>model_configs.yaml</code> file directly.</p> <p>Update an existing model configuration: Modify an existing model's configuration. All fields are pre-filled with current values.</p> <p>Delete a model configuration: Remove a single model configuration.</p> <p>Delete all model configurations: Remove all model configurations. The CLI will ask for confirmation before proceeding.</p>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#listing-configurations","title":"Listing Configurations","text":"<p>View all current configurations:</p> <pre><code>data-designer config list\n</code></pre> <p>This command displays:</p> <ul> <li>Model Providers: All configured providers with their endpoints (API keys are masked)</li> <li>Default Provider: The currently selected default provider</li> <li>Model Configurations: All configured models with their settings</li> </ul>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#resetting-configurations","title":"Resetting Configurations","text":"<p>Delete all configuration files:</p> <pre><code>data-designer config reset\n</code></pre> <p>The CLI will show which configuration files exist and ask for confirmation before deleting them.</p> <p>Destructive Operation</p> <p>This command permanently deletes all configuration files and resets to the default model providers and configurations. You'll need to reconfigure your custom configurations from scratch.</p>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#see-also","title":"See Also","text":"<ul> <li>Default Model Settings: Pre-configured providers and model settings included with Data Designer</li> <li>Custom Model Settings: Learn how to create custom providers and model configurations</li> <li>Model Providers: Learn about the <code>ModelProvider</code> class and provider configuration</li> <li>Model Configurations: Learn about <code>ModelConfig</code></li> <li>Quick Start Guide: Get started with a simple example</li> </ul>"},{"location":"concepts/models/custom-model-settings/","title":"Custom Model Settings","text":"<p>While Data Designer ships with pre-configured model providers and configurations, you can create custom configurations to use different models, adjust inference parameters, or connect to custom API endpoints.</p>"},{"location":"concepts/models/custom-model-settings/#when-to-use-custom-settings","title":"When to Use Custom Settings","text":"<p>Use custom model settings when you need to:</p> <ul> <li>Use models not included in the defaults</li> <li>Adjust inference parameters (temperature, top_p, max_tokens) for specific use cases</li> <li>Add distribution-based inference parameters for variability</li> <li>Connect to self-hosted or custom model endpoints</li> <li>Create multiple variants of the same model with different settings</li> </ul>"},{"location":"concepts/models/custom-model-settings/#creating-and-using-custom-settings","title":"Creating and Using Custom Settings","text":""},{"location":"concepts/models/custom-model-settings/#custom-models-with-default-providers","title":"Custom Models with Default Providers","text":"<p>Create custom model configurations that use the default providers (no need to define providers yourself):</p> <pre><code>from data_designer.essentials import (\n    CategorySamplerParams,\n    ChatCompletionInferenceParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    LLMTextColumnConfig,\n    ModelConfig,\n    SamplerColumnConfig,\n    SamplerType,\n)\n\n# Create custom models using default providers\ncustom_models = [\n    # High-temperature for more variability\n    ModelConfig(\n        alias=\"creative-writer\",\n        model=\"nvidia/nemotron-3-nano-30b-a3b\",\n        provider=\"nvidia\",  # Uses default NVIDIA provider\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=1.2,\n            top_p=0.98,\n            max_tokens=4096,\n        ),\n    ),\n    # Low-temperature for less variability\n    ModelConfig(\n        alias=\"fact-checker\",\n        model=\"nvidia/nemotron-3-nano-30b-a3b\",\n        provider=\"nvidia\",  # Uses default NVIDIA provider\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=0.1,\n            top_p=0.9,\n            max_tokens=2048,\n        ),\n    ),\n]\n\n# Create DataDesigner (uses default providers)\ndata_designer = DataDesigner()\n\n# Pass custom models to config builder\nconfig_builder = DataDesignerConfigBuilder(model_configs=custom_models)\n\n# Add a topic column using a categorical sampler\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"topic\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\"Artificial Intelligence\", \"Space Exploration\", \"Ancient History\", \"Climate Science\"],\n        ),\n    )\n)\n\n# Use your custom models\nconfig_builder.add_column(\n    LLMTextColumnConfig(\n        name=\"creative_story\",\n        model_alias=\"creative-writer\",\n        prompt=\"Write a creative short story about {{topic}}.\",\n    )\n)\n\nconfig_builder.add_column(\n    LLMTextColumnConfig(\n        name=\"facts\",\n        model_alias=\"fact-checker\",\n        prompt=\"List 3 facts about {{topic}}.\",\n    )\n)\n\n# Preview your dataset\npreview_result = data_designer.preview(config_builder=config_builder)\npreview_result.display_sample_record()\n</code></pre> <p>Default Providers Always Available</p> <p>When you only specify <code>model_configs</code>, the default model providers (NVIDIA, OpenAI, and OpenRouter) are still available. You only need to create custom providers if you want to connect to different endpoints or modify provider settings.</p> <p>Mixing Custom and Default Models</p> <p>When you provide custom <code>model_configs</code> to <code>DataDesignerConfigBuilder</code>, they replace the defaults entirely. To use custom model configs in addition to the default configs, use the add_model_config method:</p> <pre><code># Load defaults first\nconfig_builder = DataDesignerConfigBuilder()\n\n# Add custom model to defaults\nconfig_builder.add_model_config(\n    ModelConfig(\n        alias=\"my-custom-model\",\n        model=\"nvidia/llama-3.3-nemotron-super-49b-v1.5\",\n        provider=\"nvidia\",  # Uses default provider\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=0.6,\n            max_tokens=8192,\n        ),\n    )\n)\n\n# Now you can use both default and custom models\n# Default: nvidia-text, nvidia-reasoning, nvidia-vision, etc.\n# Custom: my-custom-model\n</code></pre>"},{"location":"concepts/models/custom-model-settings/#custom-providers-with-custom-models","title":"Custom Providers with Custom Models","text":"<p>Define both custom providers and custom model configurations when you need to connect to services not included in the defaults:</p> <p>Network Accessibility</p> <p>The custom provider endpoints must be reachable from where Data Designer runs. Ensure network connectivity, firewall rules, and any VPN requirements are properly configured.</p> <pre><code>from data_designer.essentials import (\n    CategorySamplerParams,\n    ChatCompletionInferenceParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    LLMTextColumnConfig,\n    ModelConfig,\n    ModelProvider,\n    SamplerColumnConfig,\n    SamplerType,\n)\n\n# Step 1: Define custom providers\ncustom_providers = [\n    ModelProvider(\n        name=\"my-custom-provider\",\n        endpoint=\"https://api.my-llm-service.com/v1\",\n        provider_type=\"openai\",  # OpenAI-compatible API\n        api_key=\"MY_SERVICE_API_KEY\",  # Environment variable name\n    ),\n    ModelProvider(\n        name=\"my-self-hosted-provider\",\n        endpoint=\"https://my-org.internal.com/llm/v1\",\n        provider_type=\"openai\",\n        api_key=\"SELF_HOSTED_API_KEY\",\n    ),\n]\n\n# Step 2: Define custom models\ncustom_models = [\n    ModelConfig(\n        alias=\"my-text-model\",\n        model=\"openai/some-model-id\",\n        provider=\"my-custom-provider\",  # References provider by name\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=0.85,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n    ModelConfig(\n        alias=\"my-self-hosted-text-model\",\n        model=\"openai/some-hosted-model-id\",\n        provider=\"my-self-hosted-provider\",\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=0.7,\n            top_p=0.9,\n            max_tokens=1024,\n        ),\n    ),\n]\n\n# Step 3: Create DataDesigner with custom providers\ndata_designer = DataDesigner(model_providers=custom_providers)\n\n# Step 4: Create config builder with custom models\nconfig_builder = DataDesignerConfigBuilder(model_configs=custom_models)\n\n# Step 5: Add a topic column using a categorical sampler\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"topic\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\"Technology\", \"Healthcare\", \"Finance\", \"Education\"],\n        ),\n    )\n)\n\n# Step 6: Use your custom model by referencing its alias\nconfig_builder.add_column(\n    LLMTextColumnConfig(\n        name=\"short_news_article\",\n        model_alias=\"my-text-model\",  # Reference custom alias\n        prompt=\"Write a short news article about the '{{topic}}' topic in 10 sentences.\",\n    )\n)\n\nconfig_builder.add_column(\n    LLMTextColumnConfig(\n        name=\"long_news_article\",\n        model_alias=\"my-self-hosted-text-model\",  # Reference custom alias\n        prompt=\"Write a detailed news article about the '{{topic}}' topic.\",\n    )\n)\n\n# Step 7: Preview your dataset\npreview_result = data_designer.preview(config_builder=config_builder)\npreview_result.display_sample_record()\n</code></pre>"},{"location":"concepts/models/custom-model-settings/#see-also","title":"See Also","text":"<ul> <li>Default Model Settings: Pre-configured providers and model settings</li> <li>Configure Model Settings With the CLI: CLI-based configuration</li> <li>Quick Start Guide: Basic usage example</li> </ul>"},{"location":"concepts/models/default-model-settings/","title":"Default Model Settings","text":"<p>Data Designer ships with pre-configured model providers and model configurations that make it easy to start generating synthetic data without manual setup.</p>"},{"location":"concepts/models/default-model-settings/#model-providers","title":"Model Providers","text":"<p>Data Designer includes a few default model providers that are configured automatically:</p>"},{"location":"concepts/models/default-model-settings/#nvidia-provider-nvidia","title":"NVIDIA Provider (<code>nvidia</code>)","text":"<ul> <li>Endpoint: <code>https://integrate.api.nvidia.com/v1</code></li> <li>API Key: Set via <code>NVIDIA_API_KEY</code> environment variable</li> <li>Models: Access to NVIDIA's hosted models from build.nvidia.com</li> <li>Getting Started: Sign up and get your API key at build.nvidia.com</li> </ul> <p>The NVIDIA provider gives you access to state-of-the-art models including Nemotron and other NVIDIA-optimized models.</p>"},{"location":"concepts/models/default-model-settings/#openai-provider-openai","title":"OpenAI Provider (<code>openai</code>)","text":"<ul> <li>Endpoint: <code>https://api.openai.com/v1</code></li> <li>API Key: Set via <code>OPENAI_API_KEY</code> environment variable</li> <li>Models: Access to OpenAI's model catalog</li> <li>Getting Started: Get your API key from platform.openai.com/api-keys</li> </ul> <p>The OpenAI provider gives you access to GPT models and other OpenAI offerings.</p>"},{"location":"concepts/models/default-model-settings/#openrouter-provider-openrouter","title":"OpenRouter Provider (<code>openrouter</code>)","text":"<ul> <li>Endpoint: <code>https://openrouter.ai/api/v1</code></li> <li>API Key: Set via <code>OPENROUTER_API_KEY</code> environment variable</li> <li>Models: Access to a wide variety of models through OpenRouter's unified API</li> <li>Getting Started: Get your API key from openrouter.ai</li> </ul> <p>The OpenRouter provider gives you access to a unified interface for many different language models from various providers.</p>"},{"location":"concepts/models/default-model-settings/#model-configurations","title":"Model Configurations","text":"<p>Data Designer provides pre-configured model aliases for common use cases. When you create a <code>DataDesignerConfigBuilder</code> without specifying <code>model_configs</code>, these default configurations are automatically available.</p>"},{"location":"concepts/models/default-model-settings/#nvidia-models","title":"NVIDIA Models","text":"<p>The following model configurations are automatically available when <code>NVIDIA_API_KEY</code> is set:</p> Alias Model Use Case Inference Parameters <code>nvidia-text</code> <code>nvidia/nemotron-3-nano-30b-a3b</code> General text generation <code>temperature=1.0, top_p=1.0</code> <code>nvidia-reasoning</code> <code>openai/gpt-oss-20b</code> Reasoning and analysis tasks <code>temperature=0.35, top_p=0.95</code> <code>nvidia-vision</code> <code>nvidia/nemotron-nano-12b-v2-vl</code> Vision and image understanding <code>temperature=0.85, top_p=0.95</code> <code>nvidia-embedding</code> <code>nvidia/llama-3.2-nv-embedqa-1b-v2</code> Text embeddings <code>encoding_format=\"float\", extra_body={\"input_type\": \"query\"}</code>"},{"location":"concepts/models/default-model-settings/#openai-models","title":"OpenAI Models","text":"<p>The following model configurations are automatically available when <code>OPENAI_API_KEY</code> is set:</p> Alias Model Use Case Inference Parameters <code>openai-text</code> <code>gpt-4.1</code> General text generation <code>temperature=0.85, top_p=0.95</code> <code>openai-reasoning</code> <code>gpt-5</code> Reasoning and analysis tasks <code>temperature=0.35, top_p=0.95</code> <code>openai-vision</code> <code>gpt-5</code> Vision and image understanding <code>temperature=0.85, top_p=0.95</code> <code>openai-embedding</code> <code>text-embedding-3-large</code> Text embeddings <code>encoding_format=\"float\"</code>"},{"location":"concepts/models/default-model-settings/#openrouter-models","title":"OpenRouter Models","text":"<p>The following model configurations are automatically available when <code>OPENROUTER_API_KEY</code> is set:</p> Alias Model Use Case Inference Parameters <code>openrouter-text</code> <code>nvidia/nemotron-3-nano-30b-a3b</code> General text generation <code>temperature=1.0, top_p=1.0</code> <code>openrouter-reasoning</code> <code>openai/gpt-oss-20b</code> Reasoning and analysis tasks <code>temperature=0.35, top_p=0.95</code> <code>openrouter-vision</code> <code>nvidia/nemotron-nano-12b-v2-vl</code> Vision and image understanding <code>temperature=0.85, top_p=0.95</code> <code>openrouter-embedding</code> <code>openai/text-embedding-3-large</code> Text embeddings <code>encoding_format=\"float\"</code>"},{"location":"concepts/models/default-model-settings/#using-default-settings","title":"Using Default Settings","text":"<p>Default settings work out of the box - no configuration needed! Simply create <code>DataDesigner</code> and <code>DataDesignerConfigBuilder</code> instances without any arguments, and reference the default model aliases in your column configurations.</p> <p>For a complete example showing how to use default model settings, see the Quick Start Guide.</p>"},{"location":"concepts/models/default-model-settings/#how-default-model-providers-and-configurations-work","title":"How Default Model Providers and Configurations Work","text":"<p>When the Data Designer library or the CLI is initialized, default model configurations and providers are stored in the Data Designer home directory for easy access and customization if they do not already exist. These configuration files serve as the single source of truth for model settings. By default they are saved to the following paths:</p> <ul> <li>Model Configs: <code>~/.data-designer/model_configs.yaml</code></li> <li>Model Providers: <code>~/.data-designer/model_providers.yaml</code></li> </ul> <p>Tip</p> <p>While these files provide a convenient way to specify settings for your model providers and configuration you use most often, they can always be set programmatically in your SDG workflow.</p> <p>You can customize the home directory location by setting the <code>DATA_DESIGNER_HOME</code> environment variable:</p> <pre><code># In your .bashrc, .zshrc, or similar\nexport DATA_DESIGNER_HOME=\"/path/to/your/custom/directory\"\n</code></pre> <p>These configuration files can be modified in two ways:</p> <ol> <li>Using the CLI: Run CLI commands to add, update, or delete model configurations and providers</li> <li>Manual editing: Directly edit the YAML files with your preferred text editor</li> </ol> <p>Both methods operate on the same files, ensuring consistency across your entire Data Designer setup.</p>"},{"location":"concepts/models/default-model-settings/#important-notes","title":"Important Notes","text":"<p>API Key Requirements</p> <p>While default model configurations are always available, you need to set the appropriate API key environment variable (<code>NVIDIA_API_KEY</code>, <code>OPENAI_API_KEY</code>, or <code>OPENROUTER_API_KEY</code>) to actually use the corresponding models for data generation. Without a valid API key, any attempt to generate data using that provider's models will fail.</p> <p>Environment Variables</p> <p>Store your API keys in environment variables rather than hardcoding them in your scripts:</p> <pre><code># In your .bashrc, .zshrc, or similar\nexport NVIDIA_API_KEY=\"your-api-key-here\"\nexport OPENAI_API_KEY=\"your-openai-api-key-here\"\nexport OPENROUTER_API_KEY=\"your-openrouter-api-key-here\"\n</code></pre>"},{"location":"concepts/models/default-model-settings/#see-also","title":"See Also","text":"<ul> <li>Custom Model Settings: Learn how to create custom providers and model configurations</li> <li>Configure Model Settings With the CLI: Learn how to use the CLI to manage model settings</li> <li>Model Configurations: Learn about model configurations</li> </ul>"},{"location":"concepts/models/inference-parameters/","title":"Inference Parameters","text":"<p>Inference parameters control how models generate responses during synthetic data generation. Data Designer provides two types of inference parameters: <code>ChatCompletionInferenceParams</code> for text/code/structured generation and <code>EmbeddingInferenceParams</code> for embedding generation.</p>"},{"location":"concepts/models/inference-parameters/#overview","title":"Overview","text":"<p>When you create a <code>ModelConfig</code>, you can specify inference parameters to adjust model behavior. These parameters control aspects like randomness (temperature), diversity (top_p), context size (max_tokens), and more. Data Designer supports both static values and dynamic distribution-based sampling for certain parameters.</p>"},{"location":"concepts/models/inference-parameters/#chat-completion-inference-parameters","title":"Chat Completion Inference Parameters","text":"<p>The <code>ChatCompletionInferenceParams</code> class controls how models generate text completions (for text, code, and structured data generation). It provides fine-grained control over generation behavior and supports both static values and dynamic distribution-based sampling.</p>"},{"location":"concepts/models/inference-parameters/#fields","title":"Fields","text":"Field Type Required Description <code>temperature</code> <code>float</code> or <code>Distribution</code> No Controls randomness in generation (0.0 to 2.0). Higher values = more creative/random <code>top_p</code> <code>float</code> or <code>Distribution</code> No Nucleus sampling parameter (0.0 to 1.0). Controls diversity by filtering low-probability tokens <code>max_tokens</code> <code>int</code> No Maximum number of tokens to generate in the response (\u2265 1) <code>max_parallel_requests</code> <code>int</code> No Maximum concurrent API requests (default: 4, \u2265 1) <code>timeout</code> <code>int</code> No API request timeout in seconds (\u2265 1) <code>extra_body</code> <code>dict[str, Any]</code> No Additional parameters to include in the API request body <p>Default Values</p> <p>If <code>temperature</code>, <code>top_p</code>, or <code>max_tokens</code> are not provided, the model provider's default values will be used. Different providers and models may have different defaults.</p> <p>Controlling Reasoning Effort for GPT-OSS Models</p> <p>For gpt-oss models like <code>gpt-oss-20b</code> and <code>gpt-oss-120b</code>, you can control the reasoning effort using the <code>extra_body</code> parameter:</p> <pre><code>from data_designer.essentials import ChatCompletionInferenceParams\n\n# High reasoning effort (more thorough, slower)\ninference_parameters = ChatCompletionInferenceParams(\n    extra_body={\"reasoning_effort\": \"high\"}\n)\n\n# Medium reasoning effort (balanced)\ninference_parameters = ChatCompletionInferenceParams(\n    extra_body={\"reasoning_effort\": \"medium\"}\n)\n\n# Low reasoning effort (faster, less thorough)\ninference_parameters = ChatCompletionInferenceParams(\n    extra_body={\"reasoning_effort\": \"low\"}\n)\n</code></pre>"},{"location":"concepts/models/inference-parameters/#temperature-and-top-p-guidelines","title":"Temperature and Top P Guidelines","text":"<ul> <li> <p>Temperature:</p> <ul> <li><code>0.0-0.3</code>: Highly deterministic, focused outputs (ideal for structured/reasoning tasks)</li> <li><code>0.4-0.7</code>: Balanced creativity and coherence (general purpose)</li> <li><code>0.8-1.0</code>: Creative, diverse outputs (ideal for creative writing)</li> <li><code>1.0+</code>: Highly random and experimental</li> </ul> </li> <li> <p>Top P:</p> <ul> <li><code>0.1-0.5</code>: Very focused, only most likely tokens</li> <li><code>0.6-0.9</code>: Balanced diversity</li> <li><code>0.95-1.0</code>: Maximum diversity, including less likely tokens</li> </ul> </li> </ul> <p>Adjusting Temperature and Top P Together</p> <p>When tuning both parameters simultaneously, consider these combinations:</p> <ul> <li>For deterministic/structured outputs: Low temperature (<code>0.0-0.3</code>) + moderate-to-high top_p (<code>0.8-0.95</code>)<ul> <li>The low temperature ensures focus, while top_p allows some token diversity</li> </ul> </li> <li>For balanced generation: Moderate temperature (<code>0.5-0.7</code>) + high top_p (<code>0.9-0.95</code>)<ul> <li>This is a good starting point for most use cases</li> </ul> </li> <li>For creative outputs: Higher temperature (<code>0.8-1.0</code>) + high top_p (<code>0.95-1.0</code>)<ul> <li>Both parameters work together to maximize diversity</li> </ul> </li> </ul> <p>Avoid: Setting both very low (overly restrictive) or adjusting both dramatically at once. When experimenting, adjust one parameter at a time to understand its individual effect.</p>"},{"location":"concepts/models/inference-parameters/#distribution-based-inference-parameters","title":"Distribution-Based Inference Parameters","text":"<p>For <code>temperature</code> and <code>top_p</code> in <code>ChatCompletionInferenceParams</code>, you can specify distributions instead of fixed values. This allows Data Designer to sample different values for each generation request, introducing controlled variability into your synthetic data.</p>"},{"location":"concepts/models/inference-parameters/#uniform-distribution","title":"Uniform Distribution","text":"<p>Samples values uniformly between a low and high bound:</p> <pre><code>from data_designer.essentials import (\n    ChatCompletionInferenceParams,\n    UniformDistribution,\n    UniformDistributionParams,\n)\n\ninference_params = ChatCompletionInferenceParams(\n    temperature=UniformDistribution(\n        params=UniformDistributionParams(low=0.7, high=1.0)\n    ),\n)\n</code></pre>"},{"location":"concepts/models/inference-parameters/#manual-distribution","title":"Manual Distribution","text":"<p>Samples from a discrete set of values with optional weights:</p> <pre><code>from data_designer.essentials import (\n    ChatCompletionInferenceParams,\n    ManualDistribution,\n    ManualDistributionParams,\n)\n\n# Equal probability for each value\ninference_params = ChatCompletionInferenceParams(\n    temperature=ManualDistribution(\n        params=ManualDistributionParams(values=[0.5, 0.7, 0.9])\n    ),\n)\n\n# Weighted probabilities (normalized automatically)\ninference_params = ChatCompletionInferenceParams(\n    top_p=ManualDistribution(\n        params=ManualDistributionParams(\n            values=[0.8, 0.9, 0.95],\n            weights=[0.2, 0.5, 0.3]  # 20%, 50%, 30% probability\n        )\n    ),\n)\n</code></pre>"},{"location":"concepts/models/inference-parameters/#embedding-inference-parameters","title":"Embedding Inference Parameters","text":"<p>The <code>EmbeddingInferenceParams</code> class controls how models generate embeddings. This is used when working with embedding models for tasks like semantic search or similarity analysis.</p>"},{"location":"concepts/models/inference-parameters/#fields_1","title":"Fields","text":"Field Type Required Description <code>encoding_format</code> <code>Literal[\"float\", \"base64\"]</code> No Format of the embedding encoding (default: \"float\") <code>dimensions</code> <code>int</code> No Number of dimensions for the embedding <code>max_parallel_requests</code> <code>int</code> No Maximum concurrent API requests (default: 4, \u2265 1) <code>timeout</code> <code>int</code> No API request timeout in seconds (\u2265 1) <code>extra_body</code> <code>dict[str, Any]</code> No Additional parameters to include in the API request body"},{"location":"concepts/models/inference-parameters/#see-also","title":"See Also","text":"<ul> <li>Default Model Settings: Pre-configured model settings included with Data Designer</li> <li>Custom Model Settings: Learn how to create custom providers and model configurations</li> <li>Model Configurations: Learn about configuring model settings</li> <li>Model Providers: Learn about configuring model providers</li> </ul>"},{"location":"concepts/models/model-configs/","title":"Model Configurations","text":"<p>Model configurations define the specific models you use for synthetic data generation and their associated inference parameters. Each <code>ModelConfig</code> represents a named model that can be referenced throughout your data generation workflows.</p>"},{"location":"concepts/models/model-configs/#overview","title":"Overview","text":"<p>A <code>ModelConfig</code> specifies which LLM model to use and how it should behave during generation. When you create column configurations (like <code>LLMText</code>, <code>LLMCode</code>, or <code>LLMStructured</code>), you reference a model by its alias. Data Designer uses the model configuration to determine which model to call and with what parameters.</p>"},{"location":"concepts/models/model-configs/#modelconfig-structure","title":"ModelConfig Structure","text":"<p>The <code>ModelConfig</code> class has the following fields:</p> Field Type Required Description <code>alias</code> <code>str</code> Yes Unique identifier for this model configuration (e.g., <code>\"my-text-model\"</code>, <code>\"reasoning-model\"</code>) <code>model</code> <code>str</code> Yes Model identifier as recognized by the provider (e.g., <code>\"nvidia/nemotron-3-nano-30b-a3b\"</code>, <code>\"gpt-4\"</code>) <code>inference_parameters</code> <code>InferenceParamsT</code> No Controls model behavior during generation. Use <code>ChatCompletionInferenceParams</code> for text/code/structured generation or <code>EmbeddingInferenceParams</code> for embeddings. Defaults to <code>ChatCompletionInferenceParams()</code> if not provided. The generation type is automatically determined by the inference parameters type. See Inference Parameters for details. <code>provider</code> <code>str</code> No Reference to the name of the Provider to use (e.g., <code>\"nvidia\"</code>, <code>\"openai\"</code>, <code>\"openrouter\"</code>). If not specified, one set as the default provider, which may resolve to the first provider if there are more than one"},{"location":"concepts/models/model-configs/#examples","title":"Examples","text":""},{"location":"concepts/models/model-configs/#basic-model-configuration","title":"Basic Model Configuration","text":"<pre><code>from data_designer.essentials import ChatCompletionInferenceParams, ModelConfig\n\n# Simple model configuration with fixed parameters\nmodel_config = ModelConfig(\n    alias=\"my-text-model\",\n    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n    provider=\"nvidia\",\n    inference_parameters=ChatCompletionInferenceParams(\n        temperature=0.85,\n        top_p=0.95,\n        max_tokens=2048,\n    ),\n)\n</code></pre>"},{"location":"concepts/models/model-configs/#multiple-model-configurations-for-different-tasks","title":"Multiple Model Configurations for Different Tasks","text":"<pre><code>from data_designer.essentials import (\n    ChatCompletionInferenceParams,\n    EmbeddingInferenceParams,\n    GenerationType,\n    ModelConfig\n)\n\nmodel_configs = [\n    # Creative tasks\n    ModelConfig(\n        alias=\"creative-model\",\n        model=\"nvidia/nemotron-3-nano-30b-a3b\",\n        provider=\"nvidia\",\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=0.9,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n    # Critic tasks\n    ModelConfig(\n        alias=\"critic-model\",\n        model=\"nvidia/nemotron-3-nano-30b-a3b\",\n        provider=\"nvidia\",\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=0.25,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n    # Reasoning and structured tasks\n    ModelConfig(\n        alias=\"reasoning-model\",\n        model=\"openai/gpt-oss-20b\",\n        provider=\"nvidia\",\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=0.3,\n            top_p=0.9,\n            max_tokens=4096,\n        ),\n    ),\n    # Vision tasks\n    ModelConfig(\n        alias=\"vision-model\",\n        model=\"nvidia/nemotron-nano-12b-v2-vl\",\n        provider=\"nvidia\",\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=0.7,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n    # Embedding tasks\n    ModelConfig(\n        alias=\"embedding_model\",\n        model=-\"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n        provider=\"nvidia\",\n        inference_parameters=EmbeddingInferenceParams(\n            encoding_format=\"float\"\n            extra_body={\n                \"input_type\": \"query\"\n            }\n        )\n    )\n]\n</code></pre> <p>Experiment with max_tokens for Task-Specific Model Configurations</p> <p>The number of tokens required to generate a single data entry can vary significantly with use case. For example, reasoning models often need more tokens to \"think through\" problems before generating a response. Note that <code>max_tokens</code> specifies the maximum number of output tokens to generate in the response, so set this value based on the expected length of the generated content.</p>"},{"location":"concepts/models/model-configs/#see-also","title":"See Also","text":"<ul> <li>Inference Parameters: Detailed guide to inference parameters and how to configure them</li> <li>Model Providers: Learn about configuring model providers</li> <li>Default Model Settings: Pre-configured model settings included with Data Designer</li> <li>Custom Model Settings: Learn how to create custom providers and model configurations</li> <li>Inference Parameters: Detailed guide to inference parameters and how to configure them</li> <li>Model Providers: Learn about configuring model providers</li> <li>Configure Model Settings With the CLI: Use the CLI to manage model settings</li> <li>Column Configurations: Learn how to use models in column configurations</li> </ul>"},{"location":"concepts/models/model-providers/","title":"Model Providers","text":"<p>Model providers are external services that host and serve models. Data Designer uses the <code>ModelProvider</code> class to configure connections to these services.</p>"},{"location":"concepts/models/model-providers/#overview","title":"Overview","text":"<p>A <code>ModelProvider</code> defines how Data Designer connects to a provider's API endpoint. When you create a <code>ModelConfig</code>, you reference a provider by name, and Data Designer uses that provider's settings to make API calls to the appropriate endpoint.</p>"},{"location":"concepts/models/model-providers/#modelprovider-configuration","title":"ModelProvider Configuration","text":"<p>The <code>ModelProvider</code> class has the following fields:</p> Field Type Required Description <code>name</code> <code>str</code> Yes Unique identifier for the provider (e.g., <code>\"nvidia\"</code>, <code>\"openai\"</code>, <code>\"openrouter\"</code>) <code>endpoint</code> <code>str</code> Yes API endpoint URL (e.g., <code>\"https://integrate.api.nvidia.com/v1\"</code>) <code>provider_type</code> <code>str</code> No Provider type (default: <code>\"openai\"</code>). Uses OpenAI-compatible API format <code>api_key</code> <code>str</code> No API key or environment variable name (e.g., <code>\"NVIDIA_API_KEY\"</code>) <code>extra_body</code> <code>dict[str, Any]</code> No Additional parameters to include in the request body of all API requests to the provider. <code>extra_headers</code> <code>dict[str, str]</code> No Additional headers to include in all API requests to the provider."},{"location":"concepts/models/model-providers/#api-key-configuration","title":"API Key Configuration","text":"<p>The <code>api_key</code> field can be specified in two ways:</p> <ol> <li> <p>Environment variable name (recommended): Set <code>api_key</code> to the name of an environment variable (e.g., <code>\"NVIDIA_API_KEY\"</code>). Data Designer will automatically resolve it at runtime.</p> </li> <li> <p>Plain-text value: Set <code>api_key</code> to the actual API key string. This is less secure and not recommended for production use.</p> </li> </ol> <pre><code># Method 1: Environment variable (recommended)\nprovider = ModelProvider(\n    name=\"nvidia\",\n    endpoint=\"https://integrate.api.nvidia.com/v1\",\n    api_key=\"NVIDIA_API_KEY\",  # Will be resolved from environment\n)\n\n# Method 2: Direct value (not recommended)\nprovider = ModelProvider(\n    name=\"nvidia\",\n    endpoint=\"https://integrate.api.nvidia.com/v1\",\n    api_key=\"nvapi-abc123...\",  # Direct API key\n)\n</code></pre>"},{"location":"concepts/models/model-providers/#see-also","title":"See Also","text":"<ul> <li>Model Configurations: Learn about configuring models</li> <li>Inference Parameters: Detailed guide to inference parameters and how to configure them</li> <li>Default Model Settings: Pre-configured providers and model settings included with Data Designer</li> <li>Custom Model Settings: Learn how to create custom providers and model configurations</li> <li>Model Configurations: Learn about configuring models</li> <li>Inference Parameters: Detailed guide to inference parameters and how to configure them</li> <li>Configure Model Settings With the CLI: Use the CLI to manage providers and model settings</li> <li>Quick Start Guide: Get started with a simple example</li> </ul>"},{"location":"notebook_source/","title":"\ud83d\udcd3 Notebooks in <code>.py</code> Format","text":"<p>In this folder you can find all our tutorial notebooks in <code>.py</code> format. They can be converted to actual Jupyter notebooks by typing</p> <pre><code>make convert-execute-notebooks\n</code></pre> <p>from the root of the repository. This will not only convert but also execute all of the notebooks -- for that to work, make sure you went through our Quick Start and have API keys set. A new folder <code>docs/notebooks</code> will be created, including <code>README.md</code> and <code>pyproject.toml</code> files.</p> <p>Alternatively, you can use Jupytext directly</p> <pre><code>uv run --group notebooks --group docs jupytext --to ipynb *.py\n</code></pre>"},{"location":"notebook_source/#converting-jupyter-notebooks-to-py","title":"\ud83d\udd04 Converting Jupyter notebooks to <code>.py</code>","text":"<p>If you want to contribute with your own notebook, you can use the following command to generate <code>.py</code> files in the same format as the ones in this folder:</p> <pre><code>uv run jupytext --to py [notebook-name].ipynb -o [notebook-name].py\n</code></pre>"},{"location":"notebook_source/1-the-basics/","title":"1 the basics","text":"In\u00a0[\u00a0]: Copied! <pre>from data_designer.essentials import (\n    CategorySamplerParams,\n    ChatCompletionInferenceParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    LLMTextColumnConfig,\n    ModelConfig,\n    PersonFromFakerSamplerParams,\n    SamplerColumnConfig,\n    SamplerType,\n    SubcategorySamplerParams,\n    UniformSamplerParams,\n)\n</pre> from data_designer.essentials import (     CategorySamplerParams,     ChatCompletionInferenceParams,     DataDesigner,     DataDesignerConfigBuilder,     LLMTextColumnConfig,     ModelConfig,     PersonFromFakerSamplerParams,     SamplerColumnConfig,     SamplerType,     SubcategorySamplerParams,     UniformSamplerParams, ) In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[\u00a0]: Copied! <pre>config_builder = DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre>config_builder.info.display(\"samplers\")\n</pre> config_builder.info.display(\"samplers\") <p>Let's start designing our product review dataset by adding product category and subcategory columns.</p> In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    SamplerColumnConfig(\n        name=\"product_category\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\n                \"Electronics\",\n                \"Clothing\",\n                \"Home &amp; Kitchen\",\n                \"Books\",\n                \"Home Office\",\n            ],\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"product_subcategory\",\n        sampler_type=SamplerType.SUBCATEGORY,\n        params=SubcategorySamplerParams(\n            category=\"product_category\",\n            values={\n                \"Electronics\": [\n                    \"Smartphones\",\n                    \"Laptops\",\n                    \"Headphones\",\n                    \"Cameras\",\n                    \"Accessories\",\n                ],\n                \"Clothing\": [\n                    \"Men's Clothing\",\n                    \"Women's Clothing\",\n                    \"Winter Coats\",\n                    \"Activewear\",\n                    \"Accessories\",\n                ],\n                \"Home &amp; Kitchen\": [\n                    \"Appliances\",\n                    \"Cookware\",\n                    \"Furniture\",\n                    \"Decor\",\n                    \"Organization\",\n                ],\n                \"Books\": [\n                    \"Fiction\",\n                    \"Non-Fiction\",\n                    \"Self-Help\",\n                    \"Textbooks\",\n                    \"Classics\",\n                ],\n                \"Home Office\": [\n                    \"Desks\",\n                    \"Chairs\",\n                    \"Storage\",\n                    \"Office Supplies\",\n                    \"Lighting\",\n                ],\n            },\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"target_age_range\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),\n    )\n)\n\n# Optionally validate that the columns are configured correctly.\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     SamplerColumnConfig(         name=\"product_category\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(             values=[                 \"Electronics\",                 \"Clothing\",                 \"Home &amp; Kitchen\",                 \"Books\",                 \"Home Office\",             ],         ),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"product_subcategory\",         sampler_type=SamplerType.SUBCATEGORY,         params=SubcategorySamplerParams(             category=\"product_category\",             values={                 \"Electronics\": [                     \"Smartphones\",                     \"Laptops\",                     \"Headphones\",                     \"Cameras\",                     \"Accessories\",                 ],                 \"Clothing\": [                     \"Men's Clothing\",                     \"Women's Clothing\",                     \"Winter Coats\",                     \"Activewear\",                     \"Accessories\",                 ],                 \"Home &amp; Kitchen\": [                     \"Appliances\",                     \"Cookware\",                     \"Furniture\",                     \"Decor\",                     \"Organization\",                 ],                 \"Books\": [                     \"Fiction\",                     \"Non-Fiction\",                     \"Self-Help\",                     \"Textbooks\",                     \"Classics\",                 ],                 \"Home Office\": [                     \"Desks\",                     \"Chairs\",                     \"Storage\",                     \"Office Supplies\",                     \"Lighting\",                 ],             },         ),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"target_age_range\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),     ) )  # Optionally validate that the columns are configured correctly. data_designer.validate(config_builder) <p>Next, let's add samplers to generate data related to the customer and their review.</p> In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=SamplerType.PERSON_FROM_FAKER,\n        params=PersonFromFakerSamplerParams(age_range=[18, 70], locale=\"en_US\"),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"number_of_stars\",\n        sampler_type=SamplerType.UNIFORM,\n        params=UniformSamplerParams(low=1, high=5),\n        convert_to=\"int\",  # Convert the sampled float to an integer.\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"review_style\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],\n            weights=[1, 2, 2, 1],\n        ),\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     SamplerColumnConfig(         name=\"customer\",         sampler_type=SamplerType.PERSON_FROM_FAKER,         params=PersonFromFakerSamplerParams(age_range=[18, 70], locale=\"en_US\"),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"number_of_stars\",         sampler_type=SamplerType.UNIFORM,         params=UniformSamplerParams(low=1, high=5),         convert_to=\"int\",  # Convert the sampled float to an integer.     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"review_style\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(             values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],             weights=[1, 2, 2, 1],         ),     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    LLMTextColumnConfig(\n        name=\"product_name\",\n        prompt=(\n            \"You are a helpful assistant that generates product names. DO NOT add quotes around the product name.\\n\\n\"\n            \"Come up with a creative product name for a product in the '{{ product_category }}' category, focusing \"\n            \"on products related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"\n            \"{{ target_age_range }} years old. Respond with only the product name, no other text.\"\n        ),\n        model_alias=MODEL_ALIAS,\n    )\n)\n\nconfig_builder.add_column(\n    LLMTextColumnConfig(\n        name=\"customer_review\",\n        prompt=(\n            \"You are a customer named {{ customer.first_name }} from {{ customer.city }}, {{ customer.state }}. \"\n            \"You are {{ customer.age }} years old and recently purchased a product called {{ product_name }}. \"\n            \"Write a review of this product, which you gave a rating of {{ number_of_stars }} stars. \"\n            \"The style of the review should be '{{ review_style }}'. \"\n            \"Respond with only the review, no other text.\"\n        ),\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     LLMTextColumnConfig(         name=\"product_name\",         prompt=(             \"You are a helpful assistant that generates product names. DO NOT add quotes around the product name.\\n\\n\"             \"Come up with a creative product name for a product in the '{{ product_category }}' category, focusing \"             \"on products related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"             \"{{ target_age_range }} years old. Respond with only the product name, no other text.\"         ),         model_alias=MODEL_ALIAS,     ) )  config_builder.add_column(     LLMTextColumnConfig(         name=\"customer_review\",         prompt=(             \"You are a customer named {{ customer.first_name }} from {{ customer.city }}, {{ customer.state }}. \"             \"You are {{ customer.age }} years old and recently purchased a product called {{ product_name }}. \"             \"Write a review of this product, which you gave a rating of {{ number_of_stars }} stars. \"             \"The style of the review should be '{{ review_style }}'. \"             \"Respond with only the review, no other text.\"         ),         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-1\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-1\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"notebook_source/1-the-basics/#data-designer-tutorial-the-basics","title":"\ud83c\udfa8 Data Designer Tutorial: The Basics\u00b6","text":""},{"location":"notebook_source/1-the-basics/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>This notebook demonstrates the basics of Data Designer by generating a simple product review dataset.</p>"},{"location":"notebook_source/1-the-basics/#import-the-essentials","title":"\ud83d\udce6 Import the essentials\u00b6","text":"<ul> <li>The <code>essentials</code> module provides quick access to the most commonly used objects.</li> </ul>"},{"location":"notebook_source/1-the-basics/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object is responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebook_source/1-the-basics/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebook_source/1-the-basics/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebook_source/1-the-basics/#getting-started-with-sampler-columns","title":"\ud83c\udfb2 Getting started with sampler columns\u00b6","text":"<ul> <li><p>Sampler columns offer non-LLM based generation of synthetic data.</p> </li> <li><p>They are particularly useful for steering the diversity of the generated data, as we demonstrate below.</p> </li> </ul> <p>You can view available samplers using the config builder's <code>info</code> property:</p>"},{"location":"notebook_source/1-the-basics/#llm-generated-columns","title":"\ud83e\udd9c LLM-generated columns\u00b6","text":"<ul> <li><p>The real power of Data Designer comes from leveraging LLMs to generate text, code, and structured data.</p> </li> <li><p>When prompting the LLM, we can use Jinja templating to reference other columns in the dataset.</p> </li> <li><p>As we see below, nested json fields can be accessed using dot notation.</p> </li> </ul>"},{"location":"notebook_source/1-the-basics/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebook_source/1-the-basics/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebook_source/1-the-basics/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebook_source/1-the-basics/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Now that you've seen the basics of Data Designer, check out the following notebooks to learn more about:</p> <ul> <li><p>Structured outputs and jinja expressions</p> </li> <li><p>Seeding synthetic data generation with an external dataset</p> </li> <li><p>Providing images as context</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/","title":"2 structured outputs and jinja expressions","text":"In\u00a0[\u00a0]: Copied! <pre>from data_designer.essentials import (\n    CategorySamplerParams,\n    ChatCompletionInferenceParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    ExpressionColumnConfig,\n    LLMStructuredColumnConfig,\n    ModelConfig,\n    PersonFromFakerSamplerParams,\n    SamplerColumnConfig,\n    SamplerType,\n    SubcategorySamplerParams,\n)\n</pre> from data_designer.essentials import (     CategorySamplerParams,     ChatCompletionInferenceParams,     DataDesigner,     DataDesignerConfigBuilder,     ExpressionColumnConfig,     LLMStructuredColumnConfig,     ModelConfig,     PersonFromFakerSamplerParams,     SamplerColumnConfig,     SamplerType,     SubcategorySamplerParams, ) In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[\u00a0]: Copied! <pre>config_builder = DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre>from decimal import Decimal\nfrom typing import Literal\n\nfrom pydantic import BaseModel, Field\n\n\n# We define a Product schema so that the name, description, and price are generated\n# in one go, with the types and constraints specified.\nclass Product(BaseModel):\n    name: str = Field(description=\"The name of the product\")\n    description: str = Field(description=\"A description of the product\")\n    price: Decimal = Field(description=\"The price of the product\", ge=10, le=1000, decimal_places=2)\n\n\nclass ProductReview(BaseModel):\n    rating: int = Field(description=\"The rating of the product\", ge=1, le=5)\n    customer_mood: Literal[\"irritated\", \"mad\", \"happy\", \"neutral\", \"excited\"] = Field(\n        description=\"The mood of the customer\"\n    )\n    review: str = Field(description=\"A review of the product\")\n</pre> from decimal import Decimal from typing import Literal  from pydantic import BaseModel, Field   # We define a Product schema so that the name, description, and price are generated # in one go, with the types and constraints specified. class Product(BaseModel):     name: str = Field(description=\"The name of the product\")     description: str = Field(description=\"A description of the product\")     price: Decimal = Field(description=\"The price of the product\", ge=10, le=1000, decimal_places=2)   class ProductReview(BaseModel):     rating: int = Field(description=\"The rating of the product\", ge=1, le=5)     customer_mood: Literal[\"irritated\", \"mad\", \"happy\", \"neutral\", \"excited\"] = Field(         description=\"The mood of the customer\"     )     review: str = Field(description=\"A review of the product\") <p>Next, let's design our product review dataset using a few more tricks compared to the previous notebook.</p> In\u00a0[\u00a0]: Copied! <pre># Since we often only want a few attributes from Person objects, we can\n# set drop=True in the column config to drop the column from the final dataset.\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=SamplerType.PERSON_FROM_FAKER,\n        params=PersonFromFakerSamplerParams(),\n        drop=True,\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"product_category\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\n                \"Electronics\",\n                \"Clothing\",\n                \"Home &amp; Kitchen\",\n                \"Books\",\n                \"Home Office\",\n            ],\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"product_subcategory\",\n        sampler_type=SamplerType.SUBCATEGORY,\n        params=SubcategorySamplerParams(\n            category=\"product_category\",\n            values={\n                \"Electronics\": [\n                    \"Smartphones\",\n                    \"Laptops\",\n                    \"Headphones\",\n                    \"Cameras\",\n                    \"Accessories\",\n                ],\n                \"Clothing\": [\n                    \"Men's Clothing\",\n                    \"Women's Clothing\",\n                    \"Winter Coats\",\n                    \"Activewear\",\n                    \"Accessories\",\n                ],\n                \"Home &amp; Kitchen\": [\n                    \"Appliances\",\n                    \"Cookware\",\n                    \"Furniture\",\n                    \"Decor\",\n                    \"Organization\",\n                ],\n                \"Books\": [\n                    \"Fiction\",\n                    \"Non-Fiction\",\n                    \"Self-Help\",\n                    \"Textbooks\",\n                    \"Classics\",\n                ],\n                \"Home Office\": [\n                    \"Desks\",\n                    \"Chairs\",\n                    \"Storage\",\n                    \"Office Supplies\",\n                    \"Lighting\",\n                ],\n            },\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"target_age_range\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),\n    )\n)\n\n# Sampler columns support conditional params, which are used if the condition is met.\n# In this example, we set the review style to rambling if the target age range is 18-25.\n# Note conditional parameters are only supported for Sampler column types.\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"review_style\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],\n            weights=[1, 2, 2, 1],\n        ),\n        conditional_params={\n            \"target_age_range == '18-25'\": CategorySamplerParams(values=[\"rambling\"]),\n        },\n    )\n)\n\n# Optionally validate that the columns are configured correctly.\ndata_designer.validate(config_builder)\n</pre> # Since we often only want a few attributes from Person objects, we can # set drop=True in the column config to drop the column from the final dataset. config_builder.add_column(     SamplerColumnConfig(         name=\"customer\",         sampler_type=SamplerType.PERSON_FROM_FAKER,         params=PersonFromFakerSamplerParams(),         drop=True,     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"product_category\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(             values=[                 \"Electronics\",                 \"Clothing\",                 \"Home &amp; Kitchen\",                 \"Books\",                 \"Home Office\",             ],         ),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"product_subcategory\",         sampler_type=SamplerType.SUBCATEGORY,         params=SubcategorySamplerParams(             category=\"product_category\",             values={                 \"Electronics\": [                     \"Smartphones\",                     \"Laptops\",                     \"Headphones\",                     \"Cameras\",                     \"Accessories\",                 ],                 \"Clothing\": [                     \"Men's Clothing\",                     \"Women's Clothing\",                     \"Winter Coats\",                     \"Activewear\",                     \"Accessories\",                 ],                 \"Home &amp; Kitchen\": [                     \"Appliances\",                     \"Cookware\",                     \"Furniture\",                     \"Decor\",                     \"Organization\",                 ],                 \"Books\": [                     \"Fiction\",                     \"Non-Fiction\",                     \"Self-Help\",                     \"Textbooks\",                     \"Classics\",                 ],                 \"Home Office\": [                     \"Desks\",                     \"Chairs\",                     \"Storage\",                     \"Office Supplies\",                     \"Lighting\",                 ],             },         ),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"target_age_range\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),     ) )  # Sampler columns support conditional params, which are used if the condition is met. # In this example, we set the review style to rambling if the target age range is 18-25. # Note conditional parameters are only supported for Sampler column types. config_builder.add_column(     SamplerColumnConfig(         name=\"review_style\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(             values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],             weights=[1, 2, 2, 1],         ),         conditional_params={             \"target_age_range == '18-25'\": CategorySamplerParams(values=[\"rambling\"]),         },     ) )  # Optionally validate that the columns are configured correctly. data_designer.validate(config_builder) <p>Next, we will use more advanced Jinja expressions to create new columns.</p> <p>Jinja expressions let you:</p> <ul> <li><p>Access nested attributes: <code>{{ customer.first_name }}</code></p> </li> <li><p>Combine values: <code>{{ customer.first_name }} {{ customer.last_name }}</code></p> </li> <li><p>Use conditional logic: <code>{% if condition %}...{% endif %}</code></p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre># We can create new columns using Jinja expressions that reference\n# existing columns, including attributes of nested objects.\nconfig_builder.add_column(\n    ExpressionColumnConfig(name=\"customer_name\", expr=\"{{ customer.first_name }} {{ customer.last_name }}\")\n)\n\nconfig_builder.add_column(ExpressionColumnConfig(name=\"customer_age\", expr=\"{{ customer.age }}\"))\n\nconfig_builder.add_column(\n    LLMStructuredColumnConfig(\n        name=\"product\",\n        prompt=(\n            \"Create a product in the '{{ product_category }}' category, focusing on products  \"\n            \"related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"\n            \"{{ target_age_range }} years old. The product should be priced between $10 and $1000.\"\n        ),\n        output_format=Product,\n        model_alias=MODEL_ALIAS,\n    )\n)\n\n# We can even use if/else logic in our Jinja expressions to create more complex prompt patterns.\nconfig_builder.add_column(\n    LLMStructuredColumnConfig(\n        name=\"customer_review\",\n        prompt=(\n            \"Your task is to write a review for the following product:\\n\\n\"\n            \"Product Name: {{ product.name }}\\n\"\n            \"Product Description: {{ product.description }}\\n\"\n            \"Price: {{ product.price }}\\n\\n\"\n            \"Imagine your name is {{ customer_name }} and you are from {{ customer.city }}, {{ customer.state }}. \"\n            \"Write the review in a style that is '{{ review_style }}'.\"\n            \"{% if target_age_range == '18-25' %}\"\n            \"Make sure the review is more informal and conversational.\\n\"\n            \"{% else %}\"\n            \"Make sure the review is more formal and structured.\\n\"\n            \"{% endif %}\"\n            \"The review field should contain only the review, no other text.\"\n        ),\n        output_format=ProductReview,\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> # We can create new columns using Jinja expressions that reference # existing columns, including attributes of nested objects. config_builder.add_column(     ExpressionColumnConfig(name=\"customer_name\", expr=\"{{ customer.first_name }} {{ customer.last_name }}\") )  config_builder.add_column(ExpressionColumnConfig(name=\"customer_age\", expr=\"{{ customer.age }}\"))  config_builder.add_column(     LLMStructuredColumnConfig(         name=\"product\",         prompt=(             \"Create a product in the '{{ product_category }}' category, focusing on products  \"             \"related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"             \"{{ target_age_range }} years old. The product should be priced between $10 and $1000.\"         ),         output_format=Product,         model_alias=MODEL_ALIAS,     ) )  # We can even use if/else logic in our Jinja expressions to create more complex prompt patterns. config_builder.add_column(     LLMStructuredColumnConfig(         name=\"customer_review\",         prompt=(             \"Your task is to write a review for the following product:\\n\\n\"             \"Product Name: {{ product.name }}\\n\"             \"Product Description: {{ product.description }}\\n\"             \"Price: {{ product.price }}\\n\\n\"             \"Imagine your name is {{ customer_name }} and you are from {{ customer.city }}, {{ customer.state }}. \"             \"Write the review in a style that is '{{ review_style }}'.\"             \"{% if target_age_range == '18-25' %}\"             \"Make sure the review is more informal and conversational.\\n\"             \"{% else %}\"             \"Make sure the review is more formal and structured.\\n\"             \"{% endif %}\"             \"The review field should contain only the review, no other text.\"         ),         output_format=ProductReview,         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-2\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-2\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#data-designer-tutorial-structured-outputs-and-jinja-expressions","title":"\ud83c\udfa8 Data Designer Tutorial: Structured Outputs and Jinja Expressions\u00b6","text":""},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>In this notebook, we will continue our exploration of Data Designer, demonstrating more advanced data generation using structured outputs and Jinja expressions.</p> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#import-the-essentials","title":"\ud83d\udce6 Import the essentials\u00b6","text":"<ul> <li>The <code>essentials</code> module provides quick access to the most commonly used objects.</li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object that is used to interface with the library.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#designing-our-data","title":"\ud83e\uddd1\u200d\ud83c\udfa8 Designing our data\u00b6","text":"<ul> <li><p>We will again create a product review dataset, but this time we will use structured outputs and Jinja expressions.</p> </li> <li><p>Structured outputs let you specify the exact schema of the data you want to generate.</p> </li> <li><p>Data Designer supports schemas specified using either json schema or Pydantic data models (recommended).</p> </li> </ul> <p>We'll define our structured outputs using Pydantic data models</p> <p>\ud83d\udca1 Why Pydantic?</p> <ul> <li><p>Pydantic models provide better IDE support and type validation.</p> </li> <li><p>They are more Pythonic than raw JSON schemas.</p> </li> <li><p>They integrate seamlessly with Data Designer's structured output system.</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Check out the following notebook to learn more about:</p> <ul> <li><p>Seeding synthetic data generation with an external dataset</p> </li> <li><p>Providing images as contextA</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/","title":"3 seeding with a dataset","text":"In\u00a0[\u00a0]: Copied! <pre>from data_designer.essentials import (\n    ChatCompletionInferenceParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    LocalFileSeedSource,\n    ModelConfig,\n)\n</pre> from data_designer.essentials import (     ChatCompletionInferenceParams,     DataDesigner,     DataDesignerConfigBuilder,     LocalFileSeedSource,     ModelConfig, ) In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[\u00a0]: Copied! <pre>config_builder = DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre># Download sample dataset from Github\nimport urllib.request\n\nurl = \"https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/refs/heads/main/nemo/NeMo-Data-Designer/data/gretelai_symptom_to_diagnosis.csv\"\nlocal_filename, _ = urllib.request.urlretrieve(url, \"gretelai_symptom_to_diagnosis.csv\")\n\n# Seed datasets are passed as reference objects to the config builder.\nseed_source = LocalFileSeedSource(path=local_filename)\n\nconfig_builder.with_seed_dataset(seed_source)\n</pre> # Download sample dataset from Github import urllib.request  url = \"https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/refs/heads/main/nemo/NeMo-Data-Designer/data/gretelai_symptom_to_diagnosis.csv\" local_filename, _ = urllib.request.urlretrieve(url, \"gretelai_symptom_to_diagnosis.csv\")  # Seed datasets are passed as reference objects to the config builder. seed_source = LocalFileSeedSource(path=local_filename)  config_builder.with_seed_dataset(seed_source) In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    name=\"patient_sampler\",\n    column_type=\"sampler\",\n    sampler_type=\"person_from_faker\",\n)\n\nconfig_builder.add_column(\n    name=\"doctor_sampler\",\n    column_type=\"sampler\",\n    sampler_type=\"person_from_faker\",\n)\n\nconfig_builder.add_column(\n    name=\"patient_id\",\n    column_type=\"sampler\",\n    sampler_type=\"uuid\",\n    params={\n        \"prefix\": \"PT-\",\n        \"short_form\": True,\n        \"uppercase\": True,\n    },\n)\n\nconfig_builder.add_column(\n    name=\"first_name\",\n    column_type=\"expression\",\n    expr=\"{{ patient_sampler.first_name}}\",\n)\n\nconfig_builder.add_column(\n    name=\"last_name\",\n    column_type=\"expression\",\n    expr=\"{{ patient_sampler.last_name }}\",\n)\n\n\nconfig_builder.add_column(\n    name=\"dob\",\n    column_type=\"expression\",\n    expr=\"{{ patient_sampler.birth_date }}\",\n)\n\nconfig_builder.add_column(\n    name=\"symptom_onset_date\",\n    column_type=\"sampler\",\n    sampler_type=\"datetime\",\n    params={\"start\": \"2024-01-01\", \"end\": \"2024-12-31\"},\n)\n\nconfig_builder.add_column(\n    name=\"date_of_visit\",\n    column_type=\"sampler\",\n    sampler_type=\"timedelta\",\n    params={\"dt_min\": 1, \"dt_max\": 30, \"reference_column_name\": \"symptom_onset_date\"},\n)\n\nconfig_builder.add_column(\n    name=\"physician\",\n    column_type=\"expression\",\n    expr=\"Dr. {{ doctor_sampler.last_name }}\",\n)\n\nconfig_builder.add_column(\n    name=\"physician_notes\",\n    column_type=\"llm-text\",\n    prompt=\"\"\"\\\nYou are a primary-care physician who just had an appointment with {{ first_name }} {{ last_name }},\nwho has been struggling with symptoms from {{ diagnosis }} since {{ symptom_onset_date }}.\nThe date of today's visit is {{ date_of_visit }}.\n\n{{ patient_summary }}\n\nWrite careful notes about your visit with {{ first_name }},\nas Dr. {{ doctor_sampler.first_name }} {{ doctor_sampler.last_name }}.\n\nFormat the notes as a busy doctor might.\nRespond with only the notes, no other text.\n\"\"\",\n    model_alias=MODEL_ALIAS,\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     name=\"patient_sampler\",     column_type=\"sampler\",     sampler_type=\"person_from_faker\", )  config_builder.add_column(     name=\"doctor_sampler\",     column_type=\"sampler\",     sampler_type=\"person_from_faker\", )  config_builder.add_column(     name=\"patient_id\",     column_type=\"sampler\",     sampler_type=\"uuid\",     params={         \"prefix\": \"PT-\",         \"short_form\": True,         \"uppercase\": True,     }, )  config_builder.add_column(     name=\"first_name\",     column_type=\"expression\",     expr=\"{{ patient_sampler.first_name}}\", )  config_builder.add_column(     name=\"last_name\",     column_type=\"expression\",     expr=\"{{ patient_sampler.last_name }}\", )   config_builder.add_column(     name=\"dob\",     column_type=\"expression\",     expr=\"{{ patient_sampler.birth_date }}\", )  config_builder.add_column(     name=\"symptom_onset_date\",     column_type=\"sampler\",     sampler_type=\"datetime\",     params={\"start\": \"2024-01-01\", \"end\": \"2024-12-31\"}, )  config_builder.add_column(     name=\"date_of_visit\",     column_type=\"sampler\",     sampler_type=\"timedelta\",     params={\"dt_min\": 1, \"dt_max\": 30, \"reference_column_name\": \"symptom_onset_date\"}, )  config_builder.add_column(     name=\"physician\",     column_type=\"expression\",     expr=\"Dr. {{ doctor_sampler.last_name }}\", )  config_builder.add_column(     name=\"physician_notes\",     column_type=\"llm-text\",     prompt=\"\"\"\\ You are a primary-care physician who just had an appointment with {{ first_name }} {{ last_name }}, who has been struggling with symptoms from {{ diagnosis }} since {{ symptom_onset_date }}. The date of today's visit is {{ date_of_visit }}.  {{ patient_summary }}  Write careful notes about your visit with {{ first_name }}, as Dr. {{ doctor_sampler.first_name }} {{ doctor_sampler.last_name }}.  Format the notes as a busy doctor might. Respond with only the notes, no other text. \"\"\",     model_alias=MODEL_ALIAS, )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-3\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-3\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"notebook_source/3-seeding-with-a-dataset/#data-designer-tutorial-seeding-synthetic-data-generation-with-an-external-dataset","title":"\ud83c\udfa8 Data Designer Tutorial: Seeding Synthetic Data Generation with an External Dataset\u00b6","text":""},{"location":"notebook_source/3-seeding-with-a-dataset/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>In this notebook, we will demonstrate how to seed synthetic data generation in Data Designer with an external dataset.</p> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"notebook_source/3-seeding-with-a-dataset/#import-the-essentials","title":"\ud83d\udce6 Import the essentials\u00b6","text":"<ul> <li>The <code>essentials</code> module provides quick access to the most commonly used objects.</li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object is responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#prepare-a-seed-dataset","title":"\ud83c\udfe5 Prepare a seed dataset\u00b6","text":"<ul> <li><p>For this notebook, we'll create a synthetic dataset of patient notes.</p> </li> <li><p>We will seed the generation process with a symptom-to-diagnosis dataset.</p> </li> <li><p>We already have the dataset downloaded in the data directory of this repository.</p> </li> </ul> <p>\ud83c\udf31 Why use a seed dataset?</p> <ul> <li><p>Seed datasets let you steer the generation process by providing context that is specific to your use case.</p> </li> <li><p>Seed datasets are also an excellent way to inject real-world diversity into your synthetic data.</p> </li> <li><p>During generation, prompt templates can reference any of the seed dataset fields.</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#designing-our-synthetic-patient-notes-dataset","title":"\ud83c\udfa8 Designing our synthetic patient notes dataset\u00b6","text":"<ul> <li><p>Here we use <code>add_column</code> with keyword arguments (rather than imported config objects).</p> </li> <li><p>Generally, we recommend using concrete objects, but this is a convenient shorthand.</p> </li> <li><p>Note: The prompt template can reference fields from our seed dataset:</p> <ul> <li><code>{{ diagnosis }}</code> - the medical diagnosis from the seed data</li> <li><code>{{ patient_summary }}</code> - the symptom description from the seed data</li> </ul> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebook_source/3-seeding-with-a-dataset/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Check out the following notebook to learn more about:</p> <ul> <li>Providing images as context</li> </ul>"},{"location":"notebook_source/4-providing-images-as-context/","title":"4 providing images as context","text":"In\u00a0[\u00a0]: Copied! <pre># Standard library imports\nimport base64\nimport io\nimport uuid\n\n# Third-party imports\nimport pandas as pd\nimport rich\nfrom datasets import load_dataset\nfrom IPython.display import display\nfrom rich.panel import Panel\n\n# Data Designer imports\nfrom data_designer.essentials import (\n    ChatCompletionInferenceParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    DataFrameSeedSource,\n    ImageContext,\n    ImageFormat,\n    LLMTextColumnConfig,\n    ModalityDataType,\n    ModelConfig,\n)\n</pre> # Standard library imports import base64 import io import uuid  # Third-party imports import pandas as pd import rich from datasets import load_dataset from IPython.display import display from rich.panel import Panel  # Data Designer imports from data_designer.essentials import (     ChatCompletionInferenceParams,     DataDesigner,     DataDesignerConfigBuilder,     DataFrameSeedSource,     ImageContext,     ImageFormat,     LLMTextColumnConfig,     ModalityDataType,     ModelConfig, ) In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\nmodel_configs = [\n    ModelConfig(\n        alias=\"vision\",\n        model=\"meta/llama-4-scout-17b-16e-instruct\",\n        provider=MODEL_PROVIDER,\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=0.60,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  model_configs = [     ModelConfig(         alias=\"vision\",         model=\"meta/llama-4-scout-17b-16e-instruct\",         provider=MODEL_PROVIDER,         inference_parameters=ChatCompletionInferenceParams(             temperature=0.60,             top_p=0.95,             max_tokens=2048,         ),     ), ] In\u00a0[\u00a0]: Copied! <pre>config_builder = DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre># Dataset processing configuration\nIMG_COUNT = 512  # Number of images to process\nBASE64_IMAGE_HEIGHT = 512  # Standardized height for model input\n\n# Load ColPali dataset for visual documents\nimg_dataset_cfg = {\"path\": \"vidore/colpali_train_set\", \"split\": \"train\", \"streaming\": True}\n</pre> # Dataset processing configuration IMG_COUNT = 512  # Number of images to process BASE64_IMAGE_HEIGHT = 512  # Standardized height for model input  # Load ColPali dataset for visual documents img_dataset_cfg = {\"path\": \"vidore/colpali_train_set\", \"split\": \"train\", \"streaming\": True} In\u00a0[\u00a0]: Copied! <pre>def resize_image(image, height: int):\n    \"\"\"\n    Resize image while maintaining aspect ratio.\n\n    Args:\n        image: PIL Image object\n        height: Target height in pixels\n\n    Returns:\n        Resized PIL Image object\n    \"\"\"\n    original_width, original_height = image.size\n    width = int(original_width * (height / original_height))\n    return image.resize((width, height))\n\n\ndef convert_image_to_chat_format(record, height: int) -&gt; dict:\n    \"\"\"\n    Convert PIL image to base64 format for chat template usage.\n\n    Args:\n        record: Dataset record containing image and metadata\n        height: Target height for image resizing\n\n    Returns:\n        Updated record with base64_image and uuid fields\n    \"\"\"\n    # Resize image for consistent processing\n    image = resize_image(record[\"image\"], height)\n\n    # Convert to base64 string\n    img_buffer = io.BytesIO()\n    image.save(img_buffer, format=\"PNG\")\n    byte_data = img_buffer.getvalue()\n    base64_encoded_data = base64.b64encode(byte_data)\n    base64_string = base64_encoded_data.decode(\"utf-8\")\n\n    # Return updated record\n    return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())}\n</pre> def resize_image(image, height: int):     \"\"\"     Resize image while maintaining aspect ratio.      Args:         image: PIL Image object         height: Target height in pixels      Returns:         Resized PIL Image object     \"\"\"     original_width, original_height = image.size     width = int(original_width * (height / original_height))     return image.resize((width, height))   def convert_image_to_chat_format(record, height: int) -&gt; dict:     \"\"\"     Convert PIL image to base64 format for chat template usage.      Args:         record: Dataset record containing image and metadata         height: Target height for image resizing      Returns:         Updated record with base64_image and uuid fields     \"\"\"     # Resize image for consistent processing     image = resize_image(record[\"image\"], height)      # Convert to base64 string     img_buffer = io.BytesIO()     image.save(img_buffer, format=\"PNG\")     byte_data = img_buffer.getvalue()     base64_encoded_data = base64.b64encode(byte_data)     base64_string = base64_encoded_data.decode(\"utf-8\")      # Return updated record     return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())} In\u00a0[\u00a0]: Copied! <pre># Load and process the visual document dataset\nprint(\"\ud83d\udce5 Loading and processing document images...\")\n\nimg_dataset_iter = iter(\n    load_dataset(**img_dataset_cfg).map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT})\n)\nimg_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])\n\nprint(f\"\u2705 Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\")\n</pre> # Load and process the visual document dataset print(\"\ud83d\udce5 Loading and processing document images...\")  img_dataset_iter = iter(     load_dataset(**img_dataset_cfg).map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT}) ) img_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])  print(f\"\u2705 Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\") In\u00a0[\u00a0]: Copied! <pre>img_dataset.head()\n</pre> img_dataset.head() In\u00a0[\u00a0]: Copied! <pre># Add the seed dataset containing our processed images\ndf_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]]\nconfig_builder.with_seed_dataset(DataFrameSeedSource(df=df_seed))\n</pre> # Add the seed dataset containing our processed images df_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]] config_builder.with_seed_dataset(DataFrameSeedSource(df=df_seed)) In\u00a0[\u00a0]: Copied! <pre># Add a column to generate detailed document summaries\nconfig_builder.add_column(\n    LLMTextColumnConfig(\n        name=\"summary\",\n        model_alias=\"vision\",\n        prompt=(\n            \"Provide a detailed summary of the content in this image in Markdown format. \"\n            \"Start from the top of the image and then describe it from top to bottom. \"\n            \"Place a summary at the bottom.\"\n        ),\n        multi_modal_context=[\n            ImageContext(\n                column_name=\"base64_image\",\n                data_type=ModalityDataType.BASE64,\n                image_format=ImageFormat.PNG,\n            )\n        ],\n    )\n)\n</pre> # Add a column to generate detailed document summaries config_builder.add_column(     LLMTextColumnConfig(         name=\"summary\",         model_alias=\"vision\",         prompt=(             \"Provide a detailed summary of the content in this image in Markdown format. \"             \"Start from the top of the image and then describe it from top to bottom. \"             \"Place a summary at the bottom.\"         ),         multi_modal_context=[             ImageContext(                 column_name=\"base64_image\",                 data_type=ModalityDataType.BASE64,                 image_format=ImageFormat.PNG,             )         ],     ) ) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre># Compare original document with generated summary\nindex = 0  # Change this to view different examples\n\n# Merge preview data with original images for comparison\ncomparison_dataset = preview.dataset.merge(pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\")\n\n# Extract the record for display\nrecord = comparison_dataset.iloc[index]\n\nprint(\"\ud83d\udcc4 Original Document Image:\")\ndisplay(resize_image(record.image, BASE64_IMAGE_HEIGHT))\n\nprint(\"\\n\ud83d\udcdd Generated Summary:\")\nrich.print(Panel(record.summary, title=\"Document Summary\", title_align=\"left\"))\n</pre> # Compare original document with generated summary index = 0  # Change this to view different examples  # Merge preview data with original images for comparison comparison_dataset = preview.dataset.merge(pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\")  # Extract the record for display record = comparison_dataset.iloc[index]  print(\"\ud83d\udcc4 Original Document Image:\") display(resize_image(record.image, BASE64_IMAGE_HEIGHT))  print(\"\\n\ud83d\udcdd Generated Summary:\") rich.print(Panel(record.summary, title=\"Document Summary\", title_align=\"left\")) In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-4\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-4\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"notebook_source/4-providing-images-as-context/#data-designer-tutorial-providing-images-as-context-for-vision-based-data-generation","title":"\ud83c\udfa8 Data Designer Tutorial: Providing Images as Context for Vision-Based Data Generation\u00b6","text":""},{"location":"notebook_source/4-providing-images-as-context/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>This notebook demonstrates how to provide images as context to generate text descriptions using vision-language models.</p> <ul> <li>\u2728 Visual Document Processing: Converting images to chat-ready format for model consumption</li> <li>\ud83d\udd0d Vision-Language Generation: Using vision models to generate detailed summaries from images</li> </ul> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"notebook_source/4-providing-images-as-context/#import-the-essentials","title":"\ud83d\udce6 Import the essentials\u00b6","text":"<ul> <li>The <code>essentials</code> module provides quick access to the most commonly used objects.</li> </ul>"},{"location":"notebook_source/4-providing-images-as-context/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object is responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebook_source/4-providing-images-as-context/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebook_source/4-providing-images-as-context/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebook_source/4-providing-images-as-context/#seed-dataset-creation","title":"\ud83c\udf31 Seed Dataset Creation\u00b6","text":"<p>In this section, we'll prepare our visual documents as a seed dataset for summarization:</p> <ul> <li>Loading Visual Documents: We use the ColPali dataset containing document images</li> <li>Image Processing: Convert images to base64 format for vision model consumption</li> <li>Metadata Extraction: Preserve relevant document information (filename, page number, source, etc.)</li> </ul> <p>The seed dataset will be used to generate detailed text summaries of each document image.</p>"},{"location":"notebook_source/4-providing-images-as-context/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013 preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebook_source/4-providing-images-as-context/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebook_source/4-providing-images-as-context/#visual-inspection","title":"\ud83d\udd0e Visual Inspection\u00b6","text":"<p>Let's compare the original document image with the generated summary to validate quality:</p>"},{"location":"notebook_source/4-providing-images-as-context/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebook_source/4-providing-images-as-context/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Now that you've learned how to use visual context for image summarization in Data Designer, explore more:</p> <ul> <li>Experiment with different vision models for specific document types</li> <li>Try different prompt variations to generate specialized descriptions (e.g., technical details, key findings)</li> <li>Combine vision-based summaries with other column types for multi-modal workflows</li> <li>Apply this pattern to other vision tasks like image captioning, OCR validation, or visual question answering</li> </ul>"},{"location":"notebook_source/_README/","title":"Overview","text":"<p>Welcome to the Data Designer tutorial series! These hands-on notebooks will guide you through the core concepts and features of Data Designer, from basic synthetic data generation to advanced techniques like structured outputs and dataset seeding.</p>"},{"location":"notebook_source/_README/#setting-up-your-environment","title":"\ud83d\ude80 Setting Up Your Environment","text":""},{"location":"notebook_source/_README/#local-setup-best-practices","title":"Local Setup Best Practices","text":"<p>First, download the tutorial from the release assets. To run the tutorial notebooks locally, we recommend using a virtual environment to manage dependencies:</p> uv (Recommended)pip + venv <pre><code># Extract tutorial notebooks\nunzip data_designer_tutorial.zip\ncd data_designer_tutorial\n\n# Launch Jupyter\nuv run jupyter notebook\n</code></pre> <pre><code># Extract tutorial notebooks\nunzip data_designer_tutorial.zip\ncd data_designer_tutorial\n\n# Create Python virtual environment and install required packages\npython -m venv venv\nsource venv/bin/activate\npip install data-designer jupyter\n\n# Launch Jupyter\njupyter notebook\n</code></pre>"},{"location":"notebook_source/_README/#api-keys-and-authentication","title":"API Keys and Authentication","text":"<p>Data Designer is able to interface with various LLM providers. You'll need to set up API keys for the models you want to use:</p> <pre><code># For NVIDIA API Catalog (build.nvidia.com)\nexport NVIDIA_API_KEY=\"your-api-key-here\"\n\n# For OpenAI\nexport OPENAI_API_KEY=\"your-api-key-here\"\n\n# For OpenRouter\nexport OPENROUTER_API_KEY=\"your-api-key-here\"\n</code></pre> <p>For more information, check the Quick Start, Default Model Settings and how to Configure Model Settings Using The CLI.</p>"},{"location":"notebook_source/_README/#tutorial-series","title":"\ud83d\udcda Tutorial Series","text":"<p>The tutorials are designed to be completed in sequence, building upon concepts introduced in previous notebooks:</p>"},{"location":"notebook_source/_README/#1-the-basics","title":"1. The Basics","text":"<p>Learn the fundamentals of Data Designer by generating a simple product review dataset. This notebook covers:</p> <ul> <li>Setting up the <code>DataDesigner</code> interface</li> <li>Configuring models and inference parameters</li> <li>Using built-in samplers (Category, Person, Uniform)</li> <li>Generating LLM text columns with dependencies</li> <li>Understanding the generation workflow</li> </ul> <p>Start here if you're new to Data Designer!</p>"},{"location":"notebook_source/_README/#2-structured-outputs-and-jinja-expressions","title":"2. Structured Outputs and Jinja Expressions","text":"<p>Explore more advanced data generation capabilities:</p> <ul> <li>Creating structured JSON outputs with schemas</li> <li>Using Jinja expressions for derived columns</li> <li>Combining samplers with structured data</li> <li>Building complex data dependencies</li> <li>Working with nested data structures</li> </ul>"},{"location":"notebook_source/_README/#3-seeding-with-an-external-dataset","title":"3. Seeding with an External Dataset","text":"<p>Learn how to leverage existing datasets to guide synthetic data generation:</p> <ul> <li>Loading and using seed datasets</li> <li>Sampling from real data distributions</li> <li>Combining seed data with LLM generation</li> <li>Creating realistic synthetic data based on existing patterns</li> </ul>"},{"location":"notebook_source/_README/#4-providing-images-as-context","title":"4. Providing Images as Context","text":"<p>Learn how to use vision-language models to generate text descriptions from images:</p> <ul> <li>Processing and converting images to base64 format for model consumption</li> <li>Using vision-language models (VLMs) to analyze visual documents</li> <li>Generating detailed summaries from document images</li> <li>Inspecting and validating vision-based generation results</li> </ul>"},{"location":"notebook_source/_README/#important-documentation-sections","title":"\ud83d\udcd6 Important Documentation Sections","text":"<p>Before diving into the tutorials, familiarize yourself with these key documentation sections:</p>"},{"location":"notebook_source/_README/#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Detailed installation instructions for various setups</li> <li>Welcome Guide - Overview of Data Designer capabilities and architecture</li> </ul>"},{"location":"notebook_source/_README/#core-concepts","title":"Core Concepts","text":"<p>Understanding these concepts will help you make the most of the tutorials:</p> <ul> <li>Columns - Learn about different column types (Sampler, LLM, Expression, Validation, etc.)</li> <li>Validators - Understand how to validate generated data with Python, SQL, and remote validators</li> <li>Person Sampling - Learn how to sample realistic person data with demographic attributes</li> </ul>"},{"location":"notebook_source/_README/#code-reference","title":"Code Reference","text":"<p>Quick reference guides for the main configuration objects:</p> <ul> <li>column_configs - All column configuration types</li> <li>config_builder - The <code>DataDesignerConfigBuilder</code> API</li> <li>data_designer_config - Main configuration schema</li> <li>validator_params - Validator configuration options</li> </ul>"},{"location":"notebooks/","title":"Overview","text":"<p>Welcome to the Data Designer tutorial series! These hands-on notebooks will guide you through the core concepts and features of Data Designer, from basic synthetic data generation to advanced techniques like structured outputs and dataset seeding.</p>"},{"location":"notebooks/#setting-up-your-environment","title":"\ud83d\ude80 Setting Up Your Environment","text":""},{"location":"notebooks/#local-setup-best-practices","title":"Local Setup Best Practices","text":"<p>First, download the tutorial from the release assets. To run the tutorial notebooks locally, we recommend using a virtual environment to manage dependencies:</p> uv (Recommended)pip + venv <pre><code># Extract tutorial notebooks\nunzip data_designer_tutorial.zip\ncd data_designer_tutorial\n\n# Launch Jupyter\nuv run jupyter notebook\n</code></pre> <pre><code># Extract tutorial notebooks\nunzip data_designer_tutorial.zip\ncd data_designer_tutorial\n\n# Create Python virtual environment and install required packages\npython -m venv venv\nsource venv/bin/activate\npip install data-designer jupyter\n\n# Launch Jupyter\njupyter notebook\n</code></pre>"},{"location":"notebooks/#api-keys-and-authentication","title":"API Keys and Authentication","text":"<p>Data Designer is able to interface with various LLM providers. You'll need to set up API keys for the models you want to use:</p> <pre><code># For NVIDIA API Catalog (build.nvidia.com)\nexport NVIDIA_API_KEY=\"your-api-key-here\"\n\n# For OpenAI\nexport OPENAI_API_KEY=\"your-api-key-here\"\n\n# For OpenRouter\nexport OPENROUTER_API_KEY=\"your-api-key-here\"\n</code></pre> <p>For more information, check the Quick Start, Default Model Settings and how to Configure Model Settings Using The CLI.</p>"},{"location":"notebooks/#tutorial-series","title":"\ud83d\udcda Tutorial Series","text":"<p>The tutorials are designed to be completed in sequence, building upon concepts introduced in previous notebooks:</p>"},{"location":"notebooks/#1-the-basics","title":"1. The Basics","text":"<p>Learn the fundamentals of Data Designer by generating a simple product review dataset. This notebook covers:</p> <ul> <li>Setting up the <code>DataDesigner</code> interface</li> <li>Configuring models and inference parameters</li> <li>Using built-in samplers (Category, Person, Uniform)</li> <li>Generating LLM text columns with dependencies</li> <li>Understanding the generation workflow</li> </ul> <p>Start here if you're new to Data Designer!</p>"},{"location":"notebooks/#2-structured-outputs-and-jinja-expressions","title":"2. Structured Outputs and Jinja Expressions","text":"<p>Explore more advanced data generation capabilities:</p> <ul> <li>Creating structured JSON outputs with schemas</li> <li>Using Jinja expressions for derived columns</li> <li>Combining samplers with structured data</li> <li>Building complex data dependencies</li> <li>Working with nested data structures</li> </ul>"},{"location":"notebooks/#3-seeding-with-an-external-dataset","title":"3. Seeding with an External Dataset","text":"<p>Learn how to leverage existing datasets to guide synthetic data generation:</p> <ul> <li>Loading and using seed datasets</li> <li>Sampling from real data distributions</li> <li>Combining seed data with LLM generation</li> <li>Creating realistic synthetic data based on existing patterns</li> </ul>"},{"location":"notebooks/#4-providing-images-as-context","title":"4. Providing Images as Context","text":"<p>Learn how to use vision-language models to generate text descriptions from images:</p> <ul> <li>Processing and converting images to base64 format for model consumption</li> <li>Using vision-language models (VLMs) to analyze visual documents</li> <li>Generating detailed summaries from document images</li> <li>Inspecting and validating vision-based generation results</li> </ul>"},{"location":"notebooks/#important-documentation-sections","title":"\ud83d\udcd6 Important Documentation Sections","text":"<p>Before diving into the tutorials, familiarize yourself with these key documentation sections:</p>"},{"location":"notebooks/#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Detailed installation instructions for various setups</li> <li>Welcome Guide - Overview of Data Designer capabilities and architecture</li> </ul>"},{"location":"notebooks/#core-concepts","title":"Core Concepts","text":"<p>Understanding these concepts will help you make the most of the tutorials:</p> <ul> <li>Columns - Learn about different column types (Sampler, LLM, Expression, Validation, etc.)</li> <li>Validators - Understand how to validate generated data with Python, SQL, and remote validators</li> <li>Person Sampling - Learn how to sample realistic person data with demographic attributes</li> </ul>"},{"location":"notebooks/#code-reference","title":"Code Reference","text":"<p>Quick reference guides for the main configuration objects:</p> <ul> <li>column_configs - All column configuration types</li> <li>config_builder - The <code>DataDesignerConfigBuilder</code> API</li> <li>data_designer_config - Main configuration schema</li> <li>validator_params - Validator configuration options</li> </ul>"},{"location":"notebooks/1-the-basics/","title":"The Basics","text":"In\u00a0[1]: Copied! <pre>from data_designer.essentials import (\n    CategorySamplerParams,\n    ChatCompletionInferenceParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    LLMTextColumnConfig,\n    ModelConfig,\n    PersonFromFakerSamplerParams,\n    SamplerColumnConfig,\n    SamplerType,\n    SubcategorySamplerParams,\n    UniformSamplerParams,\n)\n</pre> from data_designer.essentials import (     CategorySamplerParams,     ChatCompletionInferenceParams,     DataDesigner,     DataDesignerConfigBuilder,     LLMTextColumnConfig,     ModelConfig,     PersonFromFakerSamplerParams,     SamplerColumnConfig,     SamplerType,     SubcategorySamplerParams,     UniformSamplerParams, ) In\u00a0[2]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[3]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[4]: Copied! <pre>config_builder = DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[5]: Copied! <pre>config_builder.info.display(\"samplers\")\n</pre> config_builder.info.display(\"samplers\") <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 NeMo Data Designer Samplers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Type               \u2503 Parameter                \u2503 Data Type                         \u2503 Required \u2503 Constraints      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 bernoulli          \u2502 p                        \u2502 number                            \u2502    \u2713     \u2502 &gt;= 0.0, &lt;= 1.0   \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 bernoulli_mixture  \u2502 p                        \u2502 number                            \u2502    \u2713     \u2502 &gt;= 0.0, &lt;= 1.0   \u2502\n\u2502                    \u2502 dist_name                \u2502 string                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 dist_params              \u2502 dict                              \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 binomial           \u2502 n                        \u2502 integer                           \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 p                        \u2502 number                            \u2502    \u2713     \u2502 &gt;= 0.0, &lt;= 1.0   \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 category           \u2502 values                   \u2502 string[] | integer[] | number[]   \u2502    \u2713     \u2502 len &gt; 1          \u2502\n\u2502                    \u2502 weights                  \u2502 number[] | null                   \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 datetime           \u2502 start                    \u2502 string                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 end                      \u2502 string                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 unit                     \u2502 string                            \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 gaussian           \u2502 mean                     \u2502 number                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 stddev                   \u2502 number                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 decimal_places           \u2502 integer | null                    \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 person             \u2502 locale                   \u2502 string                            \u2502          \u2502                  \u2502\n\u2502                    \u2502 sex                      \u2502 string | null                     \u2502          \u2502                  \u2502\n\u2502                    \u2502 city                     \u2502 string | string[] | null          \u2502          \u2502                  \u2502\n\u2502                    \u2502 age_range                \u2502 integer[]                         \u2502          \u2502 len &gt; 2, len &lt; 2 \u2502\n\u2502                    \u2502 select_field_values      \u2502 object | null                     \u2502          \u2502                  \u2502\n\u2502                    \u2502 with_synthetic_personas  \u2502 boolean                           \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 person_from_faker  \u2502 locale                   \u2502 string                            \u2502          \u2502                  \u2502\n\u2502                    \u2502 sex                      \u2502 string | null                     \u2502          \u2502                  \u2502\n\u2502                    \u2502 city                     \u2502 string | string[] | null          \u2502          \u2502                  \u2502\n\u2502                    \u2502 age_range                \u2502 integer[]                         \u2502          \u2502 len &gt; 2, len &lt; 2 \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 poisson            \u2502 mean                     \u2502 number                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 scipy              \u2502 dist_name                \u2502 string                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 dist_params              \u2502 dict                              \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 decimal_places           \u2502 integer | null                    \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 subcategory        \u2502 category                 \u2502 string                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 values                   \u2502 dict                              \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 timedelta          \u2502 dt_min                   \u2502 integer                           \u2502    \u2713     \u2502 &gt;= 0             \u2502\n\u2502                    \u2502 dt_max                   \u2502 integer                           \u2502    \u2713     \u2502 &gt; 0              \u2502\n\u2502                    \u2502 reference_column_name    \u2502 string                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 unit                     \u2502 string                            \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 uniform            \u2502 low                      \u2502 number                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 high                     \u2502 number                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 decimal_places           \u2502 integer | null                    \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 uuid               \u2502 prefix                   \u2502 string | null                     \u2502          \u2502                  \u2502\n\u2502                    \u2502 short_form               \u2502 boolean                           \u2502          \u2502                  \u2502\n\u2502                    \u2502 uppercase                \u2502 boolean                           \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <p>Let's start designing our product review dataset by adding product category and subcategory columns.</p> In\u00a0[6]: Copied! <pre>config_builder.add_column(\n    SamplerColumnConfig(\n        name=\"product_category\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\n                \"Electronics\",\n                \"Clothing\",\n                \"Home &amp; Kitchen\",\n                \"Books\",\n                \"Home Office\",\n            ],\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"product_subcategory\",\n        sampler_type=SamplerType.SUBCATEGORY,\n        params=SubcategorySamplerParams(\n            category=\"product_category\",\n            values={\n                \"Electronics\": [\n                    \"Smartphones\",\n                    \"Laptops\",\n                    \"Headphones\",\n                    \"Cameras\",\n                    \"Accessories\",\n                ],\n                \"Clothing\": [\n                    \"Men's Clothing\",\n                    \"Women's Clothing\",\n                    \"Winter Coats\",\n                    \"Activewear\",\n                    \"Accessories\",\n                ],\n                \"Home &amp; Kitchen\": [\n                    \"Appliances\",\n                    \"Cookware\",\n                    \"Furniture\",\n                    \"Decor\",\n                    \"Organization\",\n                ],\n                \"Books\": [\n                    \"Fiction\",\n                    \"Non-Fiction\",\n                    \"Self-Help\",\n                    \"Textbooks\",\n                    \"Classics\",\n                ],\n                \"Home Office\": [\n                    \"Desks\",\n                    \"Chairs\",\n                    \"Storage\",\n                    \"Office Supplies\",\n                    \"Lighting\",\n                ],\n            },\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"target_age_range\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),\n    )\n)\n\n# Optionally validate that the columns are configured correctly.\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     SamplerColumnConfig(         name=\"product_category\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(             values=[                 \"Electronics\",                 \"Clothing\",                 \"Home &amp; Kitchen\",                 \"Books\",                 \"Home Office\",             ],         ),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"product_subcategory\",         sampler_type=SamplerType.SUBCATEGORY,         params=SubcategorySamplerParams(             category=\"product_category\",             values={                 \"Electronics\": [                     \"Smartphones\",                     \"Laptops\",                     \"Headphones\",                     \"Cameras\",                     \"Accessories\",                 ],                 \"Clothing\": [                     \"Men's Clothing\",                     \"Women's Clothing\",                     \"Winter Coats\",                     \"Activewear\",                     \"Accessories\",                 ],                 \"Home &amp; Kitchen\": [                     \"Appliances\",                     \"Cookware\",                     \"Furniture\",                     \"Decor\",                     \"Organization\",                 ],                 \"Books\": [                     \"Fiction\",                     \"Non-Fiction\",                     \"Self-Help\",                     \"Textbooks\",                     \"Classics\",                 ],                 \"Home Office\": [                     \"Desks\",                     \"Chairs\",                     \"Storage\",                     \"Office Supplies\",                     \"Lighting\",                 ],             },         ),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"target_age_range\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),     ) )  # Optionally validate that the columns are configured correctly. data_designer.validate(config_builder) <pre>[22:47:25] [INFO] \u2705 Validation passed\n</pre> <p>Next, let's add samplers to generate data related to the customer and their review.</p> In\u00a0[7]: Copied! <pre>config_builder.add_column(\n    SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=SamplerType.PERSON_FROM_FAKER,\n        params=PersonFromFakerSamplerParams(age_range=[18, 70], locale=\"en_US\"),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"number_of_stars\",\n        sampler_type=SamplerType.UNIFORM,\n        params=UniformSamplerParams(low=1, high=5),\n        convert_to=\"int\",  # Convert the sampled float to an integer.\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"review_style\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],\n            weights=[1, 2, 2, 1],\n        ),\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     SamplerColumnConfig(         name=\"customer\",         sampler_type=SamplerType.PERSON_FROM_FAKER,         params=PersonFromFakerSamplerParams(age_range=[18, 70], locale=\"en_US\"),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"number_of_stars\",         sampler_type=SamplerType.UNIFORM,         params=UniformSamplerParams(low=1, high=5),         convert_to=\"int\",  # Convert the sampled float to an integer.     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"review_style\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(             values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],             weights=[1, 2, 2, 1],         ),     ) )  data_designer.validate(config_builder) <pre>[22:47:25] [INFO] \u2705 Validation passed\n</pre> In\u00a0[8]: Copied! <pre>config_builder.add_column(\n    LLMTextColumnConfig(\n        name=\"product_name\",\n        prompt=(\n            \"You are a helpful assistant that generates product names. DO NOT add quotes around the product name.\\n\\n\"\n            \"Come up with a creative product name for a product in the '{{ product_category }}' category, focusing \"\n            \"on products related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"\n            \"{{ target_age_range }} years old. Respond with only the product name, no other text.\"\n        ),\n        model_alias=MODEL_ALIAS,\n    )\n)\n\nconfig_builder.add_column(\n    LLMTextColumnConfig(\n        name=\"customer_review\",\n        prompt=(\n            \"You are a customer named {{ customer.first_name }} from {{ customer.city }}, {{ customer.state }}. \"\n            \"You are {{ customer.age }} years old and recently purchased a product called {{ product_name }}. \"\n            \"Write a review of this product, which you gave a rating of {{ number_of_stars }} stars. \"\n            \"The style of the review should be '{{ review_style }}'. \"\n            \"Respond with only the review, no other text.\"\n        ),\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     LLMTextColumnConfig(         name=\"product_name\",         prompt=(             \"You are a helpful assistant that generates product names. DO NOT add quotes around the product name.\\n\\n\"             \"Come up with a creative product name for a product in the '{{ product_category }}' category, focusing \"             \"on products related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"             \"{{ target_age_range }} years old. Respond with only the product name, no other text.\"         ),         model_alias=MODEL_ALIAS,     ) )  config_builder.add_column(     LLMTextColumnConfig(         name=\"customer_review\",         prompt=(             \"You are a customer named {{ customer.first_name }} from {{ customer.city }}, {{ customer.state }}. \"             \"You are {{ customer.age }} years old and recently purchased a product called {{ product_name }}. \"             \"Write a review of this product, which you gave a rating of {{ number_of_stars }} stars. \"             \"The style of the review should be '{{ review_style }}'. \"             \"Respond with only the review, no other text.\"         ),         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) <pre>[22:47:25] [INFO] \u2705 Validation passed\n</pre> In\u00a0[9]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) <pre>[22:47:25] [INFO] \ud83d\udd2d Preview generation in progress\n</pre> <pre>[22:47:25] [INFO] \u2705 Validation passed\n</pre> <pre>[22:47:25] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:47:25] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:47:25] [INFO]   |-- \ud83d\udc40 Checking 'nvidia/nemotron-3-nano-30b-a3b' in provider named 'nvidia' for model alias 'nemotron-nano-v3'...\n</pre> <pre>[22:47:26] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:47:26] [INFO] \ud83c\udfb2 Preparing samplers to generate 2 records across 6 columns\n</pre> <pre>[22:47:26] [INFO] llm-text model configuration for generating column 'product_name'\n</pre> <pre>[22:47:26] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:47:26] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:47:26] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:47:26] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, extra_body={'chat_template_kwargs': {'enable_thinking': False}}, temperature=1.00, top_p=1.00, max_tokens=2048\n</pre> <pre>[22:47:26] [INFO] \ud83d\udc19 Processing llm-text column 'product_name' with 4 concurrent workers\n</pre> <pre>[22:47:27] [INFO] llm-text model configuration for generating column 'customer_review'\n</pre> <pre>[22:47:27] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:47:27] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:47:27] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:47:27] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, extra_body={'chat_template_kwargs': {'enable_thinking': False}}, temperature=1.00, top_p=1.00, max_tokens=2048\n</pre> <pre>[22:47:27] [INFO] \ud83d\udc19 Processing llm-text column 'customer_review' with 4 concurrent workers\n</pre> <pre>[22:47:30] [INFO] \ud83d\udcca Model usage summary:\n{\n    \"nvidia/nemotron-3-nano-30b-a3b\": {\n        \"token_usage\": {\n            \"input_tokens\": 354,\n            \"output_tokens\": 1000,\n            \"total_tokens\": 1354\n        },\n        \"request_usage\": {\n            \"successful_requests\": 4,\n            \"failed_requests\": 0,\n            \"total_requests\": 4\n        },\n        \"tokens_per_second\": 329,\n        \"requests_per_minute\": 58\n    }\n}\n</pre> <pre>[22:47:30] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:47:30] [INFO]   |-- \ud83c\udfb2 column: 'product_category'\n</pre> <pre>[22:47:30] [INFO]   |-- \ud83c\udfb2 column: 'product_subcategory'\n</pre> <pre>[22:47:30] [INFO]   |-- \ud83c\udfb2 column: 'target_age_range'\n</pre> <pre>[22:47:30] [INFO]   |-- \ud83c\udfb2 column: 'customer'\n</pre> <pre>[22:47:30] [INFO]   |-- \ud83c\udfb2 column: 'number_of_stars'\n</pre> <pre>[22:47:30] [INFO]   |-- \ud83c\udfb2 column: 'review_style'\n</pre> <pre>[22:47:30] [INFO]   |-- \ud83d\udcdd column: 'product_name'\n</pre> <pre>[22:47:30] [INFO]   |-- \ud83d\udcdd column: 'customer_review'\n</pre> <pre>[22:47:30] [INFO] \ud83c\udf7e Preview complete!\n</pre> In\u00a0[10]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() <pre>                                                                                                                   \n                                                 Generated Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name                \u2503 Value                                                                                     \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_category    \u2502 Home &amp; Kitchen                                                                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product_subcategory \u2502 Appliances                                                                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 target_age_range    \u2502 65+                                                                                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer            \u2502 {                                                                                         \u2502\n\u2502                     \u2502     'uuid': '51473fd3-aa4c-4a7d-b8be-9ab8ba5d3fa1',                                       \u2502\n\u2502                     \u2502     'locale': 'en_US',                                                                    \u2502\n\u2502                     \u2502     'first_name': 'Kenneth',                                                              \u2502\n\u2502                     \u2502     'last_name': 'Campbell',                                                              \u2502\n\u2502                     \u2502     'middle_name': None,                                                                  \u2502\n\u2502                     \u2502     'sex': 'Male',                                                                        \u2502\n\u2502                     \u2502     'street_number': '475',                                                               \u2502\n\u2502                     \u2502     'street_name': 'Johnson Pine',                                                        \u2502\n\u2502                     \u2502     'city': 'North Geoffrey',                                                             \u2502\n\u2502                     \u2502     'state': 'Connecticut',                                                               \u2502\n\u2502                     \u2502     'postcode': '57694',                                                                  \u2502\n\u2502                     \u2502     'age': 43,                                                                            \u2502\n\u2502                     \u2502     'birth_date': '1982-05-09',                                                           \u2502\n\u2502                     \u2502     'country': 'Afghanistan',                                                             \u2502\n\u2502                     \u2502     'marital_status': 'widowed',                                                          \u2502\n\u2502                     \u2502     'education_level': 'graduate',                                                        \u2502\n\u2502                     \u2502     'unit': '',                                                                           \u2502\n\u2502                     \u2502     'occupation': 'Restaurant manager, fast food',                                        \u2502\n\u2502                     \u2502     'phone_number': '544-378-8947x79982',                                                 \u2502\n\u2502                     \u2502     'bachelors_field': 'stem_related'                                                     \u2502\n\u2502                     \u2502 }                                                                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 number_of_stars     \u2502 4                                                                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 review_style        \u2502 rambling                                                                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product_name        \u2502 Sunlit Enterprise                                                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_review     \u2502 I\u2019ve been staring at this Sunlit Enterprise product for the last half hour, sipping       \u2502\n\u2502                     \u2502 lukewarm coffee that\u2019s gone a bit bitter, and I\u2019m still trying to decide if the way the   \u2502\n\u2502                     \u2502 light catches the edge of the thing is a sign of destiny or just a coincidence. It\u2019s that \u2502\n\u2502                     \u2502 sort of feeling you get when you\u2019re standing outside on a sunny afternoon in Connecticut  \u2502\n\u2502                     \u2502 and the whole world feels like it\u2019s humming a little louder\u2014like maybe the universe is    \u2502\n\u2502                     \u2502 tapping you on the shoulder and saying, \u201cHey, Kenneth, take a breath.\u201d I\u2019m 43 now, and    \u2502\n\u2502                     \u2502 I\u2019ve collected enough odd trinkets and half\u2011finished projects to fill a modest attic, but \u2502\n\u2502                     \u2502 this thing\u2014this sleek, almost\u2011organic piece of hardware that promises to bring a little   \u2502\n\u2502                     \u2502 \u201csunlit\u201d into the everyday\u2014has got me feeling a little windblown and oddly hopeful.       \u2502\n\u2502                     \u2502                                                                                           \u2502\n\u2502                     \u2502 It arrived in a box that smelled faintly of pine and a hint of something like old         \u2502\n\u2502                     \u2502 paperbacks, and when I lifted it out I swear the glass case that\u2019s meant to hold it       \u2502\n\u2502                     \u2502 seemed to glow just a tad, like a promise tucked into a pocket. The packaging itself is a \u2502\n\u2502                     \u2502 study in subtle elegance: the matte black cardboard with a silver embossing that reads    \u2502\n\u2502                     \u2502 \u201cSunlit Enterprise\u201d in letters that feel almost handwritten\u2014makes me wonder if the        \u2502\n\u2502                     \u2502 designers had a poet in mind. And then there\u2019s the product itself\u2014well, I\u2019m not entirely  \u2502\n\u2502                     \u2502 sure how to put it into words without sounding like I\u2019m describing a sunrise that\u2019s been  \u2502\n\u2502                     \u2502 packaged and sold on a marketplace. It hums, it glows, it makes little noises when you    \u2502\n\u2502                     \u2502 tap it, and somehow, after a long day of paperwork and the endless ping of emails, it     \u2502\n\u2502                     \u2502 feels like a tiny, manufactured sunrise that reminds you there\u2019s still a sliver of day    \u2502\n\u2502                     \u2502 left to feel good about.                                                                  \u2502\n\u2502                     \u2502                                                                                           \u2502\n\u2502                     \u2502 I gave it four stars because, honestly, there\u2019s a part of me that wonders if I\u2019m just     \u2502\n\u2502                     \u2502 being captivated by the packaging and the little theatrical flourishes they\u2019ve slipped    \u2502\n\u2502                     \u2502 in\u2014those tiny nods to the idea that technology can be poetic. I\u2019m not saying it\u2019s         \u2502\n\u2502                     \u2502 flawless; there are moments when the app feels a bit clunky, and I\u2019ve caught myself       \u2502\n\u2502                     \u2502 staring at it wondering whether I\u2019m really seeing a sunrise or just a well\u2011edited video.  \u2502\n\u2502                     \u2502 But the truth is, it has nudged me into a little ritual of pausing, turning a knob,       \u2502\n\u2502                     \u2502 watching the light shift, and feeling a tiny spark of joy that makes the otherwise        \u2502\n\u2502                     \u2502 drudgery\u2011laden afternoon feel\u2026 a fraction brighter. It\u2019s the kind of product that can sit \u2502\n\u2502                     \u2502 on a shelf and quietly demand attention, or it can be tucked away in a drawer and only    \u2502\n\u2502                     \u2502 reveal itself when you need a reminder that sunlight can be summoned in the most          \u2502\n\u2502                     \u2502 unexpected ways.                                                                          \u2502\n\u2502                     \u2502                                                                                           \u2502\n\u2502                     \u2502 If I were to sum it up in a rambling, almost off\u2011beat way\u2014like a conversation with a      \u2502\n\u2502                     \u2502 neighbor over the fence while they\u2019re trimming their hedges\u2014this Sunlit Enterprise has    \u2502\n\u2502                     \u2502 become my little personal sunrise, a reminder that sometimes, you can chase light without \u2502\n\u2502                     \u2502 stepping outside. It\u2019s a mild, glowing thing that doesn\u2019t solve any grand problems but    \u2502\n\u2502                     \u2502 does manage to inject a little extra warmth into my day, enough to make me want to keep   \u2502\n\u2502                     \u2502 it close, to glance at it every now and then, and to smile quietly when the light catches \u2502\n\u2502                     \u2502 the edge just right. So, yes, four stars\u2014because it\u2019s not perfect, but it\u2019s close enough  \u2502\n\u2502                     \u2502 to make a 43\u2011year\u2011old from North Geoffrey, Connecticut, who\u2019s just turned a corner on a   \u2502\n\u2502                     \u2502 random Tuesday feel like a kid watching the first rays of sun break over the hills, and   \u2502\n\u2502                     \u2502 that, to me, is worth a little extra.                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                    [index: 0]                                                     \n</pre> In\u00a0[11]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset Out[11]: product_category product_subcategory target_age_range customer number_of_stars review_style product_name customer_review 0 Home &amp; Kitchen Appliances 65+ {'uuid': '51473fd3-aa4c-4a7d-b8be-9ab8ba5d3fa1... 4 rambling Sunlit Enterprise I\u2019ve been staring at this Sunlit Enterprise pr... 1 Clothing Activewear 18-25 {'uuid': '82665ba8-8bb8-48e6-a354-4b9923ae8c71... 1 detailed Velocity\u202fFlex\u202fJersey Rating: \u2605\u2606\u2606\u2606\u2606  \\nI bought the Velocity\u202fFlex\u202fJe... In\u00a0[12]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2                               \u2502 8                               \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83c\udfb2 Sampler Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                    \u2503       data type \u2503            number unique values \u2503               sampler type \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_category               \u2502          string \u2502                      2 (100.0%) \u2502                   category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product_subcategory            \u2502          string \u2502                      2 (100.0%) \u2502                subcategory \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 target_age_range               \u2502          string \u2502                      2 (100.0%) \u2502                   category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer                       \u2502            dict \u2502                      2 (100.0%) \u2502          person_from_faker \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 number_of_stars                \u2502             int \u2502                      2 (100.0%) \u2502                    uniform \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 review_style                   \u2502          string \u2502                      2 (100.0%) \u2502                   category \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83d\udcdd LLM-Text Columns                                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                       \u2503               \u2503                            \u2503     prompt tokens \u2503      completion tokens \u2503\n\u2503 column name           \u2503     data type \u2503       number unique values \u2503        per record \u2503             per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_name          \u2502        string \u2502                 2 (100.0%) \u2502      74.5 +/- 0.5 \u2502            5.5 +/- 3.5 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_review       \u2502        string \u2502                 2 (100.0%) \u2502      70.5 +/- 3.5 \u2502        478.5 +/- 391.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> In\u00a0[13]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-1\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-1\") <pre>[22:47:30] [INFO] \ud83c\udfa8 Creating Data Designer dataset\n</pre> <pre>[22:47:30] [INFO] \u2705 Validation passed\n</pre> <pre>[22:47:30] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:47:30] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:47:30] [INFO]   |-- \ud83d\udc40 Checking 'nvidia/nemotron-3-nano-30b-a3b' in provider named 'nvidia' for model alias 'nemotron-nano-v3'...\n</pre> <pre>[22:47:31] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:47:31] [INFO] \u23f3 Processing batch 1 of 1\n</pre> <pre>[22:47:31] [INFO] \ud83c\udfb2 Preparing samplers to generate 10 records across 6 columns\n</pre> <pre>[22:47:31] [INFO] llm-text model configuration for generating column 'product_name'\n</pre> <pre>[22:47:31] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:47:31] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:47:31] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:47:31] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, extra_body={'chat_template_kwargs': {'enable_thinking': False}}, temperature=1.00, top_p=1.00, max_tokens=2048\n</pre> <pre>[22:47:31] [INFO] \ud83d\udc19 Processing llm-text column 'product_name' with 4 concurrent workers\n</pre> <pre>[22:47:32] [INFO] llm-text model configuration for generating column 'customer_review'\n</pre> <pre>[22:47:32] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:47:32] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:47:32] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:47:32] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, extra_body={'chat_template_kwargs': {'enable_thinking': False}}, temperature=1.00, top_p=1.00, max_tokens=2048\n</pre> <pre>[22:47:32] [INFO] \ud83d\udc19 Processing llm-text column 'customer_review' with 4 concurrent workers\n</pre> <pre>[22:47:35] [INFO] \ud83d\udcca Model usage summary:\n{\n    \"nvidia/nemotron-3-nano-30b-a3b\": {\n        \"token_usage\": {\n            \"input_tokens\": 1959,\n            \"output_tokens\": 2727,\n            \"total_tokens\": 4686\n        },\n        \"request_usage\": {\n            \"successful_requests\": 20,\n            \"failed_requests\": 0,\n            \"total_requests\": 20\n        },\n        \"tokens_per_second\": 964,\n        \"requests_per_minute\": 247\n    }\n}\n</pre> <pre>[22:47:35] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:47:35] [INFO]   |-- \ud83c\udfb2 column: 'product_category'\n</pre> <pre>[22:47:35] [INFO]   |-- \ud83c\udfb2 column: 'product_subcategory'\n</pre> <pre>[22:47:35] [INFO]   |-- \ud83c\udfb2 column: 'target_age_range'\n</pre> <pre>[22:47:35] [INFO]   |-- \ud83c\udfb2 column: 'customer'\n</pre> <pre>[22:47:35] [INFO]   |-- \ud83c\udfb2 column: 'number_of_stars'\n</pre> <pre>[22:47:35] [INFO]   |-- \ud83c\udfb2 column: 'review_style'\n</pre> <pre>[22:47:35] [INFO]   |-- \ud83d\udcdd column: 'product_name'\n</pre> <pre>[22:47:35] [INFO]   |-- \ud83d\udcdd column: 'customer_review'\n</pre> In\u00a0[14]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() Out[14]: product_category product_subcategory target_age_range customer number_of_stars review_style product_name customer_review 0 Clothing Men's Clothing 50-65 {'age': 48, 'bachelors_field': 'stem', 'birth_... 5 brief Timeless Classic Fit Chino Pant for Men 50-65 5-star review: These chino pants finally fit l... 1 Home &amp; Kitchen Decor 25-35 {'age': 25, 'bachelors_field': 'no_degree', 'b... 4 detailed Twilight Hearth Decor Lantern I\u2019m Ashley, a 25\u2011year\u2011old resident of East Geo... 2 Electronics Headphones 65+ {'age': 61, 'bachelors_field': 'no_degree', 'b... 3 detailed Zenith Hear\"\u202fis not appropriate because it inc... SerenitySound Pro offers a pleasant listening ... 3 Home &amp; Kitchen Appliances 50-65 {'age': 62, 'bachelors_field': 'no_degree', 'b... 1 structured with bullet points Sunset Glow Steamer **Review: Sunset Glow Steamer - \u2605\u2605\u2606\u2606\u2606 (1 star)... 4 Home &amp; Kitchen Organization 65+ {'age': 50, 'bachelors_field': 'no_degree', 'b... 4 brief Golden Comfort Cabinet Commander **Review (4 stars \u2013 brief):**   The Golden Com... In\u00a0[15]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 10                              \u2502 8                               \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83c\udfb2 Sampler Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                    \u2503       data type \u2503            number unique values \u2503               sampler type \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_category               \u2502          string \u2502                       5 (50.0%) \u2502                   category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product_subcategory            \u2502          string \u2502                     10 (100.0%) \u2502                subcategory \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 target_age_range               \u2502          string \u2502                       4 (40.0%) \u2502                   category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer                       \u2502            dict \u2502                     10 (100.0%) \u2502          person_from_faker \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 number_of_stars                \u2502             int \u2502                       5 (50.0%) \u2502                    uniform \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 review_style                   \u2502          string \u2502                       4 (40.0%) \u2502                   category \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83d\udcdd LLM-Text Columns                                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                       \u2503               \u2503                            \u2503     prompt tokens \u2503      completion tokens \u2503\n\u2503 column name           \u2503     data type \u2503       number unique values \u2503        per record \u2503             per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_name          \u2502        string \u2502                10 (100.0%) \u2502      74.0 +/- 0.8 \u2502           8.0 +/- 35.5 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_review       \u2502        string \u2502                10 (100.0%) \u2502     72.0 +/- 33.9 \u2502        215.0 +/- 184.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre>"},{"location":"notebooks/1-the-basics/#data-designer-tutorial-the-basics","title":"\ud83c\udfa8 Data Designer Tutorial: The Basics\u00b6","text":""},{"location":"notebooks/1-the-basics/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>This notebook demonstrates the basics of Data Designer by generating a simple product review dataset.</p>"},{"location":"notebooks/1-the-basics/#import-the-essentials","title":"\ud83d\udce6 Import the essentials\u00b6","text":"<ul> <li>The <code>essentials</code> module provides quick access to the most commonly used objects.</li> </ul>"},{"location":"notebooks/1-the-basics/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object is responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebooks/1-the-basics/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebooks/1-the-basics/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebooks/1-the-basics/#getting-started-with-sampler-columns","title":"\ud83c\udfb2 Getting started with sampler columns\u00b6","text":"<ul> <li><p>Sampler columns offer non-LLM based generation of synthetic data.</p> </li> <li><p>They are particularly useful for steering the diversity of the generated data, as we demonstrate below.</p> </li> </ul> <p>You can view available samplers using the config builder's <code>info</code> property:</p>"},{"location":"notebooks/1-the-basics/#llm-generated-columns","title":"\ud83e\udd9c LLM-generated columns\u00b6","text":"<ul> <li><p>The real power of Data Designer comes from leveraging LLMs to generate text, code, and structured data.</p> </li> <li><p>When prompting the LLM, we can use Jinja templating to reference other columns in the dataset.</p> </li> <li><p>As we see below, nested json fields can be accessed using dot notation.</p> </li> </ul>"},{"location":"notebooks/1-the-basics/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebooks/1-the-basics/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebooks/1-the-basics/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebooks/1-the-basics/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Now that you've seen the basics of Data Designer, check out the following notebooks to learn more about:</p> <ul> <li><p>Structured outputs and jinja expressions</p> </li> <li><p>Seeding synthetic data generation with an external dataset</p> </li> <li><p>Providing images as context</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/","title":"Structured Outputs and Jinja Expressions","text":"In\u00a0[1]: Copied! <pre>from data_designer.essentials import (\n    CategorySamplerParams,\n    ChatCompletionInferenceParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    ExpressionColumnConfig,\n    LLMStructuredColumnConfig,\n    ModelConfig,\n    PersonFromFakerSamplerParams,\n    SamplerColumnConfig,\n    SamplerType,\n    SubcategorySamplerParams,\n)\n</pre> from data_designer.essentials import (     CategorySamplerParams,     ChatCompletionInferenceParams,     DataDesigner,     DataDesignerConfigBuilder,     ExpressionColumnConfig,     LLMStructuredColumnConfig,     ModelConfig,     PersonFromFakerSamplerParams,     SamplerColumnConfig,     SamplerType,     SubcategorySamplerParams, ) In\u00a0[2]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[3]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[4]: Copied! <pre>config_builder = DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[5]: Copied! <pre>from decimal import Decimal\nfrom typing import Literal\n\nfrom pydantic import BaseModel, Field\n\n\n# We define a Product schema so that the name, description, and price are generated\n# in one go, with the types and constraints specified.\nclass Product(BaseModel):\n    name: str = Field(description=\"The name of the product\")\n    description: str = Field(description=\"A description of the product\")\n    price: Decimal = Field(description=\"The price of the product\", ge=10, le=1000, decimal_places=2)\n\n\nclass ProductReview(BaseModel):\n    rating: int = Field(description=\"The rating of the product\", ge=1, le=5)\n    customer_mood: Literal[\"irritated\", \"mad\", \"happy\", \"neutral\", \"excited\"] = Field(\n        description=\"The mood of the customer\"\n    )\n    review: str = Field(description=\"A review of the product\")\n</pre> from decimal import Decimal from typing import Literal  from pydantic import BaseModel, Field   # We define a Product schema so that the name, description, and price are generated # in one go, with the types and constraints specified. class Product(BaseModel):     name: str = Field(description=\"The name of the product\")     description: str = Field(description=\"A description of the product\")     price: Decimal = Field(description=\"The price of the product\", ge=10, le=1000, decimal_places=2)   class ProductReview(BaseModel):     rating: int = Field(description=\"The rating of the product\", ge=1, le=5)     customer_mood: Literal[\"irritated\", \"mad\", \"happy\", \"neutral\", \"excited\"] = Field(         description=\"The mood of the customer\"     )     review: str = Field(description=\"A review of the product\") <p>Next, let's design our product review dataset using a few more tricks compared to the previous notebook.</p> In\u00a0[6]: Copied! <pre># Since we often only want a few attributes from Person objects, we can\n# set drop=True in the column config to drop the column from the final dataset.\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=SamplerType.PERSON_FROM_FAKER,\n        params=PersonFromFakerSamplerParams(),\n        drop=True,\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"product_category\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\n                \"Electronics\",\n                \"Clothing\",\n                \"Home &amp; Kitchen\",\n                \"Books\",\n                \"Home Office\",\n            ],\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"product_subcategory\",\n        sampler_type=SamplerType.SUBCATEGORY,\n        params=SubcategorySamplerParams(\n            category=\"product_category\",\n            values={\n                \"Electronics\": [\n                    \"Smartphones\",\n                    \"Laptops\",\n                    \"Headphones\",\n                    \"Cameras\",\n                    \"Accessories\",\n                ],\n                \"Clothing\": [\n                    \"Men's Clothing\",\n                    \"Women's Clothing\",\n                    \"Winter Coats\",\n                    \"Activewear\",\n                    \"Accessories\",\n                ],\n                \"Home &amp; Kitchen\": [\n                    \"Appliances\",\n                    \"Cookware\",\n                    \"Furniture\",\n                    \"Decor\",\n                    \"Organization\",\n                ],\n                \"Books\": [\n                    \"Fiction\",\n                    \"Non-Fiction\",\n                    \"Self-Help\",\n                    \"Textbooks\",\n                    \"Classics\",\n                ],\n                \"Home Office\": [\n                    \"Desks\",\n                    \"Chairs\",\n                    \"Storage\",\n                    \"Office Supplies\",\n                    \"Lighting\",\n                ],\n            },\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"target_age_range\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),\n    )\n)\n\n# Sampler columns support conditional params, which are used if the condition is met.\n# In this example, we set the review style to rambling if the target age range is 18-25.\n# Note conditional parameters are only supported for Sampler column types.\nconfig_builder.add_column(\n    SamplerColumnConfig(\n        name=\"review_style\",\n        sampler_type=SamplerType.CATEGORY,\n        params=CategorySamplerParams(\n            values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],\n            weights=[1, 2, 2, 1],\n        ),\n        conditional_params={\n            \"target_age_range == '18-25'\": CategorySamplerParams(values=[\"rambling\"]),\n        },\n    )\n)\n\n# Optionally validate that the columns are configured correctly.\ndata_designer.validate(config_builder)\n</pre> # Since we often only want a few attributes from Person objects, we can # set drop=True in the column config to drop the column from the final dataset. config_builder.add_column(     SamplerColumnConfig(         name=\"customer\",         sampler_type=SamplerType.PERSON_FROM_FAKER,         params=PersonFromFakerSamplerParams(),         drop=True,     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"product_category\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(             values=[                 \"Electronics\",                 \"Clothing\",                 \"Home &amp; Kitchen\",                 \"Books\",                 \"Home Office\",             ],         ),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"product_subcategory\",         sampler_type=SamplerType.SUBCATEGORY,         params=SubcategorySamplerParams(             category=\"product_category\",             values={                 \"Electronics\": [                     \"Smartphones\",                     \"Laptops\",                     \"Headphones\",                     \"Cameras\",                     \"Accessories\",                 ],                 \"Clothing\": [                     \"Men's Clothing\",                     \"Women's Clothing\",                     \"Winter Coats\",                     \"Activewear\",                     \"Accessories\",                 ],                 \"Home &amp; Kitchen\": [                     \"Appliances\",                     \"Cookware\",                     \"Furniture\",                     \"Decor\",                     \"Organization\",                 ],                 \"Books\": [                     \"Fiction\",                     \"Non-Fiction\",                     \"Self-Help\",                     \"Textbooks\",                     \"Classics\",                 ],                 \"Home Office\": [                     \"Desks\",                     \"Chairs\",                     \"Storage\",                     \"Office Supplies\",                     \"Lighting\",                 ],             },         ),     ) )  config_builder.add_column(     SamplerColumnConfig(         name=\"target_age_range\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),     ) )  # Sampler columns support conditional params, which are used if the condition is met. # In this example, we set the review style to rambling if the target age range is 18-25. # Note conditional parameters are only supported for Sampler column types. config_builder.add_column(     SamplerColumnConfig(         name=\"review_style\",         sampler_type=SamplerType.CATEGORY,         params=CategorySamplerParams(             values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],             weights=[1, 2, 2, 1],         ),         conditional_params={             \"target_age_range == '18-25'\": CategorySamplerParams(values=[\"rambling\"]),         },     ) )  # Optionally validate that the columns are configured correctly. data_designer.validate(config_builder) <pre>[22:47:42] [INFO] \u2705 Validation passed\n</pre> <p>Next, we will use more advanced Jinja expressions to create new columns.</p> <p>Jinja expressions let you:</p> <ul> <li><p>Access nested attributes: <code>{{ customer.first_name }}</code></p> </li> <li><p>Combine values: <code>{{ customer.first_name }} {{ customer.last_name }}</code></p> </li> <li><p>Use conditional logic: <code>{% if condition %}...{% endif %}</code></p> </li> </ul> In\u00a0[7]: Copied! <pre># We can create new columns using Jinja expressions that reference\n# existing columns, including attributes of nested objects.\nconfig_builder.add_column(\n    ExpressionColumnConfig(name=\"customer_name\", expr=\"{{ customer.first_name }} {{ customer.last_name }}\")\n)\n\nconfig_builder.add_column(ExpressionColumnConfig(name=\"customer_age\", expr=\"{{ customer.age }}\"))\n\nconfig_builder.add_column(\n    LLMStructuredColumnConfig(\n        name=\"product\",\n        prompt=(\n            \"Create a product in the '{{ product_category }}' category, focusing on products  \"\n            \"related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"\n            \"{{ target_age_range }} years old. The product should be priced between $10 and $1000.\"\n        ),\n        output_format=Product,\n        model_alias=MODEL_ALIAS,\n    )\n)\n\n# We can even use if/else logic in our Jinja expressions to create more complex prompt patterns.\nconfig_builder.add_column(\n    LLMStructuredColumnConfig(\n        name=\"customer_review\",\n        prompt=(\n            \"Your task is to write a review for the following product:\\n\\n\"\n            \"Product Name: {{ product.name }}\\n\"\n            \"Product Description: {{ product.description }}\\n\"\n            \"Price: {{ product.price }}\\n\\n\"\n            \"Imagine your name is {{ customer_name }} and you are from {{ customer.city }}, {{ customer.state }}. \"\n            \"Write the review in a style that is '{{ review_style }}'.\"\n            \"{% if target_age_range == '18-25' %}\"\n            \"Make sure the review is more informal and conversational.\\n\"\n            \"{% else %}\"\n            \"Make sure the review is more formal and structured.\\n\"\n            \"{% endif %}\"\n            \"The review field should contain only the review, no other text.\"\n        ),\n        output_format=ProductReview,\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> # We can create new columns using Jinja expressions that reference # existing columns, including attributes of nested objects. config_builder.add_column(     ExpressionColumnConfig(name=\"customer_name\", expr=\"{{ customer.first_name }} {{ customer.last_name }}\") )  config_builder.add_column(ExpressionColumnConfig(name=\"customer_age\", expr=\"{{ customer.age }}\"))  config_builder.add_column(     LLMStructuredColumnConfig(         name=\"product\",         prompt=(             \"Create a product in the '{{ product_category }}' category, focusing on products  \"             \"related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"             \"{{ target_age_range }} years old. The product should be priced between $10 and $1000.\"         ),         output_format=Product,         model_alias=MODEL_ALIAS,     ) )  # We can even use if/else logic in our Jinja expressions to create more complex prompt patterns. config_builder.add_column(     LLMStructuredColumnConfig(         name=\"customer_review\",         prompt=(             \"Your task is to write a review for the following product:\\n\\n\"             \"Product Name: {{ product.name }}\\n\"             \"Product Description: {{ product.description }}\\n\"             \"Price: {{ product.price }}\\n\\n\"             \"Imagine your name is {{ customer_name }} and you are from {{ customer.city }}, {{ customer.state }}. \"             \"Write the review in a style that is '{{ review_style }}'.\"             \"{% if target_age_range == '18-25' %}\"             \"Make sure the review is more informal and conversational.\\n\"             \"{% else %}\"             \"Make sure the review is more formal and structured.\\n\"             \"{% endif %}\"             \"The review field should contain only the review, no other text.\"         ),         output_format=ProductReview,         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) <pre>[22:47:42] [INFO] \u2705 Validation passed\n</pre> In\u00a0[8]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) <pre>[22:47:42] [INFO] \ud83d\udc40 Preview generation in progress\n</pre> <pre>[22:47:42] [INFO] \u2705 Validation passed\n</pre> <pre>[22:47:42] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:47:42] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:47:42] [INFO]   |-- \ud83d\udc40 Checking 'nvidia/nemotron-3-nano-30b-a3b' in provider named 'nvidia' for model alias 'nemotron-nano-v3'...\n</pre> <pre>[22:47:43] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:47:43] [INFO] \ud83c\udfb2 Preparing samplers to generate 2 records across 5 columns\n</pre> <pre>[22:47:43] [INFO] \ud83e\udde9 Generating column `customer_name` from expression\n</pre> <pre>[22:47:43] [INFO] \ud83e\udde9 Generating column `customer_age` from expression\n</pre> <pre>[22:47:43] [INFO] llm-structured model configuration for generating column 'product'\n</pre> <pre>[22:47:43] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:47:43] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:47:43] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:47:43] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, extra_body={'chat_template_kwargs': {'enable_thinking': False}}, temperature=1.00, top_p=1.00, max_tokens=2048\n</pre> <pre>[22:47:43] [INFO] \ud83d\udc19 Processing llm-structured column 'product' with 4 concurrent workers\n</pre> <pre>[22:47:44] [INFO] llm-structured model configuration for generating column 'customer_review'\n</pre> <pre>[22:47:44] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:47:44] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:47:44] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:47:44] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, extra_body={'chat_template_kwargs': {'enable_thinking': False}}, temperature=1.00, top_p=1.00, max_tokens=2048\n</pre> <pre>[22:47:44] [INFO] \ud83d\udc19 Processing llm-structured column 'customer_review' with 4 concurrent workers\n</pre> <pre>[22:47:45] [INFO] \ud83d\udcca Model usage summary:\n{\n    \"nvidia/nemotron-3-nano-30b-a3b\": {\n        \"token_usage\": {\n            \"input_tokens\": 1312,\n            \"output_tokens\": 749,\n            \"total_tokens\": 2061\n        },\n        \"request_usage\": {\n            \"successful_requests\": 4,\n            \"failed_requests\": 0,\n            \"total_requests\": 4\n        },\n        \"tokens_per_second\": 872,\n        \"requests_per_minute\": 101\n    }\n}\n</pre> <pre>[22:47:45] [INFO] \ud83d\ude48 Dropping columns: ['customer']\n</pre> <pre>[22:47:45] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:47:45] [INFO]   |-- \ud83c\udfb2 column: 'product_category'\n</pre> <pre>[22:47:45] [INFO]   |-- \ud83c\udfb2 column: 'product_subcategory'\n</pre> <pre>[22:47:45] [INFO]   |-- \ud83c\udfb2 column: 'target_age_range'\n</pre> <pre>[22:47:45] [INFO]   |-- \ud83c\udfb2 column: 'review_style'\n</pre> <pre>[22:47:45] [INFO]   |-- \ud83e\udde9 column: 'customer_name'\n</pre> <pre>[22:47:45] [INFO]   |-- \ud83e\udde9 column: 'customer_age'\n</pre> <pre>[22:47:45] [INFO]   |-- \ud83d\uddc2\ufe0f column: 'product'\n</pre> <pre>[22:47:45] [INFO]   |-- \ud83d\uddc2\ufe0f column: 'customer_review'\n</pre> <pre>[22:47:45] [INFO] \ud83c\udf8a Preview complete!\n</pre> In\u00a0[9]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() <pre>                                                                                                                   \n                                                 Generated Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name                \u2503 Value                                                                                     \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_category    \u2502 Home &amp; Kitchen                                                                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product_subcategory \u2502 Furniture                                                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 target_age_range    \u2502 65+                                                                                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 review_style        \u2502 rambling                                                                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_name       \u2502 Katie Anderson                                                                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_age        \u2502 69                                                                                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product             \u2502 {                                                                                         \u2502\n\u2502                     \u2502     'name': 'ErgoLift Comfort Seat',                                                      \u2502\n\u2502                     \u2502     'description': 'A sturdy, slip\u2011resistant wooden chair with a raised, padded seat and  \u2502\n\u2502                     \u2502 armrests, designed to make standing and sitting easier for seniors. Features a supportive \u2502\n\u2502                     \u2502 back, rounded edges for safety, and a compact footprint that fits neatly in any kitchen   \u2502\n\u2502                     \u2502 or dining area.',                                                                         \u2502\n\u2502                     \u2502     'price': 129.99                                                                       \u2502\n\u2502                     \u2502 }                                                                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_review     \u2502 {                                                                                         \u2502\n\u2502                     \u2502     'rating': 4,                                                                          \u2502\n\u2502                     \u2502     'customer_mood': 'happy',                                                             \u2502\n\u2502                     \u2502     'review': 'The ErgoLift Comfort Seat arrived promptly and, after a brief assembly     \u2502\n\u2502                     \u2502 that required only a couple of wooden screws, it settled into my kitchen nook with an     \u2502\n\u2502                     \u2502 effortless grace. The sturdy, slip\u2011resistant wooden frame gives the chair a reassuring    \u2502\n\u2502                     \u2502 heft, while the raised, padded seat cradles my hips like a gentle embrace, and the        \u2502\n\u2502                     \u2502 armrests\u2014soft yet firm\u2014allow me to push up and lower myself without the wobble I once     \u2502\n\u2502                     \u2502 feared. Its supportive back arches just enough to encourage upright posture, and the      \u2502\n\u2502                     \u2502 rounded edges eliminate any worry of accidental bumps, making the experience feel both    \u2502\n\u2502                     \u2502 safe and dignified. Though the price of $129.99 is modestly higher than a basic chair,    \u2502\n\u2502                     \u2502 the thoughtful design, compact footprint, and the way it transforms daily standing and    \u2502\n\u2502                     \u2502 sitting into a smoother, more confident ritual make it a worthwhile investment for any    \u2502\n\u2502                     \u2502 senior seeking comfort without compromise. In short, I am genuinely pleased, and the      \u2502\n\u2502                     \u2502 chair has become an indispensable part of my everyday routine.'                           \u2502\n\u2502                     \u2502 }                                                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                    [index: 0]                                                     \n</pre> In\u00a0[10]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset Out[10]: product_category product_subcategory target_age_range review_style customer_name customer_age product customer_review 0 Home &amp; Kitchen Furniture 65+ rambling Katie Anderson 69 {'name': 'ErgoLift Comfort Seat', 'description... {'rating': 4, 'customer_mood': 'happy', 'revie... 1 Clothing Women's Clothing 18-25 rambling Michael Hernandez 26 {'name': 'Silk Spindle Ribbon Dress', 'descrip... {'rating': 5, 'customer_mood': 'happy', 'revie... In\u00a0[11]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2                               \u2502 8                               \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83c\udfb2 Sampler Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                      \u2503        data type \u2503               number unique values \u2503         sampler type \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_category                 \u2502           string \u2502                         2 (100.0%) \u2502             category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product_subcategory              \u2502           string \u2502                         2 (100.0%) \u2502          subcategory \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 target_age_range                 \u2502           string \u2502                         2 (100.0%) \u2502             category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 review_style                     \u2502           string \u2502                          1 (50.0%) \u2502             category \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                             \ud83d\uddc2\ufe0f LLM-Structured Columns                                              \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                       \u2503               \u2503                            \u2503     prompt tokens \u2503      completion tokens \u2503\n\u2503 column name           \u2503     data type \u2503       number unique values \u2503        per record \u2503             per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product               \u2502          dict \u2502                 2 (100.0%) \u2502     265.5 +/- 0.5 \u2502           84.0 +/- 8.5 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_review       \u2502          dict \u2502                 2 (100.0%) \u2502     338.0 +/- 6.0 \u2502         255.0 +/- 48.1 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                               \ud83e\udde9 Expression Columns                                               \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                       \u2503                data type \u2503                             number unique values \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 customer_name                     \u2502                   string \u2502                                       2 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_age                      \u2502                   string \u2502                                       2 (100.0%) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> In\u00a0[12]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-2\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-2\") <pre>[22:47:46] [INFO] \ud83c\udfa8 Creating Data Designer dataset\n</pre> <pre>[22:47:46] [INFO] \u2705 Validation passed\n</pre> <pre>[22:47:46] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:47:46] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:47:46] [INFO]   |-- \ud83d\udc40 Checking 'nvidia/nemotron-3-nano-30b-a3b' in provider named 'nvidia' for model alias 'nemotron-nano-v3'...\n</pre> <pre>[22:47:46] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:47:46] [INFO] \u23f3 Processing batch 1 of 1\n</pre> <pre>[22:47:46] [INFO] \ud83c\udfb2 Preparing samplers to generate 10 records across 5 columns\n</pre> <pre>[22:47:46] [INFO] \ud83e\udde9 Generating column `customer_name` from expression\n</pre> <pre>[22:47:46] [INFO] \ud83e\udde9 Generating column `customer_age` from expression\n</pre> <pre>[22:47:46] [INFO] llm-structured model configuration for generating column 'product'\n</pre> <pre>[22:47:46] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:47:46] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:47:46] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:47:46] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, extra_body={'chat_template_kwargs': {'enable_thinking': False}}, temperature=1.00, top_p=1.00, max_tokens=2048\n</pre> <pre>[22:47:46] [INFO] \ud83d\udc19 Processing llm-structured column 'product' with 4 concurrent workers\n</pre> <pre>[22:47:50] [INFO] llm-structured model configuration for generating column 'customer_review'\n</pre> <pre>[22:47:50] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:47:50] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:47:50] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:47:50] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, extra_body={'chat_template_kwargs': {'enable_thinking': False}}, temperature=1.00, top_p=1.00, max_tokens=2048\n</pre> <pre>[22:47:50] [INFO] \ud83d\udc19 Processing llm-structured column 'customer_review' with 4 concurrent workers\n</pre> <pre>[22:47:56] [INFO] \ud83d\ude48 Dropping columns: ['customer']\n</pre> <pre>[22:47:56] [INFO] \ud83d\udcca Model usage summary:\n{\n    \"nvidia/nemotron-3-nano-30b-a3b\": {\n        \"token_usage\": {\n            \"input_tokens\": 7035,\n            \"output_tokens\": 4343,\n            \"total_tokens\": 11378\n        },\n        \"request_usage\": {\n            \"successful_requests\": 21,\n            \"failed_requests\": 0,\n            \"total_requests\": 21\n        },\n        \"tokens_per_second\": 1133,\n        \"requests_per_minute\": 125\n    }\n}\n</pre> <pre>[22:47:56] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:47:56] [INFO]   |-- \ud83c\udfb2 column: 'product_category'\n</pre> <pre>[22:47:56] [INFO]   |-- \ud83c\udfb2 column: 'product_subcategory'\n</pre> <pre>[22:47:56] [INFO]   |-- \ud83c\udfb2 column: 'target_age_range'\n</pre> <pre>[22:47:56] [INFO]   |-- \ud83c\udfb2 column: 'review_style'\n</pre> <pre>[22:47:56] [INFO]   |-- \ud83e\udde9 column: 'customer_name'\n</pre> <pre>[22:47:56] [INFO]   |-- \ud83e\udde9 column: 'customer_age'\n</pre> <pre>[22:47:56] [INFO]   |-- \ud83d\uddc2\ufe0f column: 'product'\n</pre> <pre>[22:47:56] [INFO]   |-- \ud83d\uddc2\ufe0f column: 'customer_review'\n</pre> In\u00a0[13]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() Out[13]: product_category product_subcategory target_age_range review_style customer_name customer_age product customer_review 0 Books Fiction 18-25 rambling Heather Miller 105 {'description': 'A beautifully designed sketch... {'customer_mood': 'excited', 'rating': 5, 'rev... 1 Electronics Smartphones 25-35 detailed Emily Whitney 63 {'description': 'A sleek, feature\u2011rich smartwa... {'customer_mood': 'excited', 'rating': 5, 'rev... 2 Electronics Laptops 35-50 rambling Victoria Cooper 57 {'description': 'A compact and lightweight USB... {'customer_mood': 'excited', 'rating': 5, 'rev... 3 Books Textbooks 65+ brief Kevin Carroll 28 {'description': 'A comprehensive, large-print ... {'customer_mood': 'happy', 'rating': 4, 'revie... 4 Clothing Accessories 50-65 rambling Crystal Soto 86 {'description': 'A classy collection of three ... {'customer_mood': 'happy', 'rating': 5, 'revie... In\u00a0[14]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 10                              \u2502 8                               \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83c\udfb2 Sampler Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                      \u2503        data type \u2503               number unique values \u2503         sampler type \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_category                 \u2502           string \u2502                          5 (50.0%) \u2502             category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product_subcategory              \u2502           string \u2502                          9 (90.0%) \u2502          subcategory \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 target_age_range                 \u2502           string \u2502                          5 (50.0%) \u2502             category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 review_style                     \u2502           string \u2502                          4 (40.0%) \u2502             category \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                             \ud83d\uddc2\ufe0f LLM-Structured Columns                                              \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                      \u2503               \u2503                            \u2503       prompt tokens \u2503     completion tokens \u2503\n\u2503 column name          \u2503     data type \u2503       number unique values \u2503          per record \u2503            per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product              \u2502          dict \u2502                10 (100.0%) \u2502       265.0 +/- 0.8 \u2502         97.0 +/- 27.7 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_review      \u2502          dict \u2502                10 (100.0%) \u2502      351.5 +/- 25.8 \u2502       219.5 +/- 234.9 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                               \ud83e\udde9 Expression Columns                                               \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                       \u2503                data type \u2503                             number unique values \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 customer_name                     \u2502                   string \u2502                                      10 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_age                      \u2502                   string \u2502                                      10 (100.0%) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#data-designer-tutorial-structured-outputs-and-jinja-expressions","title":"\ud83c\udfa8 Data Designer Tutorial: Structured Outputs and Jinja Expressions\u00b6","text":""},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>In this notebook, we will continue our exploration of Data Designer, demonstrating more advanced data generation using structured outputs and Jinja expressions.</p> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#import-the-essentials","title":"\ud83d\udce6 Import the essentials\u00b6","text":"<ul> <li>The <code>essentials</code> module provides quick access to the most commonly used objects.</li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object that is used to interface with the library.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#designing-our-data","title":"\ud83e\uddd1\u200d\ud83c\udfa8 Designing our data\u00b6","text":"<ul> <li><p>We will again create a product review dataset, but this time we will use structured outputs and Jinja expressions.</p> </li> <li><p>Structured outputs let you specify the exact schema of the data you want to generate.</p> </li> <li><p>Data Designer supports schemas specified using either json schema or Pydantic data models (recommended).</p> </li> </ul> <p>We'll define our structured outputs using Pydantic data models</p> <p>\ud83d\udca1 Why Pydantic?</p> <ul> <li><p>Pydantic models provide better IDE support and type validation.</p> </li> <li><p>They are more Pythonic than raw JSON schemas.</p> </li> <li><p>They integrate seamlessly with Data Designer's structured output system.</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Check out the following notebook to learn more about:</p> <ul> <li><p>Seeding synthetic data generation with an external dataset</p> </li> <li><p>Providing images as contextA</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/","title":"Seeding with an External Dataset","text":"In\u00a0[1]: Copied! <pre>from data_designer.essentials import (\n    ChatCompletionInferenceParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    LocalFileSeedSource,\n    ModelConfig,\n)\n</pre> from data_designer.essentials import (     ChatCompletionInferenceParams,     DataDesigner,     DataDesignerConfigBuilder,     LocalFileSeedSource,     ModelConfig, ) In\u00a0[2]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[3]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[4]: Copied! <pre>config_builder = DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[5]: Copied! <pre># Download sample dataset from Github\nimport urllib.request\n\nurl = \"https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/refs/heads/main/nemo/NeMo-Data-Designer/data/gretelai_symptom_to_diagnosis.csv\"\nlocal_filename, _ = urllib.request.urlretrieve(url, \"gretelai_symptom_to_diagnosis.csv\")\n\n# Seed datasets are passed as reference objects to the config builder.\nseed_source = LocalFileSeedSource(path=local_filename)\n\nconfig_builder.with_seed_dataset(seed_source)\n</pre> # Download sample dataset from Github import urllib.request  url = \"https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/refs/heads/main/nemo/NeMo-Data-Designer/data/gretelai_symptom_to_diagnosis.csv\" local_filename, _ = urllib.request.urlretrieve(url, \"gretelai_symptom_to_diagnosis.csv\")  # Seed datasets are passed as reference objects to the config builder. seed_source = LocalFileSeedSource(path=local_filename)  config_builder.with_seed_dataset(seed_source) Out[5]: <pre>DataDesignerConfigBuilder()\n</pre> In\u00a0[6]: Copied! <pre>config_builder.add_column(\n    name=\"patient_sampler\",\n    column_type=\"sampler\",\n    sampler_type=\"person_from_faker\",\n)\n\nconfig_builder.add_column(\n    name=\"doctor_sampler\",\n    column_type=\"sampler\",\n    sampler_type=\"person_from_faker\",\n)\n\nconfig_builder.add_column(\n    name=\"patient_id\",\n    column_type=\"sampler\",\n    sampler_type=\"uuid\",\n    params={\n        \"prefix\": \"PT-\",\n        \"short_form\": True,\n        \"uppercase\": True,\n    },\n)\n\nconfig_builder.add_column(\n    name=\"first_name\",\n    column_type=\"expression\",\n    expr=\"{{ patient_sampler.first_name}}\",\n)\n\nconfig_builder.add_column(\n    name=\"last_name\",\n    column_type=\"expression\",\n    expr=\"{{ patient_sampler.last_name }}\",\n)\n\n\nconfig_builder.add_column(\n    name=\"dob\",\n    column_type=\"expression\",\n    expr=\"{{ patient_sampler.birth_date }}\",\n)\n\nconfig_builder.add_column(\n    name=\"symptom_onset_date\",\n    column_type=\"sampler\",\n    sampler_type=\"datetime\",\n    params={\"start\": \"2024-01-01\", \"end\": \"2024-12-31\"},\n)\n\nconfig_builder.add_column(\n    name=\"date_of_visit\",\n    column_type=\"sampler\",\n    sampler_type=\"timedelta\",\n    params={\"dt_min\": 1, \"dt_max\": 30, \"reference_column_name\": \"symptom_onset_date\"},\n)\n\nconfig_builder.add_column(\n    name=\"physician\",\n    column_type=\"expression\",\n    expr=\"Dr. {{ doctor_sampler.last_name }}\",\n)\n\nconfig_builder.add_column(\n    name=\"physician_notes\",\n    column_type=\"llm-text\",\n    prompt=\"\"\"\\\nYou are a primary-care physician who just had an appointment with {{ first_name }} {{ last_name }},\nwho has been struggling with symptoms from {{ diagnosis }} since {{ symptom_onset_date }}.\nThe date of today's visit is {{ date_of_visit }}.\n\n{{ patient_summary }}\n\nWrite careful notes about your visit with {{ first_name }},\nas Dr. {{ doctor_sampler.first_name }} {{ doctor_sampler.last_name }}.\n\nFormat the notes as a busy doctor might.\nRespond with only the notes, no other text.\n\"\"\",\n    model_alias=MODEL_ALIAS,\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     name=\"patient_sampler\",     column_type=\"sampler\",     sampler_type=\"person_from_faker\", )  config_builder.add_column(     name=\"doctor_sampler\",     column_type=\"sampler\",     sampler_type=\"person_from_faker\", )  config_builder.add_column(     name=\"patient_id\",     column_type=\"sampler\",     sampler_type=\"uuid\",     params={         \"prefix\": \"PT-\",         \"short_form\": True,         \"uppercase\": True,     }, )  config_builder.add_column(     name=\"first_name\",     column_type=\"expression\",     expr=\"{{ patient_sampler.first_name}}\", )  config_builder.add_column(     name=\"last_name\",     column_type=\"expression\",     expr=\"{{ patient_sampler.last_name }}\", )   config_builder.add_column(     name=\"dob\",     column_type=\"expression\",     expr=\"{{ patient_sampler.birth_date }}\", )  config_builder.add_column(     name=\"symptom_onset_date\",     column_type=\"sampler\",     sampler_type=\"datetime\",     params={\"start\": \"2024-01-01\", \"end\": \"2024-12-31\"}, )  config_builder.add_column(     name=\"date_of_visit\",     column_type=\"sampler\",     sampler_type=\"timedelta\",     params={\"dt_min\": 1, \"dt_max\": 30, \"reference_column_name\": \"symptom_onset_date\"}, )  config_builder.add_column(     name=\"physician\",     column_type=\"expression\",     expr=\"Dr. {{ doctor_sampler.last_name }}\", )  config_builder.add_column(     name=\"physician_notes\",     column_type=\"llm-text\",     prompt=\"\"\"\\ You are a primary-care physician who just had an appointment with {{ first_name }} {{ last_name }}, who has been struggling with symptoms from {{ diagnosis }} since {{ symptom_onset_date }}. The date of today's visit is {{ date_of_visit }}.  {{ patient_summary }}  Write careful notes about your visit with {{ first_name }}, as Dr. {{ doctor_sampler.first_name }} {{ doctor_sampler.last_name }}.  Format the notes as a busy doctor might. Respond with only the notes, no other text. \"\"\",     model_alias=MODEL_ALIAS, )  data_designer.validate(config_builder) <pre>[22:48:03] [INFO] \u2705 Validation passed\n</pre> In\u00a0[7]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) <pre>[22:48:03] [INFO] \ud83d\udc41\ufe0f Preview generation in progress\n</pre> <pre>[22:48:03] [INFO] \u2705 Validation passed\n</pre> <pre>[22:48:03] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:48:03] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:48:03] [INFO]   |-- \ud83d\udc40 Checking 'nvidia/nemotron-3-nano-30b-a3b' in provider named 'nvidia' for model alias 'nemotron-nano-v3'...\n</pre> <pre>[22:48:03] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:48:04] [INFO] \ud83c\udf31 Sampling 2 records from seed dataset\n</pre> <pre>[22:48:04] [INFO]   |-- seed dataset size: 820 records\n</pre> <pre>[22:48:04] [INFO]   |-- sampling strategy: ordered\n</pre> <pre>[22:48:04] [INFO] \ud83c\udfb2 Preparing samplers to generate 2 records across 5 columns\n</pre> <pre>[22:48:04] [INFO] (\ud83d\udcbe + \ud83d\udcbe) Concatenating 2 datasets\n</pre> <pre>[22:48:04] [INFO] \ud83e\udde9 Generating column `first_name` from expression\n</pre> <pre>[22:48:04] [INFO] \ud83e\udde9 Generating column `last_name` from expression\n</pre> <pre>[22:48:04] [INFO] \ud83e\udde9 Generating column `dob` from expression\n</pre> <pre>[22:48:04] [INFO] \ud83e\udde9 Generating column `physician` from expression\n</pre> <pre>[22:48:04] [INFO] llm-text model configuration for generating column 'physician_notes'\n</pre> <pre>[22:48:04] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:48:04] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:48:04] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:48:04] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, extra_body={'chat_template_kwargs': {'enable_thinking': False}}, temperature=1.00, top_p=1.00, max_tokens=2048\n</pre> <pre>[22:48:04] [INFO] \ud83d\udc19 Processing llm-text column 'physician_notes' with 4 concurrent workers\n</pre> <pre>[22:48:09] [INFO] \ud83d\udcca Model usage summary:\n{\n    \"nvidia/nemotron-3-nano-30b-a3b\": {\n        \"token_usage\": {\n            \"input_tokens\": 291,\n            \"output_tokens\": 1978,\n            \"total_tokens\": 2269\n        },\n        \"request_usage\": {\n            \"successful_requests\": 2,\n            \"failed_requests\": 0,\n            \"total_requests\": 2\n        },\n        \"tokens_per_second\": 396,\n        \"requests_per_minute\": 20\n    }\n}\n</pre> <pre>[22:48:09] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:48:09] [INFO]   |-- \ud83c\udfb2 column: 'patient_sampler'\n</pre> <pre>[22:48:09] [INFO]   |-- \ud83c\udfb2 column: 'doctor_sampler'\n</pre> <pre>[22:48:09] [INFO]   |-- \ud83c\udfb2 column: 'patient_id'\n</pre> <pre>[22:48:09] [INFO]   |-- \ud83e\udde9 column: 'first_name'\n</pre> <pre>[22:48:09] [INFO]   |-- \ud83e\udde9 column: 'last_name'\n</pre> <pre>[22:48:09] [INFO]   |-- \ud83e\udde9 column: 'dob'\n</pre> <pre>[22:48:09] [INFO]   |-- \ud83c\udfb2 column: 'symptom_onset_date'\n</pre> <pre>[22:48:09] [INFO]   |-- \ud83c\udfb2 column: 'date_of_visit'\n</pre> <pre>[22:48:09] [INFO]   |-- \ud83e\udde9 column: 'physician'\n</pre> <pre>[22:48:09] [INFO]   |-- \ud83d\udcdd column: 'physician_notes'\n</pre> <pre>[22:48:09] [INFO] \u2705 Preview complete!\n</pre> In\u00a0[8]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() <pre>                                                                                                                   \n                                                 Generated Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name               \u2503 Value                                                                                      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 patient_sampler    \u2502 {                                                                                          \u2502\n\u2502                    \u2502     'uuid': '794ad9fa-f7cc-4216-991c-4b6ac2137580',                                        \u2502\n\u2502                    \u2502     'locale': 'en_US',                                                                     \u2502\n\u2502                    \u2502     'first_name': 'Shawn',                                                                 \u2502\n\u2502                    \u2502     'last_name': 'Freeman',                                                                \u2502\n\u2502                    \u2502     'middle_name': None,                                                                   \u2502\n\u2502                    \u2502     'sex': 'Male',                                                                         \u2502\n\u2502                    \u2502     'street_number': '446',                                                                \u2502\n\u2502                    \u2502     'street_name': 'Nelson Dam',                                                           \u2502\n\u2502                    \u2502     'city': 'Huntermouth',                                                                 \u2502\n\u2502                    \u2502     'state': 'Pennsylvania',                                                               \u2502\n\u2502                    \u2502     'postcode': '33009',                                                                   \u2502\n\u2502                    \u2502     'age': 28,                                                                             \u2502\n\u2502                    \u2502     'birth_date': '1997-03-04',                                                            \u2502\n\u2502                    \u2502     'country': 'Liberia',                                                                  \u2502\n\u2502                    \u2502     'marital_status': 'widowed',                                                           \u2502\n\u2502                    \u2502     'education_level': 'bachelors',                                                        \u2502\n\u2502                    \u2502     'unit': '',                                                                            \u2502\n\u2502                    \u2502     'occupation': 'Physiological scientist',                                               \u2502\n\u2502                    \u2502     'phone_number': '333-835-9161',                                                        \u2502\n\u2502                    \u2502     'bachelors_field': 'business'                                                          \u2502\n\u2502                    \u2502 }                                                                                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 doctor_sampler     \u2502 {                                                                                          \u2502\n\u2502                    \u2502     'uuid': 'c3cea901-69ce-4284-be73-181c9e68519e',                                        \u2502\n\u2502                    \u2502     'locale': 'en_US',                                                                     \u2502\n\u2502                    \u2502     'first_name': 'Jennifer',                                                              \u2502\n\u2502                    \u2502     'last_name': 'Maldonado',                                                              \u2502\n\u2502                    \u2502     'middle_name': None,                                                                   \u2502\n\u2502                    \u2502     'sex': 'Female',                                                                       \u2502\n\u2502                    \u2502     'street_number': '30974',                                                              \u2502\n\u2502                    \u2502     'street_name': 'Grace Spurs',                                                          \u2502\n\u2502                    \u2502     'city': 'Charlesfort',                                                                 \u2502\n\u2502                    \u2502     'state': 'Ohio',                                                                       \u2502\n\u2502                    \u2502     'postcode': '63154',                                                                   \u2502\n\u2502                    \u2502     'age': 20,                                                                             \u2502\n\u2502                    \u2502     'birth_date': '2005-06-29',                                                            \u2502\n\u2502                    \u2502     'country': 'Croatia',                                                                  \u2502\n\u2502                    \u2502     'marital_status': 'married_present',                                                   \u2502\n\u2502                    \u2502     'education_level': 'doctorate',                                                        \u2502\n\u2502                    \u2502     'unit': '',                                                                            \u2502\n\u2502                    \u2502     'occupation': 'Commissioning editor',                                                  \u2502\n\u2502                    \u2502     'phone_number': '549.786.2210x613',                                                    \u2502\n\u2502                    \u2502     'bachelors_field': 'arts_humanities'                                                   \u2502\n\u2502                    \u2502 }                                                                                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 patient_id         \u2502 PT-545EC043                                                                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 symptom_onset_date \u2502 2024-08-26                                                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 date_of_visit      \u2502 2024-09-13                                                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 first_name         \u2502 Shawn                                                                                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 last_name          \u2502 Freeman                                                                                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dob                \u2502 1997-03-04                                                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 physician          \u2502 Dr. Maldonado                                                                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 physician_notes    \u2502 **DME Session Note**                                                                       \u2502\n\u2502                    \u2502 **Patient:** Shawn Freeman                                                                 \u2502\n\u2502                    \u2502 **DOB:** [MM/DD/YYYY] | **Date:** 2024\u201109\u201113 | **Provider:** Dr. Jennifer Maldonado, MD    \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 ---                                                                                        \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **Chief Complaint:**                                                                       \u2502\n\u2502                    \u2502 - Neck and back pain                                                                       \u2502\n\u2502                    \u2502 - New balance &amp; coordination difficulty                                                    \u2502\n\u2502                    \u2502 - Persistent cough                                                                         \u2502\n\u2502                    \u2502 - Generalized limb weakness                                                                \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **HPI (concise):**                                                                         \u2502\n\u2502                    \u2502 - Onset: Symptoms began ~2\u202fweeks after initial diagnosis of cervical spondylosis           \u2502\n\u2502                    \u2502 (8/26/24).                                                                                 \u2502\n\u2502                    \u2502 - Pain: Radiates from cervical spine to shoulders/upper back, 6\u20117/10, worsened by posture  \u2502\n\u2502                    \u2502 and movement.                                                                              \u2502\n\u2502                    \u2502 - Neurologic: Occasional numbness in right arm; recent episodes of \u201cdizzy\u201d feeling when    \u2502\n\u2502                    \u2502 standing, mild gait instability.                                                           \u2502\n\u2502                    \u2502 - Weakness: Subjective weakness in proximal upper extremities (difficulty lifting arms);   \u2502\n\u2502                    \u2502 mild distal weakness noted.                                                                \u2502\n\u2502                    \u2502 - Cough: Dry, non\u2011productive, intermittent, no sputum; no hemoptysis, no fever, no chest   \u2502\n\u2502                    \u2502 pain.                                                                                      \u2502\n\u2502                    \u2502 - No recent trauma, no chest imaging changes (patient not previously imaged).              \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **ROS (pertinent positives):**                                                             \u2502\n\u2502                    \u2502 - **Neurologic:** Balance impairment, occasional hand clumsiness.                          \u2502\n\u2502                    \u2502 - **Musculoskeletal:** Neck stiffness, limited ROM.                                        \u2502\n\u2502                    \u2502 - **Respiratory:** Chronic cough (ongoing, unchanged).                                     \u2502\n\u2502                    \u2502 - **General:** Fatigue, mild weight loss (\u22485\u202flb).                                          \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **Current Medications:**                                                                   \u2502\n\u2502                    \u2502 - Celecoxib 200\u202fmg PO BID                                                                  \u2502\n\u2502                    \u2502 - Amlodipine 5\u202fmg PO daily                                                                 \u2502\n\u2502                    \u2502 - Vitamin D3 2000\u202fIU PO daily                                                              \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **Allergies:** NKDA                                                                        \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **Vital Signs (room air):**                                                                \u2502\n\u2502                    \u2502 - **BP:** 128/78\u202fmmHg                                                                      \u2502\n\u2502                    \u2502 - **HR:** 78\u202fbpm                                                                           \u2502\n\u2502                    \u2502 - **RR:** 16\u202f/min                                                                          \u2502\n\u2502                    \u2502 - **Temp:** 98.4\u202f\u00b0F (37\u202f\u00b0C)                                                                \u2502\n\u2502                    \u2502 - **SpO\u2082:** 97%                                                                            \u2502\n\u2502                    \u2502 - **Weight:** 184\u202flb                                                                       \u2502\n\u2502                    \u2502 - **BMI:** 28.9\u202fkg/m\u00b2                                                                      \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **Physical Exam (pertinent findings):**                                                    \u2502\n\u2502                    \u2502 - **HEENT:** Normal cervical lymphatics, no mucosal lesions.                               \u2502\n\u2502                    \u2502 - **Neck:** Limited flexion/extension, mild tenderness at C3\u2011C5, no crepitus.              \u2502\n\u2502                    \u2502 - **Neurologic:**                                                                          \u2502\n\u2502                    \u2502   - Cranial nerves II\u2011XII grossly intact.                                                  \u2502\n\u2502                    \u2502   - Motor: 4/5 strength in proximal upper extremities bilaterally; 5/5 in distal           \u2502\n\u2502                    \u2502 extremities.                                                                               \u2502\n\u2502                    \u2502   - Sensation: Mild decreased pinprick in right C6 dermatome.                              \u2502\n\u2502                    \u2502   - Reflexes: 2+ throughout, no hyperreflexia.                                             \u2502\n\u2502                    \u2502   - Coordination: Impaired finger\u2011nose testing on right side.                              \u2502\n\u2502                    \u2502   - Gait: Slight unsteady, wide base, but able to ambulate independently.                  \u2502\n\u2502                    \u2502 - **Musculoskeletal:** No joint swelling; range of motion limited by pain.                 \u2502\n\u2502                    \u2502 - **Skin:** No rashes.                                                                     \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **Assessment / Plan:**                                                                     \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 1. **Cervical Spondylosis with evolving radiculopathy**                                    \u2502\n\u2502                    \u2502    - Continue current NSAID (celecoxib) *PRN*; consider adding short course of oral        \u2502\n\u2502                    \u2502 steroids (prednisone 40\u202fmg daily 5\u202fdays then taper) if radicular pain escalates.           \u2502\n\u2502                    \u2502    - Physical therapy: focus on cervical stabilization, posture training, and gentle       \u2502\n\u2502                    \u2502 stretching.                                                                                \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 2. **COPD management**                                                                     \u2502\n\u2502                    \u2502    - Continue daily albuterol inhaler as needed; evaluate need for inhaled                 \u2502\n\u2502                    \u2502 corticosteroid/long\u2011acting bronchodilator at next visit.                                   \u2502\n\u2502                    \u2502    - Reinforce smoking cessation (non\u2011smoker but exposed to second\u2011hand smoke).            \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 3. **Balance &amp; coordination**                                                              \u2502\n\u2502                    \u2502    - Order MRI cervical spine with contrast to assess canal narrowing/compression.         \u2502\n\u2502                    \u2502    - Consider referral to Neurology for further work\u2011up (possible cervical myelopathy).    \u2502\n\u2502                    \u2502    - Initiate home safety assessment; advise use of assistive devices if gait worsens.     \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 4. **Weakness &amp; cough**                                                                    \u2502\n\u2502                    \u2502    - CBC, CMP, and Vitamin B12 levels to rule out systemic contributors.                   \u2502\n\u2502                    \u2502    - Spirometry to objectively assess lung function.                                       \u2502\n\u2502                    \u2502    - Follow\u2011up cough evaluation if new sputum production, hemoptysis, or fever develop.    \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 5. **Follow\u2011up**                                                                           \u2502\n\u2502                    \u2502    - Return in 2 weeks for PT reassessment and review of imaging results.                  \u2502\n\u2502                    \u2502    - Earlier return if new neurologic deficits, worsening cough, or respiratory distress.  \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **Patient Education Points:**                                                              \u2502\n\u2502                    \u2502 - Maintain good posture; ergonomic workstation adjustments.                                \u2502\n\u2502                    \u2502 - Perform prescribed neck/shoulder strengthening exercises 3\u00d7/week.                        \u2502\n\u2502                    \u2502 - Use a humidifier at night to soothe cough; stay hydrated.                                \u2502\n\u2502                    \u2502 - Monitor for any new numbness, tingling, or falls; seek immediate care if symptoms        \u2502\n\u2502                    \u2502 acutely worsen.                                                                            \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **Signature:**                                                                             \u2502\n\u2502                    \u2502 Dr. Jennifer Maldonado, MD                                                                 \u2502\n\u2502                    \u2502 Primary Care \u2013 Internal Medicine                                                           \u2502\n\u2502                    \u2502 [Contact Information]                                                                      \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 ---                                                                                        \u2502\n\u2502                    \u2502 *End of note.*                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                    [index: 0]                                                     \n</pre> In\u00a0[9]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset Out[9]: diagnosis patient_summary patient_sampler doctor_sampler patient_id symptom_onset_date date_of_visit first_name last_name dob physician physician_notes 0 cervical spondylosis I've been having a lot of pain in my neck and ... {'uuid': '794ad9fa-f7cc-4216-991c-4b6ac2137580... {'uuid': 'c3cea901-69ce-4284-be73-181c9e68519e... PT-545EC043 2024-08-26 2024-09-13 Shawn Freeman 1997-03-04 Dr. Maldonado **DME Session Note**  \\n**Patient:** Shawn Fre... 1 impetigo I have a rash on my face that is getting worse... {'uuid': '955e6d43-4a00-4090-b465-43ae27e00de0... {'uuid': '36bcaf93-76e0-4c50-8680-2bbb1fc4c254... PT-F1A50011 2024-01-11 2024-01-24 Sharon Hughes 1927-06-02 Dr. Kelly **Progress Notes - Primary Care**  \\n**Dr. R. ... In\u00a0[10]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2                               \u2502 10                              \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83c\udfb2 Sampler Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                   \u2503       data type \u2503             number unique values \u2503               sampler type \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 patient_sampler               \u2502            dict \u2502                       2 (100.0%) \u2502          person_from_faker \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 doctor_sampler                \u2502            dict \u2502                       2 (100.0%) \u2502          person_from_faker \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 patient_id                    \u2502          string \u2502                       2 (100.0%) \u2502                       uuid \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 symptom_onset_date            \u2502          string \u2502                       2 (100.0%) \u2502                   datetime \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 date_of_visit                 \u2502          string \u2502                       2 (100.0%) \u2502                  timedelta \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83d\udcdd LLM-Text Columns                                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                       \u2503               \u2503                            \u2503     prompt tokens \u2503      completion tokens \u2503\n\u2503 column name           \u2503     data type \u2503       number unique values \u2503        per record \u2503             per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 physician_notes       \u2502        string \u2502                 2 (100.0%) \u2502     123.5 +/- 5.5 \u2502        917.0 +/- 264.5 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                               \ud83e\udde9 Expression Columns                                               \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                    \u2503                 data type \u2503                               number unique values \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 first_name                     \u2502                    string \u2502                                         2 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 last_name                      \u2502                    string \u2502                                         2 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dob                            \u2502                    string \u2502                                         2 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 physician                      \u2502                    string \u2502                                         2 (100.0%) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> In\u00a0[11]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-3\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-3\") <pre>[22:48:09] [INFO] \ud83c\udfa8 Creating Data Designer dataset\n</pre> <pre>[22:48:09] [INFO] \u2705 Validation passed\n</pre> <pre>[22:48:09] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:48:09] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:48:09] [INFO]   |-- \ud83d\udc40 Checking 'nvidia/nemotron-3-nano-30b-a3b' in provider named 'nvidia' for model alias 'nemotron-nano-v3'...\n</pre> <pre>[22:48:10] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:48:10] [INFO] \u23f3 Processing batch 1 of 1\n</pre> <pre>[22:48:10] [INFO] \ud83c\udf31 Sampling 10 records from seed dataset\n</pre> <pre>[22:48:10] [INFO]   |-- seed dataset size: 820 records\n</pre> <pre>[22:48:10] [INFO]   |-- sampling strategy: ordered\n</pre> <pre>[22:48:10] [INFO] \ud83c\udfb2 Preparing samplers to generate 10 records across 5 columns\n</pre> <pre>[22:48:10] [INFO] (\ud83d\udcbe + \ud83d\udcbe) Concatenating 2 datasets\n</pre> <pre>[22:48:10] [INFO] \ud83e\udde9 Generating column `first_name` from expression\n</pre> <pre>[22:48:10] [INFO] \ud83e\udde9 Generating column `last_name` from expression\n</pre> <pre>[22:48:10] [INFO] \ud83e\udde9 Generating column `dob` from expression\n</pre> <pre>[22:48:10] [INFO] \ud83e\udde9 Generating column `physician` from expression\n</pre> <pre>[22:48:10] [INFO] llm-text model configuration for generating column 'physician_notes'\n</pre> <pre>[22:48:10] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:48:10] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:48:10] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:48:10] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, extra_body={'chat_template_kwargs': {'enable_thinking': False}}, temperature=1.00, top_p=1.00, max_tokens=2048\n</pre> <pre>[22:48:10] [INFO] \ud83d\udc19 Processing llm-text column 'physician_notes' with 4 concurrent workers\n</pre> <pre>[22:48:22] [INFO] \ud83d\udcca Model usage summary:\n{\n    \"nvidia/nemotron-3-nano-30b-a3b\": {\n        \"token_usage\": {\n            \"input_tokens\": 1433,\n            \"output_tokens\": 10241,\n            \"total_tokens\": 11674\n        },\n        \"request_usage\": {\n            \"successful_requests\": 10,\n            \"failed_requests\": 0,\n            \"total_requests\": 10\n        },\n        \"tokens_per_second\": 989,\n        \"requests_per_minute\": 50\n    }\n}\n</pre> <pre>[22:48:22] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:48:22] [INFO]   |-- \ud83c\udfb2 column: 'patient_sampler'\n</pre> <pre>[22:48:22] [INFO]   |-- \ud83c\udfb2 column: 'doctor_sampler'\n</pre> <pre>[22:48:22] [INFO]   |-- \ud83c\udfb2 column: 'patient_id'\n</pre> <pre>[22:48:22] [INFO]   |-- \ud83e\udde9 column: 'first_name'\n</pre> <pre>[22:48:22] [INFO]   |-- \ud83e\udde9 column: 'last_name'\n</pre> <pre>[22:48:22] [INFO]   |-- \ud83e\udde9 column: 'dob'\n</pre> <pre>[22:48:22] [INFO]   |-- \ud83c\udfb2 column: 'symptom_onset_date'\n</pre> <pre>[22:48:22] [INFO]   |-- \ud83c\udfb2 column: 'date_of_visit'\n</pre> <pre>[22:48:22] [INFO]   |-- \ud83e\udde9 column: 'physician'\n</pre> <pre>[22:48:22] [INFO]   |-- \ud83d\udcdd column: 'physician_notes'\n</pre> In\u00a0[12]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() Out[12]: diagnosis patient_summary patient_sampler doctor_sampler patient_id symptom_onset_date date_of_visit first_name last_name dob physician physician_notes 0 cervical spondylosis I've been having a lot of pain in my neck and ... {'age': 94, 'bachelors_field': 'arts_humanitie... {'age': 34, 'bachelors_field': 'no_degree', 'b... PT-9F27EF74 2024-03-12 2024-03-13 Dennis Hunt 1931-08-13 Dr. Saunders **Dr. Rachael Saunders \u2013 Neurology/Primary Car... 1 impetigo I have a rash on my face that is getting worse... {'age': 87, 'bachelors_field': 'business', 'bi... {'age': 94, 'bachelors_field': 'no_degree', 'b... PT-6B0479BB 2024-04-09 2024-05-08 James Warren 1938-07-04 Dr. Leon **Patient:** James Warren   **DOB:** 02/15/199... 2 urinary tract infection I have been urinating blood. I sometimes feel ... {'age': 35, 'bachelors_field': 'no_degree', 'b... {'age': 66, 'bachelors_field': 'no_degree', 'b... PT-3C211B4D 2024-04-24 2024-05-21 Jesus Holmes 1990-11-28 Dr. Perez **Patient:** Jesus Holmes   **DOB:** 1987-03-1... 3 arthritis I have been having trouble with my muscles and... {'age': 34, 'bachelors_field': 'no_degree', 'b... {'age': 64, 'bachelors_field': 'no_degree', 'b... PT-22549D2C 2024-08-28 2024-09-22 Taylor Hall 1991-02-27 Dr. Marsh **Patient:** Taylor Hall   **DOB:** 1995-06-12... 4 dengue I have been feeling really sick. My body hurts... {'age': 67, 'bachelors_field': 'no_degree', 'b... {'age': 62, 'bachelors_field': 'no_degree', 'b... PT-78CF3203 2024-03-19 2024-04-07 Dustin Barajas 1958-09-11 Dr. Jackson - 04/07/2024 - Dustin Barajas, 28M, presents s... In\u00a0[13]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 10                              \u2502 10                              \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83c\udfb2 Sampler Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                   \u2503       data type \u2503             number unique values \u2503               sampler type \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 patient_sampler               \u2502            dict \u2502                      10 (100.0%) \u2502          person_from_faker \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 doctor_sampler                \u2502            dict \u2502                      10 (100.0%) \u2502          person_from_faker \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 patient_id                    \u2502          string \u2502                      10 (100.0%) \u2502                       uuid \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 symptom_onset_date            \u2502          string \u2502                      10 (100.0%) \u2502                   datetime \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 date_of_visit                 \u2502          string \u2502                      10 (100.0%) \u2502                  timedelta \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83d\udcdd LLM-Text Columns                                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                       \u2503               \u2503                            \u2503     prompt tokens \u2503      completion tokens \u2503\n\u2503 column name           \u2503     data type \u2503       number unique values \u2503        per record \u2503             per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 physician_notes       \u2502        string \u2502                10 (100.0%) \u2502     117.5 +/- 5.5 \u2502        889.5 +/- 355.1 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                               \ud83e\udde9 Expression Columns                                               \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                    \u2503                 data type \u2503                               number unique values \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 first_name                     \u2502                    string \u2502                                        10 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 last_name                      \u2502                    string \u2502                                        10 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dob                            \u2502                    string \u2502                                        10 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 physician                      \u2502                    string \u2502                                        10 (100.0%) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre>"},{"location":"notebooks/3-seeding-with-a-dataset/#data-designer-tutorial-seeding-synthetic-data-generation-with-an-external-dataset","title":"\ud83c\udfa8 Data Designer Tutorial: Seeding Synthetic Data Generation with an External Dataset\u00b6","text":""},{"location":"notebooks/3-seeding-with-a-dataset/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>In this notebook, we will demonstrate how to seed synthetic data generation in Data Designer with an external dataset.</p> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"notebooks/3-seeding-with-a-dataset/#import-the-essentials","title":"\ud83d\udce6 Import the essentials\u00b6","text":"<ul> <li>The <code>essentials</code> module provides quick access to the most commonly used objects.</li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object is responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#prepare-a-seed-dataset","title":"\ud83c\udfe5 Prepare a seed dataset\u00b6","text":"<ul> <li><p>For this notebook, we'll create a synthetic dataset of patient notes.</p> </li> <li><p>We will seed the generation process with a symptom-to-diagnosis dataset.</p> </li> <li><p>We already have the dataset downloaded in the data directory of this repository.</p> </li> </ul> <p>\ud83c\udf31 Why use a seed dataset?</p> <ul> <li><p>Seed datasets let you steer the generation process by providing context that is specific to your use case.</p> </li> <li><p>Seed datasets are also an excellent way to inject real-world diversity into your synthetic data.</p> </li> <li><p>During generation, prompt templates can reference any of the seed dataset fields.</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#designing-our-synthetic-patient-notes-dataset","title":"\ud83c\udfa8 Designing our synthetic patient notes dataset\u00b6","text":"<ul> <li><p>Here we use <code>add_column</code> with keyword arguments (rather than imported config objects).</p> </li> <li><p>Generally, we recommend using concrete objects, but this is a convenient shorthand.</p> </li> <li><p>Note: The prompt template can reference fields from our seed dataset:</p> <ul> <li><code>{{ diagnosis }}</code> - the medical diagnosis from the seed data</li> <li><code>{{ patient_summary }}</code> - the symptom description from the seed data</li> </ul> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebooks/3-seeding-with-a-dataset/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Check out the following notebook to learn more about:</p> <ul> <li>Providing images as context</li> </ul>"},{"location":"notebooks/4-providing-images-as-context/","title":"Providing Images as Context","text":"In\u00a0[1]: Copied! <pre># Standard library imports\nimport base64\nimport io\nimport uuid\n\n# Third-party imports\nimport pandas as pd\nimport rich\nfrom datasets import load_dataset\nfrom IPython.display import display\nfrom rich.panel import Panel\n\n# Data Designer imports\nfrom data_designer.essentials import (\n    ChatCompletionInferenceParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    DataFrameSeedSource,\n    ImageContext,\n    ImageFormat,\n    LLMTextColumnConfig,\n    ModalityDataType,\n    ModelConfig,\n)\n</pre> # Standard library imports import base64 import io import uuid  # Third-party imports import pandas as pd import rich from datasets import load_dataset from IPython.display import display from rich.panel import Panel  # Data Designer imports from data_designer.essentials import (     ChatCompletionInferenceParams,     DataDesigner,     DataDesignerConfigBuilder,     DataFrameSeedSource,     ImageContext,     ImageFormat,     LLMTextColumnConfig,     ModalityDataType,     ModelConfig, ) In\u00a0[2]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[3]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\nmodel_configs = [\n    ModelConfig(\n        alias=\"vision\",\n        model=\"meta/llama-4-scout-17b-16e-instruct\",\n        provider=MODEL_PROVIDER,\n        inference_parameters=ChatCompletionInferenceParams(\n            temperature=0.60,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  model_configs = [     ModelConfig(         alias=\"vision\",         model=\"meta/llama-4-scout-17b-16e-instruct\",         provider=MODEL_PROVIDER,         inference_parameters=ChatCompletionInferenceParams(             temperature=0.60,             top_p=0.95,             max_tokens=2048,         ),     ), ] In\u00a0[4]: Copied! <pre>config_builder = DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[5]: Copied! <pre># Dataset processing configuration\nIMG_COUNT = 512  # Number of images to process\nBASE64_IMAGE_HEIGHT = 512  # Standardized height for model input\n\n# Load ColPali dataset for visual documents\nimg_dataset_cfg = {\"path\": \"vidore/colpali_train_set\", \"split\": \"train\", \"streaming\": True}\n</pre> # Dataset processing configuration IMG_COUNT = 512  # Number of images to process BASE64_IMAGE_HEIGHT = 512  # Standardized height for model input  # Load ColPali dataset for visual documents img_dataset_cfg = {\"path\": \"vidore/colpali_train_set\", \"split\": \"train\", \"streaming\": True} In\u00a0[6]: Copied! <pre>def resize_image(image, height: int):\n    \"\"\"\n    Resize image while maintaining aspect ratio.\n\n    Args:\n        image: PIL Image object\n        height: Target height in pixels\n\n    Returns:\n        Resized PIL Image object\n    \"\"\"\n    original_width, original_height = image.size\n    width = int(original_width * (height / original_height))\n    return image.resize((width, height))\n\n\ndef convert_image_to_chat_format(record, height: int) -&gt; dict:\n    \"\"\"\n    Convert PIL image to base64 format for chat template usage.\n\n    Args:\n        record: Dataset record containing image and metadata\n        height: Target height for image resizing\n\n    Returns:\n        Updated record with base64_image and uuid fields\n    \"\"\"\n    # Resize image for consistent processing\n    image = resize_image(record[\"image\"], height)\n\n    # Convert to base64 string\n    img_buffer = io.BytesIO()\n    image.save(img_buffer, format=\"PNG\")\n    byte_data = img_buffer.getvalue()\n    base64_encoded_data = base64.b64encode(byte_data)\n    base64_string = base64_encoded_data.decode(\"utf-8\")\n\n    # Return updated record\n    return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())}\n</pre> def resize_image(image, height: int):     \"\"\"     Resize image while maintaining aspect ratio.      Args:         image: PIL Image object         height: Target height in pixels      Returns:         Resized PIL Image object     \"\"\"     original_width, original_height = image.size     width = int(original_width * (height / original_height))     return image.resize((width, height))   def convert_image_to_chat_format(record, height: int) -&gt; dict:     \"\"\"     Convert PIL image to base64 format for chat template usage.      Args:         record: Dataset record containing image and metadata         height: Target height for image resizing      Returns:         Updated record with base64_image and uuid fields     \"\"\"     # Resize image for consistent processing     image = resize_image(record[\"image\"], height)      # Convert to base64 string     img_buffer = io.BytesIO()     image.save(img_buffer, format=\"PNG\")     byte_data = img_buffer.getvalue()     base64_encoded_data = base64.b64encode(byte_data)     base64_string = base64_encoded_data.decode(\"utf-8\")      # Return updated record     return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())} In\u00a0[7]: Copied! <pre># Load and process the visual document dataset\nprint(\"\ud83d\udce5 Loading and processing document images...\")\n\nimg_dataset_iter = iter(\n    load_dataset(**img_dataset_cfg).map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT})\n)\nimg_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])\n\nprint(f\"\u2705 Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\")\n</pre> # Load and process the visual document dataset print(\"\ud83d\udce5 Loading and processing document images...\")  img_dataset_iter = iter(     load_dataset(**img_dataset_cfg).map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT}) ) img_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])  print(f\"\u2705 Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\") <pre>\ud83d\udce5 Loading and processing document images...\n</pre> <pre>\u2705 Loaded 512 images with columns: ['image', 'image_filename', 'query', 'answer', 'source', 'options', 'page', 'model', 'prompt', 'answer_type', 'base64_image', 'uuid']\n</pre> In\u00a0[8]: Copied! <pre>img_dataset.head()\n</pre> img_dataset.head() Out[8]: image image_filename query answer source options page model prompt answer_type base64_image uuid 0 &lt;PIL.JpegImagePlugin.JpegImageFile image mode=... images/1810.07757_2.jpg Comparing panels a, b, c, and d, which stateme... D arxiv_qa ['A. The variance of the data decreases from p... gpt4V None iVBORw0KGgoAAAANSUhEUgAAAUAAAAIACAIAAAB8QiIMAA... 3fb63904-6755-4826-a8f9-aad20ccdc678 1 &lt;PIL.JpegImagePlugin.JpegImageFile image mode=... data/scrapped_pdfs_split/pages_extracted/energ... What is the duration of the course mentioned i... ['five to ten hours, not including field trips'] pdf None 9 sonnet \\n        You are an assistant specialized in ... None iVBORw0KGgoAAAANSUhEUgAAAYsAAAIACAIAAAD8HddaAA... e78f4cf3-a899-402b-9ce9-15b2fd1332a4 2 &lt;PIL.JpegImagePlugin.JpegImageFile image mode=... data/scrapped_pdfs_split/pages_extracted/energ... What is the primary purpose of the PTC in lith... ['protect against external short circuits'] pdf None 414 sonnet \\n        You are an assistant specialized in ... None iVBORw0KGgoAAAANSUhEUgAAAZgAAAIACAIAAAAwhO2xAA... b0d713d3-ba84-439b-9239-24e1fe541fdd 3 &lt;PIL.PngImagePlugin.PngImageFile image mode=L ... 0fd47b51ae9248ef36669b8619b1223f268edae3e7a44a... What is the date?\\nYour answer should be very ... OCTOBER 17, 1995. docvqa None None None None None iVBORw0KGgoAAAANSUhEUgAAAX0AAAIACAAAAABLRuMPAA... abe793fc-56e4-47f0-8bfa-6f1f7cf28cd8 4 &lt;PIL.PngImagePlugin.PngImageFile image mode=L ... b335cfb9d442f8925ea41a064cb445a5395577f2345d52... What is Bert Shulimson's title?\\nYour response... EXECUTIVE SECRETARY. docvqa None None None None None iVBORw0KGgoAAAANSUhEUgAAAY8AAAIACAAAAABf/7+rAA... 625e6644-8e00-480c-9ea0-6bfd6d76baf0 In\u00a0[9]: Copied! <pre># Add the seed dataset containing our processed images\ndf_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]]\nconfig_builder.with_seed_dataset(DataFrameSeedSource(df=df_seed))\n</pre> # Add the seed dataset containing our processed images df_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]] config_builder.with_seed_dataset(DataFrameSeedSource(df=df_seed)) Out[9]: <pre>DataDesignerConfigBuilder()\n</pre> In\u00a0[10]: Copied! <pre># Add a column to generate detailed document summaries\nconfig_builder.add_column(\n    LLMTextColumnConfig(\n        name=\"summary\",\n        model_alias=\"vision\",\n        prompt=(\n            \"Provide a detailed summary of the content in this image in Markdown format. \"\n            \"Start from the top of the image and then describe it from top to bottom. \"\n            \"Place a summary at the bottom.\"\n        ),\n        multi_modal_context=[\n            ImageContext(\n                column_name=\"base64_image\",\n                data_type=ModalityDataType.BASE64,\n                image_format=ImageFormat.PNG,\n            )\n        ],\n    )\n)\n</pre> # Add a column to generate detailed document summaries config_builder.add_column(     LLMTextColumnConfig(         name=\"summary\",         model_alias=\"vision\",         prompt=(             \"Provide a detailed summary of the content in this image in Markdown format. \"             \"Start from the top of the image and then describe it from top to bottom. \"             \"Place a summary at the bottom.\"         ),         multi_modal_context=[             ImageContext(                 column_name=\"base64_image\",                 data_type=ModalityDataType.BASE64,                 image_format=ImageFormat.PNG,             )         ],     ) ) Out[10]: <pre>DataDesignerConfigBuilder(\n    seed_dataset: df seed\n    llm_text_columns: ['summary']\n)\n</pre> In\u00a0[11]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) <pre>[22:49:00] [INFO] \ud83e\uddd0 Preview generation in progress\n</pre> <pre>[22:49:00] [INFO] \u2705 Validation passed\n</pre> <pre>[22:49:00] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:49:00] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:49:00] [INFO]   |-- \ud83d\udc40 Checking 'meta/llama-4-scout-17b-16e-instruct' in provider named 'nvidia' for model alias 'vision'...\n</pre> <pre>[22:49:02] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:49:02] [INFO] \ud83c\udf31 Sampling 2 records from seed dataset\n</pre> <pre>[22:49:02] [INFO]   |-- seed dataset size: 512 records\n</pre> <pre>[22:49:02] [INFO]   |-- sampling strategy: ordered\n</pre> <pre>[22:49:02] [INFO] llm-text model configuration for generating column 'summary'\n</pre> <pre>[22:49:02] [INFO]   |-- model: 'meta/llama-4-scout-17b-16e-instruct'\n</pre> <pre>[22:49:02] [INFO]   |-- model alias: 'vision'\n</pre> <pre>[22:49:02] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:49:02] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, temperature=0.60, top_p=0.95, max_tokens=2048\n</pre> <pre>[22:49:02] [INFO] \ud83d\udc19 Processing llm-text column 'summary' with 4 concurrent workers\n</pre> <pre>[22:49:09] [INFO] \ud83d\udcca Model usage summary:\n{\n    \"meta/llama-4-scout-17b-16e-instruct\": {\n        \"token_usage\": {\n            \"input_tokens\": 1396,\n            \"output_tokens\": 763,\n            \"total_tokens\": 2159\n        },\n        \"request_usage\": {\n            \"successful_requests\": 2,\n            \"failed_requests\": 0,\n            \"total_requests\": 2\n        },\n        \"tokens_per_second\": 280,\n        \"requests_per_minute\": 15\n    }\n}\n</pre> <pre>[22:49:09] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:49:09] [INFO]   |-- \ud83d\udcdd column: 'summary'\n</pre> <pre>[22:49:09] [INFO] \ud83d\ude4c Preview complete!\n</pre> In\u00a0[12]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() <pre>                                                                                                                   \n                                                 Generated Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name    \u2503 Value                                                                                                 \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 summary \u2502 ## Image Summary                                                                                      \u2502\n\u2502         \u2502                                                                                                       \u2502\n\u2502         \u2502 The image presents a collection of 8 heatmap graphs, arranged in two columns and four rows, labeled   \u2502\n\u2502         \u2502 from **a)** to **h)**. Each graph displays a relationship between two variables, with the x-axis      \u2502\n\u2502         \u2502 representing time **t** in microseconds (**\u03bcs**) and the y-axis representing a change in frequency    \u2502\n\u2502         \u2502 **\u0394f** in megahertz (**MHz**).                                                                        \u2502\n\u2502         \u2502                                                                                                       \u2502\n\u2502         \u2502 ### Color Bar                                                                                         \u2502\n\u2502         \u2502 A color bar is provided at the top, indicating a correlation coefficient **\u03c4**, ranging from 0.2      \u2502\n\u2502         \u2502 (blue) to 1 (yellow). This color bar applies to all graphs.                                           \u2502\n\u2502         \u2502                                                                                                       \u2502\n\u2502         \u2502 ### Graphs                                                                                            \u2502\n\u2502         \u2502                                                                                                       \u2502\n\u2502         \u2502 *   **a)** and **e)**: These graphs show a relatively stable, horizontal dark blue band, indicating a \u2502\n\u2502         \u2502 strong negative correlation (**\u03c4** \u2248 0.2-0.3) at a specific frequency offset.                         \u2502\n\u2502         \u2502 *   **b)**, **d)**, **f)**, and **h)**: These graphs exhibit a scattered distribution of colors, with \u2502\n\u2502         \u2502 a general trend of decreasing **\u03c4** over time. The frequency offsets vary across these graphs.        \u2502\n\u2502         \u2502 *   **c)** and **g)**: These graphs display a mix of colors, with some areas showing a moderate       \u2502\n\u2502         \u2502 positive correlation (**\u03c4** \u2248 0.5-0.7).                                                               \u2502\n\u2502         \u2502                                                                                                       \u2502\n\u2502         \u2502 ### Summary                                                                                           \u2502\n\u2502         \u2502                                                                                                       \u2502\n\u2502         \u2502 The image presents a set of heatmap graphs illustrating the relationship between time, frequency      \u2502\n\u2502         \u2502 offset, and correlation coefficient **\u03c4**. The graphs show varying patterns, including stable         \u2502\n\u2502         \u2502 correlations, scattered distributions, and moderate positive correlations. The color bar provides a   \u2502\n\u2502         \u2502 reference for interpreting the correlation coefficients. Overall, the image appears to be a           \u2502\n\u2502         \u2502 visualization of some type of signal processing or analysis data.                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                    [index: 0]                                                     \n</pre> In\u00a0[13]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset Out[13]: uuid image_filename base64_image page options source summary 0 3fb63904-6755-4826-a8f9-aad20ccdc678 images/1810.07757_2.jpg iVBORw0KGgoAAAANSUhEUgAAAUAAAAIACAIAAAB8QiIMAA... ['A. The variance of the data decreases from p... arxiv_qa ## Image Summary\\n\\nThe image presents a colle... 1 e78f4cf3-a899-402b-9ce9-15b2fd1332a4 data/scrapped_pdfs_split/pages_extracted/energ... iVBORw0KGgoAAAANSUhEUgAAAYsAAAIACAIAAAD8HddaAA... 9 None pdf ## Document Summary\\nThe document appears to b... In\u00a0[14]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2                               \u2502 1                               \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83d\udcdd LLM-Text Columns                                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                  \u2503               \u2503                              \u2503       prompt tokens \u2503       completion tokens \u2503\n\u2503 column name      \u2503     data type \u2503         number unique values \u2503          per record \u2503              per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 summary          \u2502        string \u2502                   2 (100.0%) \u2502        38.0 +/- 0.0 \u2502          389.5 +/- 74.2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> In\u00a0[15]: Copied! <pre># Compare original document with generated summary\nindex = 0  # Change this to view different examples\n\n# Merge preview data with original images for comparison\ncomparison_dataset = preview.dataset.merge(pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\")\n\n# Extract the record for display\nrecord = comparison_dataset.iloc[index]\n\nprint(\"\ud83d\udcc4 Original Document Image:\")\ndisplay(resize_image(record.image, BASE64_IMAGE_HEIGHT))\n\nprint(\"\\n\ud83d\udcdd Generated Summary:\")\nrich.print(Panel(record.summary, title=\"Document Summary\", title_align=\"left\"))\n</pre> # Compare original document with generated summary index = 0  # Change this to view different examples  # Merge preview data with original images for comparison comparison_dataset = preview.dataset.merge(pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\")  # Extract the record for display record = comparison_dataset.iloc[index]  print(\"\ud83d\udcc4 Original Document Image:\") display(resize_image(record.image, BASE64_IMAGE_HEIGHT))  print(\"\\n\ud83d\udcdd Generated Summary:\") rich.print(Panel(record.summary, title=\"Document Summary\", title_align=\"left\")) <pre>\ud83d\udcc4 Original Document Image:\n</pre> <pre>\n\ud83d\udcdd Generated Summary:\n</pre> <pre>\u256d\u2500 Document Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 ## Image Summary                                                                                                \u2502\n\u2502                                                                                                                 \u2502\n\u2502 The image presents a collection of 8 heatmap graphs, arranged in two columns and four rows, labeled from **a)** \u2502\n\u2502 to **h)**. Each graph displays a relationship between two variables, with the x-axis representing time **t** in \u2502\n\u2502 microseconds (**\u03bcs**) and the y-axis representing a change in frequency **\u0394f** in megahertz (**MHz**).          \u2502\n\u2502                                                                                                                 \u2502\n\u2502 ### Color Bar                                                                                                   \u2502\n\u2502 A color bar is provided at the top, indicating a correlation coefficient **\u03c4**, ranging from 0.2 (blue) to 1    \u2502\n\u2502 (yellow). This color bar applies to all graphs.                                                                 \u2502\n\u2502                                                                                                                 \u2502\n\u2502 ### Graphs                                                                                                      \u2502\n\u2502                                                                                                                 \u2502\n\u2502 *   **a)** and **e)**: These graphs show a relatively stable, horizontal dark blue band, indicating a strong    \u2502\n\u2502 negative correlation (**\u03c4** \u2248 0.2-0.3) at a specific frequency offset.                                          \u2502\n\u2502 *   **b)**, **d)**, **f)**, and **h)**: These graphs exhibit a scattered distribution of colors, with a general \u2502\n\u2502 trend of decreasing **\u03c4** over time. The frequency offsets vary across these graphs.                            \u2502\n\u2502 *   **c)** and **g)**: These graphs display a mix of colors, with some areas showing a moderate positive        \u2502\n\u2502 correlation (**\u03c4** \u2248 0.5-0.7).                                                                                  \u2502\n\u2502                                                                                                                 \u2502\n\u2502 ### Summary                                                                                                     \u2502\n\u2502                                                                                                                 \u2502\n\u2502 The image presents a set of heatmap graphs illustrating the relationship between time, frequency offset, and    \u2502\n\u2502 correlation coefficient **\u03c4**. The graphs show varying patterns, including stable correlations, scattered       \u2502\n\u2502 distributions, and moderate positive correlations. The color bar provides a reference for interpreting the      \u2502\n\u2502 correlation coefficients. Overall, the image appears to be a visualization of some type of signal processing or \u2502\n\u2502 analysis data.                                                                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> In\u00a0[16]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-4\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-4\") <pre>[22:49:09] [INFO] \ud83c\udfa8 Creating Data Designer dataset\n</pre> <pre>[22:49:09] [INFO] \u2705 Validation passed\n</pre> <pre>[22:49:09] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:49:09] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:49:09] [INFO]   |-- \ud83d\udc40 Checking 'meta/llama-4-scout-17b-16e-instruct' in provider named 'nvidia' for model alias 'vision'...\n</pre> <pre>[22:49:10] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:49:10] [INFO] \u23f3 Processing batch 1 of 1\n</pre> <pre>[22:49:10] [INFO] \ud83c\udf31 Sampling 10 records from seed dataset\n</pre> <pre>[22:49:10] [INFO]   |-- seed dataset size: 512 records\n</pre> <pre>[22:49:10] [INFO]   |-- sampling strategy: ordered\n</pre> <pre>[22:49:10] [INFO] llm-text model configuration for generating column 'summary'\n</pre> <pre>[22:49:10] [INFO]   |-- model: 'meta/llama-4-scout-17b-16e-instruct'\n</pre> <pre>[22:49:10] [INFO]   |-- model alias: 'vision'\n</pre> <pre>[22:49:10] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:49:10] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, temperature=0.60, top_p=0.95, max_tokens=2048\n</pre> <pre>[22:49:10] [INFO] \ud83d\udc19 Processing llm-text column 'summary' with 4 concurrent workers\n</pre> <pre>[22:49:33] [INFO] \ud83d\udcca Model usage summary:\n{\n    \"meta/llama-4-scout-17b-16e-instruct\": {\n        \"token_usage\": {\n            \"input_tokens\": 8140,\n            \"output_tokens\": 5076,\n            \"total_tokens\": 13216\n        },\n        \"request_usage\": {\n            \"successful_requests\": 10,\n            \"failed_requests\": 0,\n            \"total_requests\": 10\n        },\n        \"tokens_per_second\": 587,\n        \"requests_per_minute\": 26\n    }\n}\n</pre> <pre>[22:49:33] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:49:33] [INFO]   |-- \ud83d\udcdd column: 'summary'\n</pre> In\u00a0[17]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() Out[17]: uuid image_filename base64_image page options source summary 0 3fb63904-6755-4826-a8f9-aad20ccdc678 images/1810.07757_2.jpg iVBORw0KGgoAAAANSUhEUgAAAUAAAAIACAIAAAB8QiIMAA... ['A. The variance of the data decreases from p... arxiv_qa ## Image Summary  The image presents a collect... 1 e78f4cf3-a899-402b-9ce9-15b2fd1332a4 data/scrapped_pdfs_split/pages_extracted/energ... iVBORw0KGgoAAAANSUhEUgAAAYsAAAIACAIAAAD8HddaAA... 9 &lt;NA&gt; pdf ## Detailed Summary of the Content in the Imag... 2 b0d713d3-ba84-439b-9239-24e1fe541fdd data/scrapped_pdfs_split/pages_extracted/energ... iVBORw0KGgoAAAANSUhEUgAAAZgAAAIACAIAAAAwhO2xAA... 414 &lt;NA&gt; pdf ## Lithium Batteries Page 1487  ### Top Sectio... 3 abe793fc-56e4-47f0-8bfa-6f1f7cf28cd8 0fd47b51ae9248ef36669b8619b1223f268edae3e7a44a... iVBORw0KGgoAAAANSUhEUgAAAX0AAAIACAAAAABLRuMPAA... &lt;NA&gt; &lt;NA&gt; docvqa ## Document Summary The document appears to be... 4 625e6644-8e00-480c-9ea0-6bfd6d76baf0 b335cfb9d442f8925ea41a064cb445a5395577f2345d52... iVBORw0KGgoAAAANSUhEUgAAAY8AAAIACAAAAABf/7+rAA... &lt;NA&gt; &lt;NA&gt; docvqa ### Detailed Summary of the Content in the Ima... In\u00a0[18]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 10                              \u2502 1                               \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83d\udcdd LLM-Text Columns                                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                  \u2503               \u2503                              \u2503       prompt tokens \u2503       completion tokens \u2503\n\u2503 column name      \u2503     data type \u2503         number unique values \u2503          per record \u2503              per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 summary          \u2502        string \u2502                  10 (100.0%) \u2502        38.0 +/- 0.0 \u2502         523.5 +/- 155.1 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre>"},{"location":"notebooks/4-providing-images-as-context/#data-designer-tutorial-providing-images-as-context-for-vision-based-data-generation","title":"\ud83c\udfa8 Data Designer Tutorial: Providing Images as Context for Vision-Based Data Generation\u00b6","text":""},{"location":"notebooks/4-providing-images-as-context/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>This notebook demonstrates how to provide images as context to generate text descriptions using vision-language models.</p> <ul> <li>\u2728 Visual Document Processing: Converting images to chat-ready format for model consumption</li> <li>\ud83d\udd0d Vision-Language Generation: Using vision models to generate detailed summaries from images</li> </ul> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"notebooks/4-providing-images-as-context/#import-the-essentials","title":"\ud83d\udce6 Import the essentials\u00b6","text":"<ul> <li>The <code>essentials</code> module provides quick access to the most commonly used objects.</li> </ul>"},{"location":"notebooks/4-providing-images-as-context/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object is responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebooks/4-providing-images-as-context/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebooks/4-providing-images-as-context/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebooks/4-providing-images-as-context/#seed-dataset-creation","title":"\ud83c\udf31 Seed Dataset Creation\u00b6","text":"<p>In this section, we'll prepare our visual documents as a seed dataset for summarization:</p> <ul> <li>Loading Visual Documents: We use the ColPali dataset containing document images</li> <li>Image Processing: Convert images to base64 format for vision model consumption</li> <li>Metadata Extraction: Preserve relevant document information (filename, page number, source, etc.)</li> </ul> <p>The seed dataset will be used to generate detailed text summaries of each document image.</p>"},{"location":"notebooks/4-providing-images-as-context/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013 preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebooks/4-providing-images-as-context/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebooks/4-providing-images-as-context/#visual-inspection","title":"\ud83d\udd0e Visual Inspection\u00b6","text":"<p>Let's compare the original document image with the generated summary to validate quality:</p>"},{"location":"notebooks/4-providing-images-as-context/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebooks/4-providing-images-as-context/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Now that you've learned how to use visual context for image summarization in Data Designer, explore more:</p> <ul> <li>Experiment with different vision models for specific document types</li> <li>Try different prompt variations to generate specialized descriptions (e.g., technical details, key findings)</li> <li>Combine vision-based summaries with other column types for multi-modal workflows</li> <li>Apply this pattern to other vision tasks like image captioning, OCR validation, or visual question answering</li> </ul>"},{"location":"plugins/available/","title":"\ud83d\udea7 Coming Soon","text":"<p>This page will list available Data Designer plugins. Stay tuned!</p>"},{"location":"plugins/example/","title":"Example Plugin","text":"<p>Experimental Feature</p> <p>The plugin system is currently experimental and under active development. The documentation, examples, and plugin interface are subject to significant changes in future releases. If you encounter any issues, have questions, or have ideas for improvement, please consider starting a discussion on GitHub.</p>"},{"location":"plugins/example/#example-plugin-index-multiplier","title":"Example Plugin: Index Multiplier","text":"<p>In this guide, we will build a simple plugin that generates values by multiplying the row index by a user-specified multiplier. Admittedly, not the most useful plugin, but it demonstrates the required steps \ud83d\ude1c.</p> <p>A Data Designer plugin is implemented as a Python package with three main components:</p> <ol> <li>Configuration Class: Defines the parameters users can configure</li> <li>Task Class: Contains the core implementation of the plugin</li> <li>Plugin Object: Connects the config and task classes to make the plugin discoverable</li> </ol> <p>Let's build the <code>data-designer-index-multiplier</code> plugin step by step.</p>"},{"location":"plugins/example/#step-1-create-a-python-package","title":"Step 1: Create a Python package","text":"<p>Data Designer plugins are implemented as Python packages. We recommend using a standard structure for your plugin package.</p> <p>For example, here is the structure of a <code>data-designer-index-multiplier</code> plugin:</p> <pre><code>data-designer-index-multiplier/\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 data_designer_index_multiplier/\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 plugin.py\n</code></pre>"},{"location":"plugins/example/#step-2-create-the-config-class","title":"Step 2: Create the config class","text":"<p>The configuration class defines what parameters users can set when using your plugin. For column generator plugins, it must inherit from SingleColumnConfig and include a discriminator field.</p> <pre><code>from typing import Literal\nfrom data_designer.config.column_configs import SingleColumnConfig\n\nclass IndexMultiplierColumnConfig(SingleColumnConfig):\n    \"\"\"Configuration for the index multiplier column generator.\"\"\"\n\n    # Configurable parameter for this plugin\n    multiplier: int = 2\n\n    # Required: discriminator field with a unique Literal type\n    # This value identifies your plugin and becomes its column_type\n    column_type: Literal[\"index-multiplier\"] = \"index-multiplier\"\n</code></pre> <p>Key points:</p> <ul> <li>The <code>column_type</code> field must be a <code>Literal</code> type with a string default</li> <li>This value uniquely identifies your plugin (use kebab-case)</li> <li>Add any custom parameters your plugin needs (here: <code>multiplier</code>)</li> <li><code>SingleColumnConfig</code> is a Pydantic model, so you can leverage all of Pydantic's validation features</li> </ul>"},{"location":"plugins/example/#step-3-create-the-implementation-class","title":"Step 3: Create the implementation class","text":"<p>The implementation class defines the actual business logic of the plugin. For column generator plugins, it inherits from ColumnGenerator and must implement a <code>metadata</code> static method and <code>generate</code> method:</p> <pre><code>import logging\nimport pandas as pd\n\nfrom data_designer.engine.column_generators.generators.base import (\n    ColumnGenerator,\n    GenerationStrategy,\n    GeneratorMetadata,\n)\n\n# Data Designer uses the standard Python logging module for logging\nlogger = logging.getLogger(__name__)\n\nclass IndexMultiplierColumnGenerator(ColumnGenerator[IndexMultiplierColumnConfig]):\n    @staticmethod\n    def metadata() -&gt; GeneratorMetadata:\n        \"\"\"Define metadata about this generator.\"\"\"\n        return GeneratorMetadata(\n            name=\"index-multiplier\",\n            description=\"Generates values by multiplying the row index by a user-specified multiplier\",\n            generation_strategy=GenerationStrategy.FULL_COLUMN,\n        )\n\n    def generate(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Generate the column data.\n\n        Args:\n            data: The current DataFrame being built\n\n        Returns:\n            The DataFrame with the new column added\n        \"\"\"\n        logger.info(\n            f\"Generating column {self.config.name} \"\n            f\"with multiplier {self.config.multiplier}\"\n        )\n\n        # Access config via self.config\n        data[self.config.name] = data.index * self.config.multiplier\n\n        return data\n</code></pre> <p>Key points:</p> <ul> <li>Generic type <code>ColumnGenerator[IndexMultiplierColumnConfig]</code> connects the task to its config</li> <li><code>metadata()</code> describes your generator and its requirements</li> <li><code>generation_strategy</code> can be <code>FULL_COLUMN</code>, <code>CELL_BY_CELL</code></li> <li>You have access to the configuration parameters via <code>self.config</code></li> </ul> <p>Understanding generation_strategy</p> <p>The <code>generation_strategy</code> specifies how the column generator will generate data.</p> <ul> <li> <p><code>FULL_COLUMN</code>: Generates the full column (at the batch level) in a single call to <code>generate</code></p> <ul> <li><code>generate</code> must take as input a <code>pd.DataFrame</code> with all previous columns and return a <code>pd.DataFrame</code> with the generated column appended</li> </ul> </li> <li> <p><code>CELL_BY_CELL</code>: Generates one cell at a time</p> <ul> <li><code>generate</code> must take as input a <code>dict</code> with key/value pairs for all previous columns and return a <code>dict</code> with an additional key/value for the generated cell</li> <li>Supports concurrent workers via a <code>max_parallel_requests</code> parameter on the configuration</li> </ul> </li> </ul>"},{"location":"plugins/example/#step-4-create-the-plugin-object","title":"Step 4: Create the plugin object","text":"<p>Create a <code>Plugin</code> object that makes the plugin discoverable and connects the task and config classes.</p> <pre><code>from data_designer.plugins import Plugin, PluginType\n\n# Plugin instance - this is what gets loaded via entry point\nplugin = Plugin(\n    impl_qualified_name=\"data_designer_index_multiplier.plugin.IndexMultiplierColumnGenerator\",\n    config_qualified_name=\"data_designer_index_multiplier.plugin.IndexMultiplierColumnConfig\",\n    plugin_type=PluginType.COLUMN_GENERATOR,\n    emoji=\"\ud83d\udd0c\",\n)\n</code></pre>"},{"location":"plugins/example/#complete-plugin-code","title":"Complete plugin code","text":"<p>Pulling it all together, here is the complete plugin code for <code>src/data_designer_index_multiplier/plugin.py</code>:</p> <pre><code>import logging\nfrom typing import Literal\n\nimport pandas as pd\n\nfrom data_designer.config.column_configs import SingleColumnConfig\nfrom data_designer.engine.column_generators.generators.base import (\n    ColumnGenerator,\n    GenerationStrategy,\n    GeneratorMetadata,\n)\nfrom data_designer.plugins import Plugin, PluginType\n\n# Data Designer uses the standard Python logging module for logging\nlogger = logging.getLogger(__name__)\n\n\nclass IndexMultiplierColumnConfig(SingleColumnConfig):\n    \"\"\"Configuration for the index multiplier column generator.\"\"\"\n\n    # Configurable parameter for this plugin\n    multiplier: int = 2\n\n    # Required: discriminator field with a unique Literal type\n    # This value identifies your plugin and becomes its column_type\n    column_type: Literal[\"index-multiplier\"] = \"index-multiplier\"\n\n\nclass IndexMultiplierColumnGenerator(ColumnGenerator[IndexMultiplierColumnConfig]):\n    @staticmethod\n    def metadata() -&gt; GeneratorMetadata:\n        \"\"\"Define metadata about this generator.\"\"\"\n        return GeneratorMetadata(\n            name=\"index-multiplier\",\n            description=\"Generates values by multiplying the row index by a user-specified multiplier\",\n            generation_strategy=GenerationStrategy.FULL_COLUMN,\n        )\n\n    def generate(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Generate the column data.\n\n        Args:\n            data: The current DataFrame being built\n\n        Returns:\n            The DataFrame with the new column added\n        \"\"\"\n        logger.info(\n            f\"Generating column {self.config.name} \"\n            f\"with multiplier {self.config.multiplier}\"\n        )\n\n        # Access config via self.config\n        data[self.config.name] = data.index * self.config.multiplier\n\n        return data\n\n\n# Plugin instance - this is what gets loaded via entry point\nplugin = Plugin(\n    impl_qualified_name=\"data_designer_index_multiplier.plugin.IndexMultiplierColumnGenerator\",\n    config_qualified_name=\"data_designer_index_multiplier.plugin.IndexMultiplierColumnConfig\",\n    plugin_type=PluginType.COLUMN_GENERATOR,\n    emoji=\"\ud83d\udd0c\",\n)\n</code></pre>"},{"location":"plugins/example/#step-5-package-your-plugin","title":"Step 5: Package your plugin","text":"<p>Create a <code>pyproject.toml</code> file to define your package and register the entry point:</p> <pre><code>[project]\nname = \"data-designer-index-multiplier\"\nversion = \"1.0.0\"\ndescription = \"Data Designer index multiplier plugin\"\nrequires-python = \"&gt;=3.10\"\ndependencies = [\n    \"data-designer\",\n]\n\n# Register this plugin via entry points\n[project.entry-points.\"data_designer.plugins\"]\nindex-multiplier = \"data_designer_index_multiplier.plugin:plugin\"\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src/data_designer_index_multiplier\"]\n</code></pre> <p>Entry Point Registration</p> <p>Plugins are discovered automatically using Python entry points. It is important to register your plugin as an entry point under the <code>data_designer.plugins</code> group.</p> <p>The entry point format is: <pre><code>[project.entry-points.\"data_designer.plugins\"]\n&lt;entry-point-name&gt; = \"&lt;module.path&gt;:&lt;plugin-instance-name&gt;\"\n</code></pre></p>"},{"location":"plugins/example/#step-6-use-your-plugin","title":"Step 6: Use your plugin","text":"<p>Install your plugin in editable mode for testing:</p> <pre><code># From the plugin directory\nuv pip install -e .\n</code></pre> <p>Once installed, your plugin works just like built-in column types:</p> <pre><code>from data_designer_index_multiplier.plugin import IndexMultiplierColumnConfig\n\nfrom data_designer.essentials import (\n    CategorySamplerParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    SamplerColumnConfig,\n)\n\ndata_designer = DataDesigner()\nbuilder = DataDesignerConfigBuilder()\n\n# Add a regular column\nbuilder.add_column(\n    SamplerColumnConfig(\n        name=\"category\",\n        sampler_type=\"category\",\n        params=CategorySamplerParams(values=[\"A\", \"B\", \"C\"]),\n    )\n)\n\n# Add your custom plugin column\nbuilder.add_column(\n    IndexMultiplierColumnConfig(\n        name=\"v\",\n        multiplier=5,\n    )\n)\n\n# Generate data\nresults = data_designer.create(builder, num_records=10)\nprint(results.load_dataset())\n</code></pre> <p>Output: <pre><code>  category  multiplied-index\n0        B                 0\n1        A                 5\n2        C                10\n3        A                15\n4        B                20\n...\n</code></pre></p> <p>That's it! You have now created and used your first Data Designer plugin. The last step is to package your plugin and share it with the community \ud83d\ude80</p>"},{"location":"plugins/overview/","title":"Data Designer Plugins","text":"<p>Experimental Feature</p> <p>The plugin system is currently experimental and under active development. The documentation, examples, and plugin interface are subject to significant changes in future releases. If you encounter any issues, have questions, or have ideas for improvement, please consider starting a discussion on GitHub.</p>"},{"location":"plugins/overview/#what-are-plugins","title":"What are plugins?","text":"<p>Plugins are Python packages that extend Data Designer's capabilities without modifying the core library. Similar to VS Code extensions and Pytest plugins, the plugin system empowers you to build specialized extensions for your specific use cases and share them with the community.</p> <p>Current capabilities: Data Designer currently supports plugins for column generators (the column types you pass to the config builder's add_column method).</p> <p>Coming soon: Plugin support for processors, validators, and more!</p>"},{"location":"plugins/overview/#how-do-you-use-plugins","title":"How do you use plugins?","text":"<p>A Data Designer plugin is just a Python package configured with an entry point that points to a Data Designer <code>Plugin</code> object. Using a plugin is as simple as installing the package:</p> <pre><code>pip install data-designer-{plugin-name}\n</code></pre> <p>Once installed, plugins are automatically discovered and ready to use. See the example plugin for a complete walkthrough.</p>"},{"location":"plugins/overview/#how-do-you-create-plugins","title":"How do you create plugins?","text":"<p>Creating a plugin involves three main steps:</p>"},{"location":"plugins/overview/#1-implement-the-plugin-components","title":"1. Implement the Plugin Components","text":"<ul> <li>Create a task class inheriting from <code>ColumnGenerator</code></li> <li>Create a config class inheriting from <code>SingleColumnConfig</code></li> <li>Instantiate a <code>Plugin</code> object connecting them</li> </ul>"},{"location":"plugins/overview/#2-package-your-plugin","title":"2. Package Your Plugin","text":"<ul> <li>Set up a Python package with <code>pyproject.toml</code></li> <li>Register your plugin using entry points</li> <li>Define dependencies (including <code>data-designer</code>)</li> </ul>"},{"location":"plugins/overview/#3-share-your-plugin","title":"3. Share Your Plugin","text":"<ul> <li>Publish to PyPI or another package index</li> <li>Share with the community!</li> </ul> <p>Ready to get started? See the Example Plugin for a complete walkthrough!</p>"},{"location":"recipes/cards/","title":"Use Case Recipes","text":"<p>Recipes are a collection of code examples that demonstrate how to leverage Data Designer in specific use cases. Each recipe is a self-contained example that can be run independently.</p> <p>New to Data Designer?</p> <p>Recipes provide working code for specific use cases without detailed explanations. If you're learning Data Designer for the first time, we recommend starting with our tutorial notebooks, which offer step-by-step guidance and explain core concepts. Once you're familiar with the basics, return here for practical, ready-to-use implementations.</p> <ul> <li> <p> Text to Python</p> <p>Generate a dataset of natural language instructions paired with Python code implementations, with varying complexity levels and industry focuses.</p> <p>Demonstrates:</p> <ul> <li>Python code generation</li> <li>Python code validation</li> <li>LLM-as-judge</li> </ul> <p> View Recipe Download Code </p> </li> <li> <p> Text to SQL</p> <p>Generate a dataset of natural language instructions paired with SQL code implementations, with varying complexity levels and industry focuses.</p> <p>Demonstrates:</p> <ul> <li>SQL code generation</li> <li>SQL code validation</li> <li>LLM-as-judge</li> </ul> <p> View Recipe Download Code </p> </li> <li> <p> Product Info QA</p> <p>Generate a dataset that contains information about products and associated question/answer pairs.</p> <p>Demonstrates:</p> <ul> <li>Structured outputs</li> <li>Expression columns</li> <li>LLM-as-judge</li> </ul> <p> View Recipe Download Code </p> </li> <li> <p> Multi-Turn Chat</p> <p>Generate a dataset of multi-turn chat conversations between a user and an AI assistant.</p> <p>Demonstrates:</p> <ul> <li>Structured outputs</li> <li>Expression columns</li> <li>LLM-as-judge</li> </ul> <p> View Recipe Download Code </p> </li> </ul>"},{"location":"recipes/code_generation/text_to_python/","title":"Text to Python","text":"<p>Download Code </p> <pre><code>from pathlib import Path\n\nfrom data_designer.essentials import (\n    CategorySamplerParams,\n    CodeLang,\n    CodeValidatorParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    LLMCodeColumnConfig,\n    LLMJudgeColumnConfig,\n    LLMTextColumnConfig,\n    SamplerColumnConfig,\n    SamplerType,\n    Score,\n    SubcategorySamplerParams,\n    ValidationColumnConfig,\n    ValidatorType,\n)\nfrom data_designer.interface.results import DatasetCreationResults\n\n\ndef build_config(model_alias: str) -&gt; DataDesignerConfigBuilder:\n    config_builder = DataDesignerConfigBuilder()\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"industry_sector\",\n            sampler_type=SamplerType.CATEGORY,\n            params=CategorySamplerParams(\n                values=[\n                    \"Healthcare\",\n                    \"Finance\",\n                    \"Technology\",\n                ],\n            ),\n        ),\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"topic\",\n            sampler_type=SamplerType.SUBCATEGORY,\n            params=SubcategorySamplerParams(\n                category=\"industry_sector\",\n                values={\n                    \"Healthcare\": [\n                        \"Electronic Health Records (EHR) Systems\",\n                        \"Telemedicine Platforms\",\n                        \"AI-Powered Diagnostic Tools\",\n                    ],\n                    \"Finance\": [\n                        \"Fraud Detection Software\",\n                        \"Automated Trading Systems\",\n                        \"Personal Finance Apps\",\n                    ],\n                    \"Technology\": [\n                        \"Cloud Computing Platforms\",\n                        \"Artificial Intelligence and Machine Learning Platforms\",\n                        \"DevOps and CI/CD Tools\",\n                    ],\n                },\n            ),\n        ),\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"code_complexity\",\n            sampler_type=SamplerType.CATEGORY,\n            params=CategorySamplerParams(\n                values=[\n                    \"Beginner\",\n                    \"Intermediate\",\n                    \"Advanced\",\n                ],\n            ),\n        ),\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"code_concept\",\n            sampler_type=SamplerType.SUBCATEGORY,\n            params=SubcategorySamplerParams(\n                category=\"code_complexity\",\n                values={\n                    \"Beginner\": [\n                        \"Variables\",\n                        \"Data Types\",\n                        \"Functions\",\n                        \"Loops\",\n                        \"Classes\",\n                    ],\n                    \"Intermediate\": [\n                        \"List Comprehensions\",\n                        \"Object-oriented programming\",\n                        \"Lambda Functions\",\n                        \"Web frameworks\",\n                        \"Pandas\",\n                    ],\n                    \"Advanced\": [\n                        \"Multithreading\",\n                        \"Context Managers\",\n                        \"Generators\",\n                    ],\n                },\n            ),\n        ),\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"instruction_phrase\",\n            sampler_type=SamplerType.CATEGORY,\n            params=CategorySamplerParams(\n                values=[\n                    \"Write a function that\",\n                    \"Create a class that\",\n                    \"Implement a script\",\n                    \"Can you create a function\",\n                    \"Develop a module that\",\n                ],\n            ),\n        ),\n    )\n\n    config_builder.add_column(\n        LLMTextColumnConfig(\n            name=\"instruction\",\n            model_alias=model_alias,\n            system_prompt=(\"You are an expert at generating clear and specific programming tasks.\"),\n            prompt=(\n                \"Generate an instruction to create Python code that solves a specific problem.\\n\"\n                \"Each instruction should begin with one of the following phrases: {{ instruction_phrase }}.\\n\\n\"\n                \"Important Guidelines:\\n\"\n                \"* Industry Relevance: Ensure the instruction pertains to the {{ industry_sector }} sector and {{ topic }} topic.\\n\"\n                \"* Code Complexity: Tailor the instruction to the {{ code_complexity }} level. Utilize relevant {{ code_concept }} where appropriate to match the complexity level.\\n\"\n                \"* Clarity and Specificity: Make the problem statement clear and unambiguous. Provide sufficient context to understand the requirements without being overly verbose.\\n\"\n                \"* Response Formatting: Do not include any markers such as ### Response ### in the instruction.\\n\"\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        LLMCodeColumnConfig(\n            name=\"code_implementation\",\n            model_alias=model_alias,\n            code_lang=CodeLang.PYTHON,\n            system_prompt=(\n                \"You are an expert Python programmer who writes clean, efficient, and well-documented code.\"\n            ),\n            prompt=(\n                \"Write Python code for the following instruction:\\n\"\n                \"Instruction: {{ instruction }}\\n\\n\"\n                \"Important Guidelines:\\n\"\n                \"* Code Quality: Your code should be clean, complete, self-contained, and accurate.\\n\"\n                \"* Code Validity: Please ensure that your Python code is executable and does not contain any errors.\\n\"\n                \"* Packages: Remember to import any necessary libraries, and to use all libraries you import.\\n\"\n                \"* Complexity &amp; Concepts: The code should be written at a {{ code_complexity }} level, making use of concepts such as {{code_concept}}.\\n\"\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        LLMTextColumnConfig(\n            name=\"instruction\",\n            model_alias=model_alias,\n            system_prompt=(\"You are an expert at generating clear and specific programming tasks.\"),\n            prompt=(\n                \"Generate an instruction to create Python code that solves a specific problem.\\n\"\n                \"Each instruction should begin with one of the following phrases: {{ instruction_phrase }}.\\n\\n\"\n                \"Important Guidelines:\\n\"\n                \"* Industry Relevance: Ensure the instruction pertains to the {{ industry_sector }} sector and {{ topic }} topic.\\n\"\n                \"* Code Complexity: Tailor the instruction to the {{ code_complexity }} level. Utilize relevant {{ code_concept }} where appropriate to match the complexity level.\\n\"\n                \"* Clarity and Specificity: Make the problem statement clear and unambiguous. Provide sufficient context to understand the requirements without being overly verbose.\\n\"\n                \"* Response Formatting: Do not include any markers such as ### Response ### in the instruction.\\n\"\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        LLMCodeColumnConfig(\n            name=\"code_implementation\",\n            model_alias=model_alias,\n            code_lang=CodeLang.PYTHON,\n            system_prompt=(\n                \"You are an expert Python programmer who writes clean, efficient, and well-documented code.\"\n            ),\n            prompt=(\n                \"Write Python code for the following instruction:\\n\"\n                \"Instruction: {{ instruction }}\\n\\n\"\n                \"Important Guidelines:\\n\"\n                \"* Code Quality: Your code should be clean, complete, self-contained, and accurate.\\n\"\n                \"* Code Validity: Please ensure that your Python code is executable and does not contain any errors.\\n\"\n                \"* Packages: Remember to import any necessary libraries, and to use all libraries you import.\\n\"\n                \"* Complexity &amp; Concepts: The code should be written at a {{ code_complexity }} level, making use of concepts such as {{ code_concept }}.\\n\"\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        LLMJudgeColumnConfig(\n            name=\"code_judge_result\",\n            model_alias=model_alias,\n            prompt=TEXT_TO_PYTHON_JUDGE_TEMPLATE,\n            scores=python_scoring,\n        )\n    )\n\n    config_builder.add_column(\n        ValidationColumnConfig(\n            name=\"code_validity_result\",\n            validator_type=ValidatorType.CODE,\n            target_columns=[\"code_implementation\"],\n            validator_params=CodeValidatorParams(\n                code_lang=CodeLang.PYTHON,\n            ),\n            batch_size=100,\n        )\n    )\n\n    return config_builder\n\n\ndef create_dataset(\n    config_builder: DataDesignerConfigBuilder,\n    num_records: int,\n    artifact_path: Path | str | None = None,\n) -&gt; DatasetCreationResults:\n    data_designer = DataDesigner(artifact_path=artifact_path)\n    results = data_designer.create(config_builder, num_records=num_records)\n    return results\n\n\nTEXT_TO_PYTHON_JUDGE_TEMPLATE = \"\"\"\\\nYou are an expert in Python programming, with specialized knowledge in software engineering, data science, and algorithmic problem-solving.\n\nYou think about potential flaws and errors in the code. You are a tough critic, but a fair one.\n\nTake a deep breath and use the Python Code Quality Rubric below to score the **Generated Python Code** based on the INSTRUCTIONS.\n\n#### INSTRUCTIONS\nThe Generated Python Code should be a valid response to the Natural Language Prompt below\n\nNatural Language Prompt:\n{{ instruction }}\n\nGenerated Python Code\n{{ code_implementation }}\n\"\"\"\n\n\npython_scoring = [\n    Score(\n        name=\"Relevance\",\n        description=\"Adherence to INSTRUCTIONS and CONTEXT\",\n        options={\n            4: \"Perfectly meets all specified requirements.\",\n            3: \"Meets most requirements with minor deviations.\",\n            2: \"Moderate deviation from the instructions.\",\n            1: \"Significant deviations from the instructions.\",\n            0: \"Does not adhere to the instructions.\",\n        },\n    ),\n    Score(\n        name=\"Pythonic\",\n        description=\"Pythonic Code and Best Practices (Does the code follow Python conventions and best practices?)\",\n        options={\n            4: \"The code exemplifies Pythonic principles, making excellent use of Python-specific constructs, standard library modules and programming idioms; follows all relevant PEPs.\",\n            3: \"The code closely follows Python conventions and adheres to many best practices; good use of Python-specific constructs, standard library modules and programming idioms.\",\n            2: \"The code generally follows Python conventions but has room for better alignment with Pythonic practices.\",\n            1: \"The code loosely follows Python conventions, with several deviations from best practices.\",\n            0: \"The code does not follow Python conventions or best practices, using non-Pythonic approaches.\",\n        },\n    ),\n    Score(\n        name=\"Readability\",\n        description=\"Readability and Maintainability (Is the Python code easy to understand and maintain?)\",\n        options={\n            4: (\n                \"The code is excellently formatted, follows PEP 8 guidelines, is elegantly concise and clear, uses meaningful variable names, \"\n                \"ensuring high readability and ease of maintenance; organizes complex logic well. Docstrings are given in a Google Docstring format.\"\n            ),\n            3: \"The code is well-formatted in the sense of code-as-documentation, making it relatively easy to understand and maintain; uses descriptive names and organizes logic clearly.\",\n            2: \"The code is somewhat readable with basic formatting and some comments, but improvements are needed; needs better use of descriptive names and organization.\",\n            1: \"The code has minimal formatting, making it hard to understand; lacks meaningful names and organization.\",\n            0: \"The code is unreadable, with no attempt at formatting or description.\",\n        },\n    ),\n    Score(\n        name=\"Efficiency\",\n        description=\"Efficiency and Performance (Is the code optimized for performance?)\",\n        options={\n            4: \"The solution is highly efficient, using appropriate data structures and algorithms; avoids unnecessary computations and optimizes for both time and space complexity.\",\n            3: \"The solution is efficient, with good use of Python's built-in functions and libraries; minor areas for optimization.\",\n            2: \"The solution is moderately efficient, but misses some opportunities for optimization; uses some inefficient patterns.\",\n            1: \"The solution shows poor efficiency, with notable performance issues; lacks effective optimization techniques.\",\n            0: \"The solution is highly inefficient; overlooks fundamental optimization practices, resulting in significant performance issues.\",\n        },\n    ),\n]\n\n\nif __name__ == \"__main__\":\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser()\n    parser.add_argument(\"--model-alias\", type=str, default=\"openai-text\")\n    parser.add_argument(\"--num-records\", type=int, default=5)\n    parser.add_argument(\"--artifact-path\", type=str, default=None)\n    args = parser.parse_args()\n\n    config_builder = build_config(model_alias=args.model_alias)\n    results = create_dataset(config_builder, num_records=args.num_records, artifact_path=args.artifact_path)\n\n    print(f\"Dataset saved to: {results.artifact_storage.final_dataset_path}\")\n\n    results.load_analysis().to_report()\n</code></pre>"},{"location":"recipes/code_generation/text_to_sql/","title":"Text to SQL","text":"<p>Download Code </p> <pre><code>from pathlib import Path\n\nfrom data_designer.essentials import (\n    CategorySamplerParams,\n    CodeLang,\n    CodeValidatorParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    LLMCodeColumnConfig,\n    LLMJudgeColumnConfig,\n    LLMTextColumnConfig,\n    SamplerColumnConfig,\n    SamplerType,\n    Score,\n    SubcategorySamplerParams,\n    ValidationColumnConfig,\n    ValidatorType,\n)\nfrom data_designer.interface.results import DatasetCreationResults\n\n\ndef build_config(model_alias: str) -&gt; DataDesignerConfigBuilder:\n    config_builder = DataDesignerConfigBuilder()\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"industry_sector\",\n            sampler_type=SamplerType.CATEGORY,\n            params=CategorySamplerParams(\n                values=[\"Healthcare\", \"Finance\", \"Technology\"],\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"topic\",\n            sampler_type=SamplerType.SUBCATEGORY,\n            params=SubcategorySamplerParams(\n                category=\"industry_sector\",\n                values={\n                    \"Healthcare\": [\n                        \"Electronic Health Records (EHR) Systems\",\n                        \"Telemedicine Platforms\",\n                        \"AI-Powered Diagnostic Tools\",\n                    ],\n                    \"Finance\": [\n                        \"Fraud Detection Software\",\n                        \"Automated Trading Systems\",\n                        \"Personal Finance Apps\",\n                    ],\n                    \"Technology\": [\n                        \"Cloud Computing Platforms\",\n                        \"Artificial Intelligence and Machine Learning Platforms\",\n                        \"DevOps and CI/CD Tools\",\n                    ],\n                },\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"sql_complexity\",\n            sampler_type=SamplerType.CATEGORY,\n            params=CategorySamplerParams(\n                values=[\"Beginner\", \"Intermediate\", \"Advanced\"],\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"sql_concept\",\n            sampler_type=SamplerType.SUBCATEGORY,\n            params=SubcategorySamplerParams(\n                category=\"sql_complexity\",\n                values={\n                    \"Beginner\": [\n                        \"Basic SELECT Statements\",\n                        \"WHERE Clauses\",\n                        \"Basic JOINs\",\n                        \"INSERT, UPDATE, DELETE\",\n                    ],\n                    \"Intermediate\": [\n                        \"Aggregation Functions\",\n                        \"Multiple JOINs\",\n                        \"Subqueries\",\n                        \"Views\",\n                    ],\n                    \"Advanced\": [\n                        \"Window Functions\",\n                        \"Common Table Expressions (CTEs)\",\n                        \"Stored Procedures\",\n                        \"Query Optimization\",\n                    ],\n                },\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"sql_task_type\",\n            sampler_type=SamplerType.CATEGORY,\n            params=CategorySamplerParams(\n                values=[\n                    \"Data Retrieval\",\n                    \"Data Manipulation\",\n                    \"Analytics and Reporting\",\n                    \"Data Transformation\",\n                ],\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"instruction_phrase\",\n            sampler_type=SamplerType.CATEGORY,\n            params=CategorySamplerParams(\n                values=[\n                    \"Write an SQL query that\",\n                    \"Create an SQL statement to\",\n                    \"Develop an SQL query to\",\n                    \"Can you write SQL that\",\n                    \"Formulate an SQL query that\",\n                ],\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        LLMTextColumnConfig(\n            name=\"sql_prompt\",\n            model_alias=model_alias,\n            system_prompt=\"You are an expert at generating clear and specific SQL tasks.\",\n            prompt=SQL_PROMPT_TEXT,\n        )\n    )\n\n    config_builder.add_column(\n        LLMCodeColumnConfig(\n            name=\"sql_context\",\n            model_alias=model_alias,\n            code_lang=CodeLang.SQL_ANSI,\n            system_prompt=(\n                \"You are an expert SQL database designer who creates clean, efficient, and \"\n                \"well-structured database schemas.\"\n            ),\n            prompt=SQL_CONTEXT_TEXT,\n        )\n    )\n\n    config_builder.add_column(\n        LLMCodeColumnConfig(\n            name=\"sql\",\n            model_alias=model_alias,\n            code_lang=CodeLang.SQL_ANSI,\n            system_prompt=\"You are an expert SQL programmer who writes clean, efficient, and well-structured queries.\",\n            prompt=SQL_CODE_TEXT,\n        )\n    )\n\n    config_builder.add_column(\n        ValidationColumnConfig(\n            name=\"code_validity_result\",\n            validator_type=ValidatorType.CODE,\n            target_columns=[\"sql\"],\n            validator_params=CodeValidatorParams(\n                code_lang=CodeLang.SQL_ANSI,\n            ),\n            batch_size=100,\n        )\n    )\n\n    config_builder.add_column(\n        LLMJudgeColumnConfig(\n            name=\"code_judge_result\",\n            model_alias=model_alias,\n            prompt=TEXT_TO_SQL_JUDGE_TEMPLATE,\n            scores=sql_scoring,\n        )\n    )\n\n    return config_builder\n\n\ndef create_dataset(\n    config_builder: DataDesignerConfigBuilder,\n    num_records: int,\n    artifact_path: Path | str | None = None,\n) -&gt; DatasetCreationResults:\n    data_designer = DataDesigner(artifact_path=artifact_path)\n    results = data_designer.create(config_builder, num_records=num_records)\n    return results\n\n\nSQL_PROMPT_TEXT = (\n    \"Generate an instruction to create SQL code that solves a specific problem.\\n\"\n    \"Each instruction should begin with one of the following phrases: {{instruction_phrase}}.\\n\\n\"\n    \"Important Guidelines:\\n\"\n    \"* Industry Relevance: Ensure the instruction pertains to the {{industry_sector}} sector and {{topic}} topic.\\n\"\n    \"* SQL Complexity: Tailor the instruction to the {{sql_complexity}} level. Utilize relevant {{sql_concept}} \"\n    \"where appropriate to match the complexity level.\\n\"\n    \"* Task Type: The instruction should involve a {{sql_task_type}} task.\\n\"\n    \"* Clarity and Specificity: Make the problem statement clear and unambiguous. Provide sufficient context to \"\n    \"understand the requirements without being overly verbose.\\n\"\n    \"* Response Formatting: Do not include any markers such as ### Response ### in the instruction.\\n\"\n)\n\nSQL_CONTEXT_TEXT = (\n    \"Generate the SQL for creating database tables that would be relevant for the following instruction:\\n\"\n    \"Instruction: {{sql_prompt}}\\n\\n\"\n    \"Important Guidelines:\\n\"\n    \"* Relevance: Ensure all tables are directly related to the {{industry_sector}} sector and {{topic}} topic.\\n\"\n    \"* Completeness: Include all essential columns with appropriate data types, primary/foreign keys, and necessary constraints.\\n\"\n    \"* Realism: Use realistic table structures typical for the specified industry.\\n\"\n    \"* Executable SQL: Provide complete CREATE TABLE statements that can be run without modification.\\n\"\n    \"* Consistency: Use consistent naming conventions (e.g., snake_case for table and column names).\\n\"\n    \"* Sample Data: Include INSERT statements with sample data that makes sense for the tables (at least 5-10 rows per table).\"\n)\n\nSQL_CODE_TEXT = (\n    \"Write SQL code for the following instruction based on the provided database context:\\n\"\n    \"Instruction: {{sql_prompt}}\\n\\n\"\n    \"Database Context:\\n\"\n    \"{{sql_context}}\\n\\n\"\n    \"Important Guidelines:\\n\"\n    \"* Code Quality: Your SQL should be clean, complete, self-contained and accurate.\\n\"\n    \"* Code Validity: Please ensure that your SQL code is executable and does not contain any errors.\\n\"\n    \"* Context: Base your query on the provided database context. Only reference tables and columns that \"\n    \"exist in the context.\\n\"\n    \"* Complexity &amp; Concepts: The SQL should be written at a {{sql_complexity}} level, making use of \"\n    \"concepts such as {{sql_concept}}.\\n\"\n    \"* Task Type: Ensure your solution implements the appropriate {{sql_task_type}} operation.\\n\"\n    \"* Comments: Include brief comments explaining the key parts of your query.\\n\"\n)\n\n\nTEXT_TO_SQL_JUDGE_TEMPLATE = \"\"\"\\\nYou are an expert in SQL with deep knowledge of relational modeling, query semantics,\nand performance tuning across common dialects (e.g., PostgreSQL, MySQL, SQLite, SQL Server).\nYou think critically about correctness, readability, and efficiency.\n\nUse the SQL Query Quality Rubric below to score the **Generated SQL Query** based on the INSTRUCTIONS.\n\n#### INSTRUCTIONS\nThe Generated SQL Query should be a valid response to the Natural Language Prompt below\n\nNatural Language Prompt:\n{{ sql_prompt }}\n\nDatabase Context:\n{{ sql_context }}\n\nGenerated SQL Query\n{{ sql }}\n\"\"\"\n\n\nsql_scoring = [\n    Score(\n        name=\"Relevance\",\n        description=\"Adherence to INSTRUCTIONS and CONTEXT\",\n        options={\n            4: \"Perfectly meets all specified requirements.\",\n            3: \"Meets most requirements with minor deviations.\",\n            2: \"Moderate deviation from the instructions.\",\n            1: \"Significant deviations from the instructions.\",\n            0: \"Does not adhere to the instructions.\",\n        },\n    ),\n    Score(\n        name=\"SQL Correctness\",\n        description=\"Syntax and semantic correctness; returns the intended result\",\n        options={\n            4: \"Valid SQL with correct joins, filters, grouping/aggregation, and NULL handling; produces the intended result set under the stated/implicit dialect.\",\n            3: \"Generally correct with minor issues (e.g., edge-case NULLs, minor grouping detail) but still likely yields the intended result.\",\n            2: \"Partially correct; noticeable semantic mistakes (joins, grouping, filters) that may change results or fail in edge cases.\",\n            1: \"Largely incorrect; major semantic or syntactic errors likely causing failure or wrong results.\",\n            0: \"Invalid SQL or unrelated to the task; will not run or cannot produce a meaningful result.\",\n        },\n    ),\n    Score(\n        name=\"Readability\",\n        description=\"Formatting, clarity, and maintainability\",\n        options={\n            4: \"Cleanly formatted (keywords/clauses consistently styled), clear structure (CTEs/subqueries where helpful), meaningful table/column aliases, and concise.\",\n            3: \"Generally readable with consistent formatting and understandable aliases; could be organized slightly better.\",\n            2: \"Somewhat readable but inconsistent formatting or confusing aliasing; structure is harder to follow.\",\n            1: \"Poorly formatted and hard to read; unclear structure and aliasing.\",\n            0: \"Unreadable or chaotic; no meaningful structure or styling.\",\n        },\n    ),\n    Score(\n        name=\"Efficiency\",\n        description=\"Query performance best practices\",\n        options={\n            4: \"Uses sargable predicates, appropriate joins, selective filters early, avoids SELECT *, unnecessary DISTINCT, and wasteful subqueries; likely to use indexes effectively.\",\n            3: \"Mostly efficient; minor opportunities for improvement (e.g., simplifying expressions, reducing data early).\",\n            2: \"Moderate inefficiencies (e.g., non-sargable filters, unnecessary nested subqueries, broad SELECT *).\",\n            1: \"Notably inefficient patterns likely causing large scans or poor plans.\",\n            0: \"Highly inefficient; ignores basic best practices and likely to perform very poorly.\",\n        },\n    ),\n]\n\nif __name__ == \"__main__\":\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser()\n    parser.add_argument(\"--model-alias\", type=str, default=\"openai-text\")\n    parser.add_argument(\"--num-records\", type=int, default=5)\n    parser.add_argument(\"--artifact-path\", type=str, default=None)\n    args = parser.parse_args()\n\n    config_builder = build_config(model_alias=args.model_alias)\n    results = create_dataset(config_builder, num_records=args.num_records, artifact_path=args.artifact_path)\n\n    print(f\"Dataset saved to: {results.artifact_storage.final_dataset_path}\")\n\n    results.load_analysis().to_report()\n</code></pre>"},{"location":"recipes/qa_and_chat/multi_turn_chat/","title":"Multi-Turn Chat","text":"<p>Download Code </p> <pre><code>from pathlib import Path\nfrom typing import Literal\n\nfrom pydantic import BaseModel, Field\n\nfrom data_designer.essentials import (\n    CategorySamplerParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    LLMJudgeColumnConfig,\n    LLMStructuredColumnConfig,\n    LLMTextColumnConfig,\n    SamplerColumnConfig,\n    SamplerType,\n    Score,\n    SubcategorySamplerParams,\n)\nfrom data_designer.interface.results import DatasetCreationResults\n\n\ndef build_config(model_alias: str) -&gt; DataDesignerConfigBuilder:\n    config_builder = DataDesignerConfigBuilder()\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"domain\",\n            sampler_type=SamplerType.CATEGORY,\n            params=CategorySamplerParams(values=[\"Tech Support\", \"Personal Finances\", \"Educational Guidance\"]),\n        )\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"topic\",\n            sampler_type=SamplerType.SUBCATEGORY,\n            params=SubcategorySamplerParams(\n                category=\"domain\",\n                values={\n                    \"Tech Support\": [\n                        \"Troubleshooting a Laptop\",\n                        \"Setting Up a Home Wi-Fi Network\",\n                        \"Installing Software Updates\",\n                    ],\n                    \"Personal Finances\": [\n                        \"Budgeting Advice\",\n                        \"Understanding Taxes\",\n                        \"Investment Strategies\",\n                    ],\n                    \"Educational Guidance\": [\n                        \"Choosing a College Major\",\n                        \"Effective Studying Techniques\",\n                        \"Learning a New Language\",\n                    ],\n                },\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"complexity\",\n            sampler_type=SamplerType.CATEGORY,\n            params=CategorySamplerParams(values=[\"Basic\", \"Intermediate\", \"Advanced\"]),\n        )\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"conversation_length\",\n            sampler_type=SamplerType.CATEGORY,\n            params=CategorySamplerParams(values=[2, 4, 6, 8]),\n        )\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"user_mood\",\n            sampler_type=SamplerType.CATEGORY,\n            params=CategorySamplerParams(values=[\"happy\", \"silly\", \"sarcastic\", \"combative\", \"disappointed\", \"toxic\"]),\n        )\n    )\n\n    config_builder.add_column(\n        LLMTextColumnConfig(\n            name=\"assistant_system_prompt\",\n            prompt=(\n                \"Write a reasonable system prompt for a helpful AI assistant with expertise in \"\n                \"{{domain}} and {{topic}}. The AI assistant must not engage in harmful behaviors.\"\n            ),\n            model_alias=model_alias,\n        )\n    )\n\n    config_builder.add_column(\n        LLMTextColumnConfig(\n            name=\"user_task\",\n            prompt=\"Define a simple task related to {{topic}} of {{complexity}} complexity for the user.\",\n            model_alias=model_alias,\n        )\n    )\n\n    config_builder.add_column(\n        LLMStructuredColumnConfig(\n            name=\"conversation\",\n            prompt=(\n                \"&lt;task&gt;\\n{{user_task}}\\n&lt;/task&gt;\\n\\n\"\n                \"&lt;system_prompt&gt;{{assistant_system_prompt}}&lt;/system_prompt&gt;\\n\\n\"\n                \"Generate a conversation between a user and an AI assistant with &lt;system_prompt&gt; about &lt;task&gt;.\\n\"\n                \"User is asking the assistant for advice and is in a {{user_mood}} mood.\\n\"\n                \"The conversation must be {{conversation_length}} messages in length.\\n\"\n                \"The conversation must come to a natural end in {{conversation_length}} messages and if the assistant is unable \"\n                \"to solve the user's needs by then, they should offer to continue the conversation later or redirect them to additional resources.\\n\"\n                \"The conversation must be realistic and natural, incorporating details from both the user's and assistant's profiles.\\n\"\n                \"The AI assistant role has the system prompt defined in &lt;system_prompt&gt;. All of its responses must conform to &lt;system_prompt&gt;:\\n\"\n            ),\n            output_format=ChatConversation,\n            model_alias=model_alias,\n        )\n    )\n\n    config_builder.add_column(\n        LLMJudgeColumnConfig(\n            name=\"toxicity_evaluation\",\n            prompt=TOXICITY_PROMPT_TEMPLATE,\n            scores=[toxicity_score],\n            model_alias=model_alias,\n        )\n    )\n\n    return config_builder\n\n\ndef create_dataset(\n    config_builder: DataDesignerConfigBuilder,\n    num_records: int,\n    artifact_path: Path | str | None = None,\n) -&gt; DatasetCreationResults:\n    data_designer = DataDesigner(artifact_path=artifact_path)\n    results = data_designer.create(config_builder, num_records=num_records)\n    return results\n\n\ntoxicity_score = Score(\n    name=\"Toxicity\",\n    description=\"Evaluation of harmful, harassing, or hateful language in conversations\",\n    options={\n        \"4\": \"No toxicity detected; conversation is respectful and appropriate.\",\n        \"3\": \"Mild toxicity; slightly rude or sarcastic but not hateful or harmful.\",\n        \"2\": \"Moderate toxicity; contains disrespectful or harassing language.\",\n        \"1\": \"Severe toxicity; contains overt hate, harassment, or harmful content.\",\n        \"0\": \"Extreme toxicity; contains dangerous, threatening, or abusive content.\",\n    },\n)\n\nTOXICITY_PROMPT_TEMPLATE = (\n    \"You are an expert in content moderation and digital communication ethics. \"\n    \"You are evaluating the toxicity level in user messages.\\n\"\n    \"Take a deep breath and carefully analyze the conversation provided. \\n\"\n    \"Use the rubric to score the level of toxicity in the USER'S messages only.\\n\"\n    \"Consider:\\n\"\n    \"- Focus primarily on harassment, hate, or harmful language\\n\"\n    \"- Evaluate the full context of the conversation\\n\"\n    \"- Be objective and consistent in your assessment\\n\"\n    \"- If any single message exhibits toxicity, it should influence the final score\\n\"\n    \"## CONVERSATION\\n\"\n    \"{{ conversation }}\"\n)\n\n\nclass Message(BaseModel):\n    \"\"\"A single message turn in the conversation.\"\"\"\n\n    role: Literal[\"user\", \"assistant\"] = Field(..., description=\"Which role is writing the message.\")\n    content: str = Field(..., description=\"Message contents.\")\n\n\nclass ChatConversation(BaseModel):\n    \"\"\"A chat conversation between a specific user and an AI assistant.\n    * All conversations are initiated by the user role.\n    * The assistant role always responds to the user message.\n    * Turns alternate between user and assistant roles.\n    * The last message is always from the assistant role.\n    * Message content can be long or short.\n    * All assistant messages are faithful responses and must be answered fully.\n    \"\"\"\n\n    conversation: list[Message] = Field(..., description=\"List of all messages in the conversation.\")\n\n\nif __name__ == \"__main__\":\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser()\n    parser.add_argument(\"--model-alias\", type=str, default=\"openai-text\")\n    parser.add_argument(\"--num-records\", type=int, default=5)\n    parser.add_argument(\"--artifact-path\", type=str, default=None)\n    args = parser.parse_args()\n\n    config_builder = build_config(model_alias=args.model_alias)\n    results = create_dataset(config_builder, num_records=args.num_records, artifact_path=args.artifact_path)\n\n    print(f\"Dataset saved to: {results.artifact_storage.final_dataset_path}\")\n\n    results.load_analysis().to_report()\n</code></pre>"},{"location":"recipes/qa_and_chat/product_info_qa/","title":"Product Info QA","text":"<p>Download Code </p> <pre><code>import string\nfrom pathlib import Path\n\nfrom pydantic import BaseModel, Field\n\nfrom data_designer.essentials import (\n    BernoulliSamplerParams,\n    CategorySamplerParams,\n    DataDesigner,\n    DataDesignerConfigBuilder,\n    ExpressionColumnConfig,\n    LLMJudgeColumnConfig,\n    LLMStructuredColumnConfig,\n    LLMTextColumnConfig,\n    SamplerColumnConfig,\n    SamplerType,\n    Score,\n    UniformSamplerParams,\n)\nfrom data_designer.interface.results import DatasetCreationResults\n\n\ndef build_config(model_alias: str) -&gt; DataDesignerConfigBuilder:\n    config_builder = DataDesignerConfigBuilder()\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"category\",\n            sampler_type=SamplerType.CATEGORY,\n            params=CategorySamplerParams(\n                values=[\n                    \"Electronics\",\n                    \"Clothing\",\n                    \"Home Appliances\",\n                    \"Groceries\",\n                    \"Toiletries\",\n                    \"Sports Equipment\",\n                    \"Toys\",\n                    \"Books\",\n                    \"Pet Supplies\",\n                    \"Tools &amp; Home Improvement\",\n                    \"Beauty\",\n                    \"Health &amp; Wellness\",\n                    \"Outdoor Gear\",\n                    \"Automotive\",\n                    \"Jewelry\",\n                    \"Watches\",\n                    \"Office Supplies\",\n                    \"Gifts\",\n                    \"Arts &amp; Crafts\",\n                    \"Baby &amp; Kids\",\n                    \"Music\",\n                    \"Video Games\",\n                    \"Movies\",\n                    \"Software\",\n                    \"Tech Devices\",\n                ]\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"price_tens_of_dollars\",\n            sampler_type=SamplerType.UNIFORM,\n            params=UniformSamplerParams(low=1, high=200),\n        )\n    )\n\n    config_builder.add_column(\n        ExpressionColumnConfig(\n            name=\"product_price\",\n            expr=\"{{ (price_tens_of_dollars * 10) - 0.01 | round(2) }}\",\n            dtype=\"float\",\n        )\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"first_letter\",\n            sampler_type=SamplerType.CATEGORY,\n            params=CategorySamplerParams(values=list(string.ascii_uppercase)),\n        )\n    )\n\n    config_builder.add_column(\n        SamplerColumnConfig(\n            name=\"is_hallucination\",\n            sampler_type=SamplerType.BERNOULLI,\n            params=BernoulliSamplerParams(p=0.5),\n        )\n    )\n\n    config_builder.add_column(\n        LLMStructuredColumnConfig(\n            name=\"product_info\",\n            model_alias=model_alias,\n            prompt=(\n                \"Generate a realistic product description for a product in the {{ category }} \"\n                \"category that costs {{ product_price }}.\\n\"\n                \"The name of the product MUST start with the letter {{ first_letter }}.\\n\"\n            ),\n            output_format=ProductInfo,\n        )\n    )\n\n    config_builder.add_column(\n        LLMTextColumnConfig(\n            name=\"question\",\n            model_alias=model_alias,\n            prompt=(\"Ask a question about the following product:\\n\\n {{ product_info }}\"),\n        )\n    )\n\n    config_builder.add_column(\n        LLMTextColumnConfig(\n            name=\"answer\",\n            model_alias=model_alias,\n            prompt=(\n                \"{%- if is_hallucination == 0 -%}\\n\"\n                \"&lt;product_info&gt;\\n\"\n                \"{{ product_info }}\\n\"\n                \"&lt;/product_info&gt;\\n\"\n                \"{%- endif -%}\\n\"\n                \"User Question: {{ question }}\\n\"\n                \"Directly and succinctly answer the user's question.\\n\"\n                \"{%- if is_hallucination == 1 -%}\\n\"\n                \"Make up whatever information you need to in order to answer the user's request.\\n\"\n                \"{%- endif -%}\"\n            ),\n        )\n    )\n\n    # Evaluate answer quality\n    config_builder.add_column(\n        LLMJudgeColumnConfig(\n            name=\"llm_answer_metrics\",\n            model_alias=model_alias,\n            prompt=(\n                \"&lt;product_info&gt;\\n\"\n                \"{{ product_info }}\\n\"\n                \"&lt;/product_info&gt;\\n\"\n                \"User Question: {{question }}\\n\"\n                \"AI Assistant Answer: {{ answer }}\\n\"\n                \"Judge the AI assistant's response to the user's question about the product described in &lt;product_info&gt;.\"\n            ),\n            scores=answer_quality_scores,\n        )\n    )\n\n    config_builder.add_column(\n        ExpressionColumnConfig(\n            name=\"completeness_result\",\n            expr=\"{{ llm_answer_metrics.completeness.score }}\",\n        )\n    )\n\n    config_builder.add_column(\n        ExpressionColumnConfig(\n            name=\"accuracy_result\",\n            expr=\"{{ llm_answer_metrics.accuracy.score }}\",\n        )\n    )\n\n    return config_builder\n\n\ndef create_dataset(\n    config_builder: DataDesignerConfigBuilder,\n    num_records: int,\n    artifact_path: Path | str | None = None,\n) -&gt; DatasetCreationResults:\n    data_designer = DataDesigner(artifact_path=artifact_path)\n    results = data_designer.create(config_builder, num_records=num_records)\n    return results\n\n\nclass ProductInfo(BaseModel):\n    product_name: str = Field(..., description=\"A realistic product name for the market.\")\n    key_features: list[str] = Field(..., min_length=1, max_length=3, description=\"Key product features.\")\n    description: str = Field(\n        ...,\n        description=\"A short, engaging description of what the product does, highlighting a unique but believable feature.\",\n    )\n    price_usd: float = Field(..., description=\"The price of the product\", ge=10, le=1000, decimal_places=2)\n\n\ncompleteness_score = Score(\n    name=\"Completeness\",\n    description=\"Evaluation of AI assistant's thoroughness in addressing all aspects of the user's query.\",\n    options={\n        \"Complete\": \"The response thoroughly covers all key points requested in the question, providing sufficient detail to satisfy the user's information needs.\",\n        \"PartiallyComplete\": \"The response addresses the core question but omits certain important details or fails to elaborate on relevant aspects that were requested.\",\n        \"Incomplete\": \"The response significantly lacks necessary information, missing major components of what was asked and leaving the query largely unanswered.\",\n    },\n)\n\naccuracy_score = Score(\n    name=\"Accuracy\",\n    description=\"Evaluation of how factually correct the AI assistant's response is relative to the product information.\",\n    options={\n        \"Accurate\": \"The information provided aligns perfectly with the product specifications without introducing any misleading or incorrect details.\",\n        \"PartiallyAccurate\": \"While some information is correctly stated, the response contains minor factual errors or potentially misleading statements about the product.\",\n        \"Inaccurate\": \"The response presents significantly wrong information about the product, with claims that contradict the actual product details.\",\n    },\n)\n\nanswer_quality_scores = [completeness_score, accuracy_score]\n\n\nif __name__ == \"__main__\":\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser()\n    parser.add_argument(\"--model-alias\", type=str, default=\"openai-text\")\n    parser.add_argument(\"--num-records\", type=int, default=5)\n    parser.add_argument(\"--artifact-path\", type=str, default=None)\n    args = parser.parse_args()\n\n    config_builder = build_config(model_alias=args.model_alias)\n    results = create_dataset(config_builder, num_records=args.num_records, artifact_path=args.artifact_path)\n\n    print(f\"Dataset saved to: {results.artifact_storage.final_dataset_path}\")\n\n    results.load_analysis().to_report()\n</code></pre>"},{"location":"scripts/generate_colab_notebooks/","title":"Generate colab notebooks","text":"In\u00a0[\u00a0]: Copied! <pre># SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"Script to generate Colab-compatible notebooks from notebook source files.\n\nThis script processes jupytext percent-format Python files and:\n1. Injects Colab-specific setup cells (pip install, API key from secrets)\n2. Injects cells before the \"Import the essentials\" section\n3. Saves the result as .ipynb files in docs/colab_notebooks\n\"\"\"\n</pre> # SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. # SPDX-License-Identifier: Apache-2.0 \"\"\"Script to generate Colab-compatible notebooks from notebook source files.  This script processes jupytext percent-format Python files and: 1. Injects Colab-specific setup cells (pip install, API key from secrets) 2. Injects cells before the \"Import the essentials\" section 3. Saves the result as .ipynb files in docs/colab_notebooks \"\"\" In\u00a0[\u00a0]: Copied! <pre>from __future__ import annotations\n</pre> from __future__ import annotations In\u00a0[\u00a0]: Copied! <pre>import argparse\nfrom pathlib import Path\n</pre> import argparse from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import jupytext\nfrom nbformat import NotebookNode\nfrom nbformat.v4 import new_code_cell, new_markdown_cell\n</pre> import jupytext from nbformat import NotebookNode from nbformat.v4 import new_code_cell, new_markdown_cell In\u00a0[\u00a0]: Copied! <pre>COLAB_SETUP_MARKDOWN = \"\"\"\\\n### \u26a1 Colab Setup\n\nRun the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from [build.nvidia.com](https://build.nvidia.com).\n\"\"\"\n</pre> COLAB_SETUP_MARKDOWN = \"\"\"\\ ### \u26a1 Colab Setup  Run the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from [build.nvidia.com](https://build.nvidia.com). \"\"\" In\u00a0[\u00a0]: Copied! <pre>ADDITIONAL_DEPENDENCIES = {\n    \"4-providing-images-as-context.py\": \"pillow&gt;=12.0.0\",\n}\n</pre> ADDITIONAL_DEPENDENCIES = {     \"4-providing-images-as-context.py\": \"pillow&gt;=12.0.0\", } In\u00a0[\u00a0]: Copied! <pre>COLAB_INSTALL_CELL = \"\"\"\\\n%%capture\n!pip install -U data-designer\"\"\"\n</pre> COLAB_INSTALL_CELL = \"\"\"\\ %%capture !pip install -U data-designer\"\"\" In\u00a0[\u00a0]: Copied! <pre>COLAB_API_KEY_CELL = \"\"\"\\\nimport getpass\nimport os\n\nfrom google.colab import userdata\n\ntry:\n    os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\nexcept userdata.SecretNotFoundError:\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")\"\"\"\n</pre> COLAB_API_KEY_CELL = \"\"\"\\ import getpass import os  from google.colab import userdata  try:     os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\") except userdata.SecretNotFoundError:     os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")\"\"\" In\u00a0[\u00a0]: Copied! <pre>def create_colab_setup_cells(additional_dependencies: str) -&gt; list[NotebookNode]:\n    \"\"\"Create the Colab-specific setup cells to inject before imports.\"\"\"\n    cells = []\n    cells += [new_markdown_cell(source=COLAB_SETUP_MARKDOWN)]\n\n    install_cell = COLAB_INSTALL_CELL\n    if additional_dependencies:\n        install_cell += f\" {additional_dependencies}\"\n    cells += [new_code_cell(source=install_cell)]\n\n    cells += [new_code_cell(source=COLAB_API_KEY_CELL)]\n    return cells\n</pre> def create_colab_setup_cells(additional_dependencies: str) -&gt; list[NotebookNode]:     \"\"\"Create the Colab-specific setup cells to inject before imports.\"\"\"     cells = []     cells += [new_markdown_cell(source=COLAB_SETUP_MARKDOWN)]      install_cell = COLAB_INSTALL_CELL     if additional_dependencies:         install_cell += f\" {additional_dependencies}\"     cells += [new_code_cell(source=install_cell)]      cells += [new_code_cell(source=COLAB_API_KEY_CELL)]     return cells In\u00a0[\u00a0]: Copied! <pre>def find_import_section_index(cells: list[NotebookNode]) -&gt; int:\n    \"\"\"Find the index of the 'Import the essentials' markdown cell.\"\"\"\n    first_code_cell_index = -1\n    for i, cell in enumerate(cells):\n        if first_code_cell_index == -1 and cell.get(\"cell_type\") == \"code\":\n            first_code_cell_index = i\n\n        if cell.get(\"cell_type\") == \"markdown\":\n            source = cell.get(\"source\", \"\")\n            if \"import\" in source.lower() and \"essentials\" in source.lower():\n                return i\n    return first_code_cell_index\n</pre> def find_import_section_index(cells: list[NotebookNode]) -&gt; int:     \"\"\"Find the index of the 'Import the essentials' markdown cell.\"\"\"     first_code_cell_index = -1     for i, cell in enumerate(cells):         if first_code_cell_index == -1 and cell.get(\"cell_type\") == \"code\":             first_code_cell_index = i          if cell.get(\"cell_type\") == \"markdown\":             source = cell.get(\"source\", \"\")             if \"import\" in source.lower() and \"essentials\" in source.lower():                 return i     return first_code_cell_index In\u00a0[\u00a0]: Copied! <pre>def process_notebook(notebook: NotebookNode, source_path: Path) -&gt; NotebookNode:\n    \"\"\"Process a notebook to make it Colab-compatible.\n\n    Args:\n        notebook: The input notebook\n\n    Returns:\n        The processed notebook with Colab setup cells injected\n    \"\"\"\n    cells = notebook.cells\n\n    additional_dependencies = ADDITIONAL_DEPENDENCIES.get(source_path.name, \"\")\n\n    # Find where to insert Colab setup (before \"Import the essentials\")\n    import_idx = find_import_section_index(cells)\n\n    if import_idx == -1:\n        # If not found, insert after first cell (title)\n        import_idx = 1\n\n    # Insert Colab setup cells before the import section\n    colab_cells = create_colab_setup_cells(additional_dependencies)\n    processed_cells = cells[:import_idx] + colab_cells + cells[import_idx:]\n\n    notebook.cells = processed_cells\n    return notebook\n</pre> def process_notebook(notebook: NotebookNode, source_path: Path) -&gt; NotebookNode:     \"\"\"Process a notebook to make it Colab-compatible.      Args:         notebook: The input notebook      Returns:         The processed notebook with Colab setup cells injected     \"\"\"     cells = notebook.cells      additional_dependencies = ADDITIONAL_DEPENDENCIES.get(source_path.name, \"\")      # Find where to insert Colab setup (before \"Import the essentials\")     import_idx = find_import_section_index(cells)      if import_idx == -1:         # If not found, insert after first cell (title)         import_idx = 1      # Insert Colab setup cells before the import section     colab_cells = create_colab_setup_cells(additional_dependencies)     processed_cells = cells[:import_idx] + colab_cells + cells[import_idx:]      notebook.cells = processed_cells     return notebook In\u00a0[\u00a0]: Copied! <pre>def generate_colab_notebook(source_path: Path, output_dir: Path) -&gt; Path:\n    \"\"\"Generate a Colab-compatible notebook from a source file.\n\n    Args:\n        source_path: Path to the jupytext percent-format Python source file\n        output_dir: Directory to save the output notebook\n\n    Returns:\n        Path to the generated notebook\n    \"\"\"\n    # Read the source file using jupytext\n    notebook = jupytext.read(source_path)\n\n    # Process the notebook for Colab\n    notebook = process_notebook(notebook, source_path)\n\n    # Determine output path\n    output_path = output_dir / f\"{source_path.stem}.ipynb\"\n\n    # Ensure output directory exists\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Write the notebook\n    jupytext.write(notebook, output_path, config={\"metadata\": {\"jupytext\": {\"cell_metadata_filter\": \"-id\"}}})\n\n    return output_path\n</pre> def generate_colab_notebook(source_path: Path, output_dir: Path) -&gt; Path:     \"\"\"Generate a Colab-compatible notebook from a source file.      Args:         source_path: Path to the jupytext percent-format Python source file         output_dir: Directory to save the output notebook      Returns:         Path to the generated notebook     \"\"\"     # Read the source file using jupytext     notebook = jupytext.read(source_path)      # Process the notebook for Colab     notebook = process_notebook(notebook, source_path)      # Determine output path     output_path = output_dir / f\"{source_path.stem}.ipynb\"      # Ensure output directory exists     output_dir.mkdir(parents=True, exist_ok=True)      # Write the notebook     jupytext.write(notebook, output_path, config={\"metadata\": {\"jupytext\": {\"cell_metadata_filter\": \"-id\"}}})      return output_path In\u00a0[\u00a0]: Copied! <pre>def main() -&gt; None:\n    \"\"\"Main entry point for the script.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Generate Colab-compatible notebooks from notebook source files.\")\n    parser.add_argument(\n        \"--source-dir\",\n        type=Path,\n        default=Path(\"docs/notebook_source\"),\n        help=\"Directory containing notebook source files (default: docs/notebook_source)\",\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        type=Path,\n        default=Path(\"docs/colab_notebooks\"),\n        help=\"Directory to save Colab notebooks (default: docs/colab_notebooks)\",\n    )\n    parser.add_argument(\n        \"--files\",\n        nargs=\"*\",\n        help=\"Specific files to process (if not specified, process all .py files)\",\n    )\n\n    args = parser.parse_args()\n\n    # Get list of source files\n    if args.files:\n        source_files = [args.source_dir / f for f in args.files]\n    else:\n        source_files = sorted(args.source_dir.glob(\"*.py\"))\n        # Filter out files starting with underscore (like _README.md, _pyproject.toml)\n        source_files = [f for f in source_files if not f.name.startswith(\"_\")]\n\n    if not source_files:\n        print(f\"No source files found in {args.source_dir}\")\n        return\n\n    print(f\"\ud83d\udcd3 Generating Colab notebooks from {len(source_files)} source file(s)...\")\n    print(f\"   Source: {args.source_dir}\")\n    print(f\"   Output: {args.output_dir}\")\n    print()\n\n    for source_path in source_files:\n        if not source_path.exists():\n            print(f\"\u26a0\ufe0f  Skipping {source_path} (file not found)\")\n            continue\n\n        try:\n            output_path = generate_colab_notebook(source_path, args.output_dir)\n            print(f\"\u2705 {source_path.name} \u2192 {output_path.name}\")\n        except Exception as e:\n            print(f\"\u274c {source_path.name}: {e}\")\n\n    print()\n    print(f\"\u2728 Colab notebooks saved to {args.output_dir}/\")\n</pre> def main() -&gt; None:     \"\"\"Main entry point for the script.\"\"\"     parser = argparse.ArgumentParser(description=\"Generate Colab-compatible notebooks from notebook source files.\")     parser.add_argument(         \"--source-dir\",         type=Path,         default=Path(\"docs/notebook_source\"),         help=\"Directory containing notebook source files (default: docs/notebook_source)\",     )     parser.add_argument(         \"--output-dir\",         type=Path,         default=Path(\"docs/colab_notebooks\"),         help=\"Directory to save Colab notebooks (default: docs/colab_notebooks)\",     )     parser.add_argument(         \"--files\",         nargs=\"*\",         help=\"Specific files to process (if not specified, process all .py files)\",     )      args = parser.parse_args()      # Get list of source files     if args.files:         source_files = [args.source_dir / f for f in args.files]     else:         source_files = sorted(args.source_dir.glob(\"*.py\"))         # Filter out files starting with underscore (like _README.md, _pyproject.toml)         source_files = [f for f in source_files if not f.name.startswith(\"_\")]      if not source_files:         print(f\"No source files found in {args.source_dir}\")         return      print(f\"\ud83d\udcd3 Generating Colab notebooks from {len(source_files)} source file(s)...\")     print(f\"   Source: {args.source_dir}\")     print(f\"   Output: {args.output_dir}\")     print()      for source_path in source_files:         if not source_path.exists():             print(f\"\u26a0\ufe0f  Skipping {source_path} (file not found)\")             continue          try:             output_path = generate_colab_notebook(source_path, args.output_dir)             print(f\"\u2705 {source_path.name} \u2192 {output_path.name}\")         except Exception as e:             print(f\"\u274c {source_path.name}: {e}\")      print()     print(f\"\u2728 Colab notebooks saved to {args.output_dir}/\") In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"}]}