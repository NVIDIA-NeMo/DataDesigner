{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7bc100e",
   "metadata": {},
   "source": [
    "# üé® Data Designer Tutorial: Image-to-Image Editing\n",
    "\n",
    "#### üìö What you'll learn\n",
    "\n",
    "This notebook shows how to edit existing images by combining a seed dataset with image generation. You'll load animal portrait photographs from HuggingFace, feed them as context to an autoregressive model, and generate fun edited versions with accessories like sunglasses, top hats, and bow ties.\n",
    "\n",
    "- üå± **Seed datasets with images**: Load a HuggingFace image dataset and use it as a seed\n",
    "- üñºÔ∏è **Image context for editing**: Pass existing images to an image-generation model via `multi_modal_context`\n",
    "- üé≤ **Sampler-driven diversity**: Combine sampled accessories and settings with seed images for varied results\n",
    "- üíæ **Preview vs create**: Preview stores base64 in the dataframe; create saves images to disk\n",
    "\n",
    "This tutorial uses an **autoregressive** model (one that supports both image input *and* image output via the chat completions API). Diffusion models (DALL¬∑E, Stable Diffusion, etc.) do not support image context‚Äîsee [Tutorial 5](https://nvidia-nemo.github.io/DataDesigner/latest/notebooks/5-generating-images/) for text-to-image generation with diffusion models.\n",
    "\n",
    "If this is your first time using Data Designer, we recommend starting with the [first notebook](https://nvidia-nemo.github.io/DataDesigner/latest/notebooks/1-the-basics/) in this tutorial series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d134dd",
   "metadata": {},
   "source": [
    "### üì¶ Import Data Designer\n",
    "\n",
    "- `data_designer.config` provides the configuration API.\n",
    "- `DataDesigner` is the main interface for generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec66ab",
   "metadata": {},
   "source": [
    "### ‚ö° Colab Setup\n",
    "\n",
    "Run the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from [build.nvidia.com](https://build.nvidia.com).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba921281",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U data-designer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a3272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\n",
    "except userdata.SecretNotFoundError:\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a2d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import uuid\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from IPython.display import Image as IPImage\n",
    "from IPython.display import display\n",
    "\n",
    "import data_designer.config as dd\n",
    "from data_designer.interface import DataDesigner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf64783d",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Initialize the Data Designer interface\n",
    "\n",
    "When initialized without arguments, [default model providers](https://nvidia-nemo.github.io/DataDesigner/latest/concepts/models/default-model-settings/) are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_designer = DataDesigner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea842b",
   "metadata": {},
   "source": [
    "### üéõÔ∏è Define an image-editing model\n",
    "\n",
    "We need an **autoregressive** model that supports both image input and image output via the chat completions API. This lets us pass existing images as context and receive edited images back.\n",
    "\n",
    "- Use `ImageInferenceParams` so Data Designer treats this model as an image generator.\n",
    "- Image-specific options are model-dependent; pass them via `extra_body`.\n",
    "\n",
    "> **Note**: This tutorial uses the Flux 2 Pro model via [OpenRouter](https://openrouter.ai). Set `OPENROUTER_API_KEY` in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f33076",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PROVIDER = \"openrouter\"\n",
    "MODEL_ID = \"black-forest-labs/flux.2-pro\"\n",
    "MODEL_ALIAS = \"image-editor\"\n",
    "\n",
    "model_configs = [\n",
    "    dd.ModelConfig(\n",
    "        alias=MODEL_ALIAS,\n",
    "        model=MODEL_ID,\n",
    "        provider=MODEL_PROVIDER,\n",
    "        inference_parameters=dd.ImageInferenceParams(\n",
    "            extra_body={\"height\": 512, \"width\": 512},\n",
    "        ),\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4012e12",
   "metadata": {},
   "source": [
    "### üå± Load animal portraits from HuggingFace\n",
    "\n",
    "We'll load animal face photographs from the [AFHQ](https://huggingface.co/datasets/huggan/AFHQv2) (Animal Faces-HQ) dataset, convert them to base64, and use them as a seed dataset.\n",
    "\n",
    "AFHQ contains high-quality 512√ó512 close-up portraits of cats, dogs, and wildlife‚Äîperfect subjects for adding fun accessories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c090a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_COUNT = 10\n",
    "BASE64_IMAGE_HEIGHT = 512\n",
    "\n",
    "ANIMAL_LABELS = {0: \"cat\", 1: \"dog\", 2: \"wild\"}\n",
    "\n",
    "\n",
    "def resize_image(image, height: int):\n",
    "    \"\"\"Resize image maintaining aspect ratio.\"\"\"\n",
    "    original_width, original_height = image.size\n",
    "    width = int(original_width * (height / original_height))\n",
    "    return image.resize((width, height))\n",
    "\n",
    "\n",
    "def prepare_record(record: dict, height: int) -> dict:\n",
    "    \"\"\"Convert a HuggingFace record to base64 with metadata.\"\"\"\n",
    "    image = resize_image(record[\"image\"], height)\n",
    "    img_buffer = io.BytesIO()\n",
    "    image.save(img_buffer, format=\"PNG\")\n",
    "    base64_string = base64.b64encode(img_buffer.getvalue()).decode(\"utf-8\")\n",
    "    return {\n",
    "        \"uuid\": str(uuid.uuid4()),\n",
    "        \"base64_image\": base64_string,\n",
    "        \"animal\": ANIMAL_LABELS[record[\"label\"]],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Streaming animal portraits from HuggingFace...\")\n",
    "hf_dataset = load_dataset(\"huggan/AFHQv2\", split=\"train\", streaming=True)\n",
    "\n",
    "hf_iter = iter(hf_dataset)\n",
    "records = [prepare_record(next(hf_iter), BASE64_IMAGE_HEIGHT) for _ in range(SEED_COUNT)]\n",
    "df_seed = pd.DataFrame(records)\n",
    "\n",
    "print(f\"‚úÖ Prepared {len(df_seed)} animal portraits with columns: {list(df_seed.columns)}\")\n",
    "df_seed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4bb935",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Build the configuration\n",
    "\n",
    "We combine three ingredients:\n",
    "\n",
    "1. **Seed dataset** ‚Äî original animal portraits as base64 and their species labels\n",
    "2. **Sampler columns** ‚Äî randomly sample accessories and settings for each image\n",
    "3. **Image column with context** ‚Äî generate an edited image using the original as reference\n",
    "\n",
    "The `multi_modal_context` parameter on `ImageColumnConfig` tells Data Designer to pass the seed image to the model alongside the text prompt. The model receives both the image and the editing instructions, and generates a new image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058315e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n",
    "\n",
    "# 1. Seed the original animal portraits\n",
    "config_builder.with_seed_dataset(dd.DataFrameSeedSource(df=df_seed))\n",
    "\n",
    "# 2. Add sampler columns for accessory diversity\n",
    "config_builder.add_column(\n",
    "    dd.SamplerColumnConfig(\n",
    "        name=\"accessory\",\n",
    "        sampler_type=dd.SamplerType.CATEGORY,\n",
    "        params=dd.CategorySamplerParams(\n",
    "            values=[\n",
    "                \"a tiny top hat\",\n",
    "                \"oversized sunglasses\",\n",
    "                \"a red bow tie\",\n",
    "                \"a knitted beanie\",\n",
    "                \"a flower crown\",\n",
    "                \"a monocle and mustache\",\n",
    "                \"a pirate hat and eye patch\",\n",
    "                \"a chef hat\",\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    dd.SamplerColumnConfig(\n",
    "        name=\"setting\",\n",
    "        sampler_type=dd.SamplerType.CATEGORY,\n",
    "        params=dd.CategorySamplerParams(\n",
    "            values=[\n",
    "                \"a cozy living room\",\n",
    "                \"a sunny park\",\n",
    "                \"a photo studio with soft lighting\",\n",
    "                \"a red carpet event\",\n",
    "                \"a holiday card backdrop with snowflakes\",\n",
    "                \"a tropical beach at sunset\",\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    dd.SamplerColumnConfig(\n",
    "        name=\"art_style\",\n",
    "        sampler_type=dd.SamplerType.CATEGORY,\n",
    "        params=dd.CategorySamplerParams(\n",
    "            values=[\n",
    "                \"a photorealistic style\",\n",
    "                \"a Disney Pixar 3D render\",\n",
    "                \"a watercolor painting\",\n",
    "                \"a pop art poster\",\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Image column that reads the seed image as context and generates an edited version\n",
    "config_builder.add_column(\n",
    "    dd.ImageColumnConfig(\n",
    "        name=\"edited_image\",\n",
    "        prompt=(\n",
    "            \"Edit this {{ animal }} portrait photo. \"\n",
    "            \"Add {{ accessory }} on the animal. \"\n",
    "            \"Place the {{ animal }} in {{ setting }}. \"\n",
    "            \"Render the result in {{ art_style }}. \"\n",
    "            \"Keep the animal's face, expression, and features faithful to the original photo.\"\n",
    "        ),\n",
    "        model_alias=MODEL_ALIAS,\n",
    "        multi_modal_context=[\n",
    "            dd.ImageContext(\n",
    "                column_name=\"base64_image\",\n",
    "                data_type=dd.ModalityDataType.BASE64,\n",
    "                image_format=dd.ImageFormat.PNG,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "data_designer.validate(config_builder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4e2c3",
   "metadata": {},
   "source": [
    "### üîÅ Preview: quick iteration\n",
    "\n",
    "In **preview** mode, generated images are stored as base64 strings in the dataframe. Use this to iterate on your prompts, accessories, and sampler values before scaling up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d2468",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = data_designer.preview(config_builder, num_records=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1398cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(preview.dataset)):\n",
    "    preview.display_sample_record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7663308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea373c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### üîé Compare original vs edited\n",
    "\n",
    "Let's display the original animal portraits next to their accessorized versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f36623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_before_after(row: pd.Series, index: int, base_path=None) -> None:\n",
    "    \"\"\"Display original vs edited image for a single record.\n",
    "\n",
    "    When base_path is None (preview mode), edited_image is decoded from base64.\n",
    "    When base_path is provided (create mode), edited_image is loaded from disk.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Record {index}: {row['animal']} wearing {row['accessory']}\")\n",
    "    print(f\"Setting: {row['setting']}\")\n",
    "    print(f\"Style: {row['art_style']}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    print(\"\\nüì∑ Original portrait:\")\n",
    "    display(IPImage(data=base64.b64decode(row[\"base64_image\"])))\n",
    "\n",
    "    print(\"\\nüé® Edited version:\")\n",
    "    edited = row.get(\"edited_image\")\n",
    "    if edited is None:\n",
    "        return\n",
    "    if base_path is None:\n",
    "        images = edited if isinstance(edited, list) else [edited]\n",
    "        for img_b64 in images:\n",
    "            display(IPImage(data=base64.b64decode(img_b64)))\n",
    "    else:\n",
    "        paths = edited if not isinstance(edited, str) else [edited]\n",
    "        for path in paths:\n",
    "            display(IPImage(filename=str(base_path / path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994011a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in preview.dataset.iterrows():\n",
    "    display_before_after(row, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760cce8",
   "metadata": {},
   "source": [
    "### üÜô Create at scale\n",
    "\n",
    "In **create** mode, images are saved to disk in an `images/<column_name>/` folder with UUID filenames. The dataframe stores relative paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d151c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = data_designer.create(config_builder, num_records=5, dataset_name=\"tutorial-6-edited-images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = results.load_dataset()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in dataset.head(10).iterrows():\n",
    "    display_before_after(row, index, base_path=results.artifact_storage.base_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12ed83",
   "metadata": {},
   "source": [
    "## ‚è≠Ô∏è Next steps\n",
    "\n",
    "- Experiment with different autoregressive models for image editing\n",
    "- Try more creative editing prompts (style transfer, background replacement, artistic filters)\n",
    "- Combine image editing with text generation (e.g., generate captions for edited images using an LLM-Text column)\n",
    "\n",
    "Related tutorials:\n",
    "\n",
    "- [The basics](https://nvidia-nemo.github.io/DataDesigner/latest/notebooks/1-the-basics/): samplers and LLM text columns\n",
    "- [Providing images as context](https://nvidia-nemo.github.io/DataDesigner/latest/notebooks/4-providing-images-as-context/): image-to-text with VLMs\n",
    "- [Generating images](https://nvidia-nemo.github.io/DataDesigner/latest/notebooks/5-generating-images/): text-to-image generation with diffusion models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
