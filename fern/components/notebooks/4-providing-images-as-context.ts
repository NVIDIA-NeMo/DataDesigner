/** Auto-generated by ipynb-to-fern-json.py - do not edit */
export default { cells: [
  {
    "type": "markdown",
    "source": "# \ud83c\udfa8 Data Designer Tutorial: Providing Images as Context for Vision-Based Data Generation"
  },
  {
    "type": "markdown",
    "source": "#### \ud83d\udcda What you'll learn\n\nThis notebook demonstrates how to provide images as context to generate text descriptions using vision-language models.\n\n- \u2728 **Visual Document Processing**: Converting images to chat-ready format for model consumption\n- \ud83d\udd0d **Vision-Language Generation**: Using vision models to generate detailed summaries from images\n\nIf this is your first time using Data Designer, we recommend starting with the [first notebook](https://nvidia-nemo.github.io/DataDesigner/latest/notebooks/1-the-basics/) in this tutorial series."
  },
  {
    "type": "markdown",
    "source": "### \ud83d\udce6 Import Data Designer\n\n- `data_designer.config` provides access to the configuration API.\n\n- `DataDesigner` is the main interface for data generation."
  },
  {
    "type": "markdown",
    "source": "### \u26a1 Colab Setup\n\nRun the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from [build.nvidia.com](https://build.nvidia.com)."
  },
  {
    "type": "code",
    "source": "%%capture\n!pip install -U data-designer \"pillow>=12.0.0,<13\" \"datasets>=4.0.0,<5\"",
    "language": "python"
  },
  {
    "type": "code",
    "source": "import getpass\nimport os\n\nfrom google.colab import userdata\n\ntry:\n    os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\nexcept userdata.SecretNotFoundError:\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")",
    "language": "python"
  },
  {
    "type": "code",
    "source": "# Standard library imports\nimport base64\nimport io\nimport uuid\n\n# Third-party imports\nimport pandas as pd\nimport rich\nfrom datasets import load_dataset\nfrom IPython.display import display\nfrom rich.panel import Panel\n\n# Data Designer imports\nimport data_designer.config as dd\nfrom data_designer.interface import DataDesigner",
    "language": "python"
  },
  {
    "type": "markdown",
    "source": "### \u2699\ufe0f Initialize the Data Designer interface\n\n- `DataDesigner` is the main object responsible for managing the data generation process.\n\n- When initialized without arguments, the [default model providers](https://nvidia-nemo.github.io/DataDesigner/latest/concepts/models/default-model-settings/) are used."
  },
  {
    "type": "code",
    "source": "data_designer = DataDesigner()",
    "language": "python"
  },
  {
    "type": "markdown",
    "source": "### \ud83c\udf9b\ufe0f Define model configurations\n\n- Each `ModelConfig` defines a model that can be used during the generation process.\n\n- The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).\n\n- The \"model provider\" is the external service that hosts the model (see the [model config](https://nvidia-nemo.github.io/DataDesigner/latest/concepts/models/default-model-settings/) docs for more details).\n\n- By default, we use [build.nvidia.com](https://build.nvidia.com/models) as the model provider."
  },
  {
    "type": "code",
    "source": "# This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\nmodel_configs = [\n    dd.ModelConfig(\n        alias=\"vision\",\n        model=\"meta/llama-4-scout-17b-16e-instruct\",\n        provider=MODEL_PROVIDER,\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=0.60,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n]",
    "language": "python"
  },
  {
    "type": "markdown",
    "source": "### \ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\n\n- The Data Designer config defines the dataset schema and generation process.\n\n- The config builder provides an intuitive interface for building this configuration.\n\n- The list of model configs is provided to the builder at initialization."
  },
  {
    "type": "code",
    "source": "config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)",
    "language": "python"
  },
  {
    "type": "markdown",
    "source": "### \ud83c\udf31 Seed Dataset Creation\n\nIn this section, we'll prepare our visual documents as a seed dataset for summarization:\n\n- **Loading Visual Documents**: We use the ColPali dataset containing document images\n- **Image Processing**: Convert images to base64 format for vision model consumption\n- **Metadata Extraction**: Preserve relevant document information (filename, page number, source, etc.)\n\nThe seed dataset will be used to generate detailed text summaries of each document image."
  },
  {
    "type": "code",
    "source": "# Dataset processing configuration\nIMG_COUNT = 512  # Number of images to process\nBASE64_IMAGE_HEIGHT = 512  # Standardized height for model input\n\n# Load ColPali dataset for visual documents\nimg_dataset_cfg = {\"path\": \"vidore/colpali_train_set\", \"split\": \"train\", \"streaming\": True}",
    "language": "python"
  },
  {
    "type": "code",
    "source": "def resize_image(image, height: int):\n    \"\"\"\n    Resize image while maintaining aspect ratio.\n\n    Args:\n        image: PIL Image object\n        height: Target height in pixels\n\n    Returns:\n        Resized PIL Image object\n    \"\"\"\n    original_width, original_height = image.size\n    width = int(original_width * (height / original_height))\n    return image.resize((width, height))\n\n\ndef convert_image_to_chat_format(record, height: int) -> dict:\n    \"\"\"\n    Convert PIL image to base64 format for chat template usage.\n\n    Args:\n        record: Dataset record containing image and metadata\n        height: Target height for image resizing\n\n    Returns:\n        Updated record with base64_image and uuid fields\n    \"\"\"\n    # Resize image for consistent processing\n    image = resize_image(record[\"image\"], height)\n\n    # Convert to base64 string\n    img_buffer = io.BytesIO()\n    image.save(img_buffer, format=\"PNG\")\n    byte_data = img_buffer.getvalue()\n    base64_encoded_data = base64.b64encode(byte_data)\n    base64_string = base64_encoded_data.decode(\"utf-8\")\n\n    # Return updated record\n    return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())}",
    "language": "python"
  },
  {
    "type": "code",
    "source": "# Load and process the visual document dataset\nprint(\"\ud83d\udce5 Loading and processing document images...\")\n\nimg_dataset_iter = iter(\n    load_dataset(**img_dataset_cfg).map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT})\n)\nimg_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])\n\nprint(f\"\u2705 Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\")",
    "language": "python"
  },
  {
    "type": "code",
    "source": "img_dataset.head()",
    "language": "python"
  },
  {
    "type": "code",
    "source": "# Add the seed dataset containing our processed images\ndf_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]]\nconfig_builder.with_seed_dataset(dd.DataFrameSeedSource(df=df_seed))",
    "language": "python"
  },
  {
    "type": "code",
    "source": "# Add a column to generate detailed document summaries\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"summary\",\n        model_alias=\"vision\",\n        prompt=(\n            \"Provide a detailed summary of the content in this image in Markdown format. \"\n            \"Start from the top of the image and then describe it from top to bottom. \"\n            \"Place a summary at the bottom.\"\n        ),\n        multi_modal_context=[\n            dd.ImageContext(\n                column_name=\"base64_image\",\n                data_type=dd.ModalityDataType.BASE64,\n                image_format=dd.ImageFormat.PNG,\n            )\n        ],\n    )\n)\n\ndata_designer.validate(config_builder)",
    "language": "python"
  },
  {
    "type": "markdown",
    "source": "### \ud83d\udd01 Iteration is key \u2013 preview the dataset!\n\n1. Use the `preview` method to generate a sample of records quickly.\n\n2. Inspect the results for quality and format issues.\n\n3. Adjust column configurations, prompts, or parameters as needed.\n\n4. Re-run the preview until satisfied."
  },
  {
    "type": "code",
    "source": "preview = data_designer.preview(config_builder, num_records=2)",
    "language": "python"
  },
  {
    "type": "code",
    "source": "# Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()",
    "language": "python"
  },
  {
    "type": "code",
    "source": "# The preview dataset is available as a pandas DataFrame.\npreview.dataset",
    "language": "python"
  },
  {
    "type": "markdown",
    "source": "### \ud83d\udcca Analyze the generated data\n\n- Data Designer automatically generates a basic statistical analysis of the generated data.\n\n- This analysis is available via the `analysis` property of generation result objects."
  },
  {
    "type": "code",
    "source": "# Print the analysis as a table.\npreview.analysis.to_report()",
    "language": "python"
  },
  {
    "type": "markdown",
    "source": "### \ud83d\udd0e Visual Inspection\n\nLet's compare the original document image with the generated summary to validate quality:"
  },
  {
    "type": "code",
    "source": "# Compare original document with generated summary\nindex = 0  # Change this to view different examples\n\n# Merge preview data with original images for comparison\ncomparison_dataset = preview.dataset.merge(pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\")\n\n# Extract the record for display\nrecord = comparison_dataset.iloc[index]\n\nprint(\"\ud83d\udcc4 Original Document Image:\")\ndisplay(resize_image(record.image, BASE64_IMAGE_HEIGHT))\n\nprint(\"\\n\ud83d\udcdd Generated Summary:\")\nrich.print(Panel(record.summary, title=\"Document Summary\", title_align=\"left\"))",
    "language": "python"
  },
  {
    "type": "markdown",
    "source": "### \ud83c\udd99 Scale up!\n\n- Happy with your preview data?\n\n- Use the `create` method to submit larger Data Designer generation jobs."
  },
  {
    "type": "code",
    "source": "results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-4\")",
    "language": "python"
  },
  {
    "type": "code",
    "source": "# Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()",
    "language": "python"
  },
  {
    "type": "code",
    "source": "# Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()",
    "language": "python"
  },
  {
    "type": "markdown",
    "source": "## \u23ed\ufe0f Next Steps\n\nNow that you've learned how to use visual context for image summarization in Data Designer, explore more:\n\n- Experiment with different vision models for specific document types\n- Try different prompt variations to generate specialized descriptions (e.g., technical details, key findings)\n- Combine vision-based summaries with other column types for multi-modal workflows\n- Apply this pattern to other vision tasks like image captioning, OCR validation, or visual question answering\n\n- [Generating images](https://nvidia-nemo.github.io/DataDesigner/latest/notebooks/5-generating-images/) with Data Designer"
  }
] };
