---
title: "Deep Research Trajectories with NeMo Data Designer and MCP Tool Use"
description: Using MCP tool-use support to generate multi-turn research trajectories for training deep research agents.
---

import { TrajectoryViewer } from "@/components/TrajectoryViewer";
import trajectory from "@/components/trajectories/4hop-example";
import { ExpandableCode } from "@/components/ExpandableCode";
import { openresearcherDemoCode } from "@/components/devnotes/openresearcher-demo-code";
import { prepare_corpusCode } from "@/components/devnotes/prepare_corpus-code";
import { retriever_mcpCode } from "@/components/devnotes/retriever_mcp-code";

Data Designer v0.5.0's MCP [tool-use support](/docs/concepts/tool-use-and-mcp) lets you generate multi-turn research trajectories, the kind of data needed to train deep research agents that iteratively search, read, and synthesize evidence before answering a question.

---

Deep research agents like [OpenResearcher](https://github.com/TIGER-AI-Lab/OpenResearcher) (Li, Jiang, Ma et al., 2026) and [Universal Deep Research](https://arxiv.org/abs/2509.00244) (Belcak & Molchanov, 2025) generate long reasoning chains interleaved with tool calls: formulating queries, retrieving documents, reading passages, refining hypotheses, and eventually synthesizing an answer. Training these agents requires trajectory data capturing the full multi-turn interaction between a model and its tools: every search, every document opened, every dead end explored.

OpenResearcher demonstrated something worth paying attention to: synthetic trajectories generated against a *local* retriever ([BM25](https://dl.acm.org/doi/abs/10.1561/1500000019) over a static corpus, no web APIs) are sufficient to train [Nemotron Nano 3](https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16) to outperform GPT-4.1 on deep research benchmarks. The data format (complete tool-use traces showing how a model moves through an information space) matters more than model scale. Nemotron Nano 3, with only 3B active parameters, beats models orders of magnitude larger on multi-hop research tasks.

This post shows how to generate that same kind of training data using Data Designer's MCP tool-use capabilities. We build a retriever as an MCP server, construct a corpus with known-good evidence, run a teacher model through the full research process, and use an LLM judge for rejection sampling. The result is a pipeline that produces high-quality research trajectories you can use for supervised fine-tuning or as a starting point for RL.

<TrajectoryViewer {...trajectory} />

---

## Step 1: Building the Retrieval MCP Server

OpenResearcher's key design choice is a three-tool browser interface rather than a single retrieval call. The paper argues (and their ablations confirm) that separating search, document opening, and in-document search forces the model to develop genuine research strategies: skimming results, diving into promising documents, hunting for specific evidence within them. A single monolithic "retrieve" tool collapses this entire workflow into one step, which produces shorter and less useful training trajectories.

We implement the same three tools as an MCP server that Data Designer can invoke during generation. Our retriever uses [BM25S](https://github.com/xhluca/bm25s) for fast lexical search over the corpus:

```python
from mcp.server.fastmcp import FastMCP

mcp_server = FastMCP("corpus-retriever")

@mcp_server.tool()
def search(query: str, top_k: int = 10) -> dict:
    """Search for candidate documents to explore."""
    # BM25S search over the corpus, returns ranked results with snippets
    ...

@mcp_server.tool(name="open")
def open_document(doc_id: str) -> dict:
    """Open a document for detailed inspection with cursor-numbered chunks."""
    # Returns content formatted as [1] paragraph... [2] paragraph...
    ...

@mcp_server.tool()
def find(doc_id: str, query: str) -> dict:
    """Find matching passages inside a document by keyword."""
    # Returns matching chunks with cursor positions
    ...

if __name__ == "__main__":
    mcp_server.run()
```

- **`search`** returns a ranked list of document IDs with short snippets, enough for the model to decide which documents look promising.
- **`open`** returns the full document content, split into cursor-numbered chunks so the model can reference specific passages.
- **`find`** does targeted keyword search *within* a single document, letting the model locate specific evidence without reading the entire thing.

The cursor-based chunking across `open` and `find` gives the model a way to scan long documents incrementally, the way a human researcher would scan a paper for the relevant section rather than reading it cover to cover.

The server runs as a local stdio process, which means Data Designer launches and manages it automatically. No external services, no API keys for retrieval, no rate limits.

---

## Step 2: Building the Corpus

The corpus design follows directly from OpenResearcher's most striking ablation result. They tested what happens when you vary the retrieval corpus while keeping the reasoning model fixed (GPT-OSS-120B). The results, from the [OpenResearcher Appendix](https://boiled-honeycup-4c7.notion.site/Appendix-301e290627b58082abffd1ea2c262eb2):

| Corpus | BrowseComp-Plus Accuracy |
| :---- | :----: |
| Golden passages only (BrowseComp-Plus corpus) | 56.0% |
| 15M FineWeb + golden passages | 31.2% |
| 15M FineWeb only | 0.71% |

Without golden passages (documents known to contain evidence for the question), accuracy drops to nearly zero. The model can't learn research strategies from trajectories where every search is a dead end.

The original OpenResearcher corpus uses 15M documents from [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) as distractors alongside 10K golden passages. For this demonstration, we use a lighter-weight approach: we construct the corpus from multi-hop QA datasets: [HotpotQA](https://arxiv.org/abs/1809.09600) (2-hop questions requiring two pieces of linked evidence) and [MuSiQue](https://arxiv.org/abs/2108.00573) (2-4 hop questions composed from single-hop sub-questions). Each question comes with annotated supporting passages, the specific paragraphs that contain the evidence needed to answer it. Golden passages go into the corpus alongside non-supporting passages from the same datasets as distractors, at roughly a 1:9 ratio. The model has to search through noise to find the signal, which is exactly the skill we want the training data to teach.

The key constraint is that golden passages must be *findable but not obvious*. If the corpus is too small or the golden passages are too easy to identify, the trajectories won't transfer to real-world research where evidence is sparse. The distractor ratio controls this difficulty, and the paper's ablations give us a good starting point for tuning it.

---

## Step 3: The Data Designer Pipeline

With the retriever server and corpus ready, the Data Designer pipeline ties everything together. We configure a teacher model, point it at the MCP retriever, and let it research each question from scratch. For this demo we hosted our own inference server, but anyone can try this pipeline using [Nemotron Nano 3 on build.nvidia.com](https://build.nvidia.com/nvidia/nemotron-3-nano-30b-a3b) with a free API key using the model configuration shown below.

```python
import data_designer.config as dd
from data_designer.interface import DataDesigner

# Search rollout model for trajectory generation
config = dd.DataDesignerConfigBuilder()
config.add_model_config(
    dd.ModelConfig(
        alias="search_rollout_model",
        model="nvidia/nemotron-3-nano-30b-a3b",
        provider="nvidia",
        inference_parameters=dd.ChatCompletionInferenceParams(
            temperature=1.0,
            top_p=0.95,
            max_tokens=16384,
        ),
    )
)
```

The temperature and top_p settings matter here. We want diverse research strategies across seeds (different query formulations, different document exploration orders) so that rejection sampling has a rich pool to select from. Setting temperature to 1.0 with top_p at 0.95 gives enough variation that the same question can produce meaningfully different trajectories across seeds.

The MCP tool configuration tells Data Designer which server to use and how many tool-call turns to allow:

```python
# MCP retriever tool configuration
tool_config = dd.ToolConfig(
    tool_alias="knowledge-base",
    providers=["corpus-retriever"],
    max_tool_call_turns=150,
)
config.add_tool_config(tool_config)
```

We set `max_tool_call_turns` high (150) because deep research trajectories can be long. Our longest observed trajectory used 25 tool calls across 53 messages. Capping too low would truncate the most interesting research chains.

The seed dataset contains the research questions alongside reference answers (which we'll use for rejection sampling in Step 4):

```python
config.with_seed_dataset(
    dd.LocalFileSeedSource(path="questions.jsonl"),
)

config.add_column(
    dd.ExpressionColumnConfig(
        name="research_question",
        expr="{{ question }}",
    )
)
```

The core of the pipeline is the research column, where the teacher model receives a question and a system prompt instructing it to use the retriever tools:

```python
SYSTEM_PROMPT = """You are a thorough research assistant. You have access to three tools \
for navigating a knowledge base:
- search(query, top_k): Find candidate documents relevant to your query
- open(doc_id): Open a document to read its full content in numbered chunks
- find(doc_id, query): Locate specific passages within a document by keyword

Your task is to research the given question by searching for relevant documents, \
reading their content, and synthesizing an answer from the evidence you find. \
Be systematic: formulate search queries, explore promising results, and gather \
evidence before answering. Cite specific passages when possible."""

config.add_column(
    dd.LLMTextColumnConfig(
        name="research_answer",
        prompt="Research and answer thoroughly:\n\n{{ research_question }}",
        model_alias="search_rollout_model",
        system_prompt=SYSTEM_PROMPT,
        tool_alias="knowledge-base",
        with_trace=dd.TraceType.ALL_MESSAGES,
        extract_reasoning_content=True,
    )
)
```

Two settings are doing the important work here:

- **`with_trace=dd.TraceType.ALL_MESSAGES`** captures the *entire* interaction (every tool call, every tool response, every intermediate reasoning step) into a separate trace column in ChatML format. This is the training data: the full trajectory of how the model moved through the information space.
- **`extract_reasoning_content=True`** pulls out the model's internal chain-of-thought separately, so you can include or exclude it depending on your training setup.

---

## Step 4: Rejection Sampling with an LLM Judge

Not every trajectory leads to a correct answer. OpenResearcher's approach is straightforward. Generate multiple trajectories per question, score them for correctness, and keep only the ones that got the right answer. We implement this with Data Designer's `LLMJudgeColumnConfig`, using a separate (smaller) model as the judge:

```python
# Judge model for rejection sampling
config.add_model_config(
    dd.ModelConfig(
        alias="judge",
        model="nvidia/nemotron-3-nano-30b-a3b",
        provider="nvidia",
    )
)

config.add_column(
    dd.LLMJudgeColumnConfig(
        name="correctness",
        model_alias="judge",
        prompt=(
            "Question: {{ research_question }}\n"
            "Reference answer: {{ answer }}\n"
            "Generated answer: {{ research_answer }}\n"
            "Does the generated answer correctly address the question?"
        ),
        scores=[
            dd.Score(
                name="correct",
                description="Is the answer factually correct?",
                options={
                    1: "Correct",
                    0: "Incorrect",
                },
            ),
        ],
    )
)
```

The judge compares the generated answer against the reference answer from the seed dataset. Using a smaller model as judge is deliberate. We don't need the judge to *reason* about the question, just to compare two answers for factual agreement. This keeps costs down when scoring thousands of trajectories.

In practice, you'd generate multiple trajectories per question (varying the random seed) and filter to `correctness.correct == 1`. The incorrect trajectories aren't wasted; they can serve as negative examples for preference-based training methods like DPO.

---

## Multi-Turn Tool Calling: Rough Edges in the Open Model Ecosystem

The pipeline described above is straightforward in principle. In practice, getting multi-turn tool calling to work reliably with open-weight models served through vLLM turned out to be the hardest part of this project.

We tested two open-weight models on a self-hosted [vLLM (v0.15.1)](https://github.com/vllm-project/vllm/releases/tag/v0.15.1) instance: [GPT-OSS-120B](https://huggingface.co/openai/gpt-oss-120b) and [Kimi K2.5](https://huggingface.co/moonshotai/Kimi-K2.5). Both failed to produce usable research trajectories, for related but distinct reasons.

<Warning>
**GPT-OSS-120B** uses a "Harmony" output format that routes text through named channels (reasoning, final answer, tool calls). When tools are involved, vLLM's parser consistently routes the model's output to the wrong channel: everything lands in `reasoning_content` while the `content` field stays empty. This happens at all `reasoning_effort` levels.

**Kimi K2.5** exhibits a different failure mode. With thinking mode enabled, it has the same channel-routing problem. With thinking mode disabled, the model produces content text, but after the first tool result, it *narrates* what it plans to do next rather than emitting another tool call.
</Warning>

The original OpenResearcher codebase handles this by bypassing vLLM's tool call parser entirely. They hit the raw `/completions` endpoint, parse `<tool_call>` XML tags from the output with regex, and continue looping until the model emits an explicit answer marker.

The open-source tool-calling stack is growing and maturing quickly, but multi-turn tool use with reasoning models is still a rough edge. For now, the practical path is to use models with battle-tested tool-calling support through their native APIs, which is what we do in the results below.

---

## Results

We ran 64 questions uniformly sampled across 2, 3, and 4-hop difficulty levels from MuSiQue, with 50K FineWeb web documents as distractors (a 1:100 golden-to-distractor ratio). We tested two models, Claude Opus 4.5 (via API) and Nemotron Nano 3 (30B total / 3B active params, self-hosted via vLLM with reasoning disabled).

| | Claude Opus 4.5 | Nemotron Nano 3 |
| :---- | :----: | :----: |
| **Samples** | 64 (55 completed) | 64 (61 completed) |
| **Overall accuracy** | 41/55 (75%) | 32/61 (52%) |
| **2-hop accuracy** | 18/23 (78%) | 13/23 (57%) |
| **3-hop accuracy** | 15/18 (83%) | 11/22 (50%) |
| **4-hop accuracy** | 8/14 (57%) | 8/16 (50%) |
| **Avg tool calls** | 16.8 | 11.8 |
| **Max tool calls** | 57 | 63 |
| **Avg messages per trajectory** | 40.4 | 26.5 |
| **Max messages per trajectory** | 117 | 129 |

Opus is 22 points more accurate, but Nano runs roughly 5x faster on self-hosted hardware. Both models show tool usage scaling with hop count. Nano uses fewer tools but achieves lower accuracy, with the largest gap on 2-hop questions (78% vs 57%). Splitting by correctness reveals the same pattern in both models: incorrect trajectories are longer.

<Accordion title="Detailed results by model and correctness">

**Claude Opus 4.5:**

| Outcome | Hops | Count | Avg Tool Calls | Avg Messages | Avg Answer Length |
| :---- | :----: | :----: | :----: | :----: | :----: |
| **Correct** | 2 | 18 | 7.3 | 18.9 | 1,072 chars |
| | 3 | 15 | 14.9 | 35.7 | 1,372 chars |
| | 4 | 8 | 21.0 | 50.6 | 1,705 chars |
| | **All** | **41** | **12.8** | **31.2** | **1,305 chars** |
| **Incorrect** | 2 | 5 | 21.0 | 48.6 | 1,534 chars |
| | 3 | 3 | 25.7 | 63.0 | 1,795 chars |
| | 4 | 6 | 36.0 | 85.2 | 1,903 chars |
| | **All** | **14** | **28.4** | **67.4** | **1,748 chars** |

**Nemotron Nano 3:**

| Outcome | Hops | Count | Avg Tool Calls | Avg Messages | Avg Answer Length |
| :---- | :----: | :----: | :----: | :----: | :----: |
| **Correct** | 2 | 13 | 6.5 | 16.1 | 773 chars |
| | 3 | 11 | 12.7 | 28.5 | 708 chars |
| | 4 | 8 | 8.0 | 19.0 | 1,600 chars |
| | **All** | **32** | **9.0** | **21.1** | **957 chars** |
| **Incorrect** | 2 | 10 | 10.1 | 23.2 | 799 chars |
| | 3 | 11 | 18.0 | 39.0 | 1,163 chars |
| | 4 | 8 | 16.2 | 35.5 | 848 chars |
| | **All** | **29** | **14.8** | **32.6** | **951 chars** |

</Accordion>

Correct trajectories are shorter at every hop level for both models. Incorrect trajectories are roughly twice as long because the model keeps searching when it can't find evidence, then writes a longer answer to compensate. This anti-correlation between trajectory length and correctness is consistent across model scales, which means trajectory length alone could serve as a lightweight filter during rejection sampling.

---

## Closing Remarks

Thanks to the [OpenResearcher](https://github.com/TIGER-AI-Lab/OpenResearcher) team for their work showing that synthetic research trajectories over local retrieval can train small models to compete with much larger ones. Their results suggest we're only beginning to understand how LLMs interact with search tools and how the structure of those interactions shapes what models learn. We're excited to see where the community takes synthetic data research using [NeMo Data Designer](https://github.com/NVIDIA-NeMo/DataDesigner) as both the models and the tooling continue to improve.

---

## Try For Yourself

<ExpandableCode
  summary="Full source: openresearcher_demo.py"
  code={openresearcherDemoCode}
  language="python"
/>

<ExpandableCode
  summary="Full source: prepare_corpus.py"
  code={prepare_corpusCode}
  language="python"
/>

<ExpandableCode
  summary="Full source: retriever_mcp.py"
  code={retriever_mcpCode}
  language="python"
/>



## See Also

- [NeMo Data Designer on GitHub](https://github.com/NVIDIA-NeMo/DataDesigner)
- [OpenResearcher on GitHub](https://github.com/TIGER-AI-Lab/OpenResearcher)
- [OpenResearcher blog post](https://boiled-honeycup-4c7.notion.site/OpenResearcher-A-Fully-Open-Pipeline-for-Long-Horizon-Deep-Research-Trajectory-Synthesis-2f7e290627b5800cb3a0cd7e8d6ec0ea)
- [HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering](https://arxiv.org/abs/1809.09600)
- [MuSiQue: Multi-hop Questions via Single-hop Question Composition](https://arxiv.org/abs/2108.00573)
- [BM25S: Fast lexical search in Python](https://github.com/xhluca/bm25s)
