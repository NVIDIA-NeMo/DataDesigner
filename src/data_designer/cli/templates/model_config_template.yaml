# Model Configuration
# This file defines the models available for LLM-based data generation.
# Each model config specifies the model, parameters, and provider.

model_configs:
  # User-defined alias for the model (used in column configs)
  - alias: llama-3-70b

    # Model ID (e.g., from build.nvidia.com or OpenAI)
    model: meta/llama-3.3-70b-instruct

    # Optional: Specify which provider to use (overrides default)
    # If not specified, uses the default provider
    # provider: nvidia

    # Inference parameters for generation
    inference_parameters:
      # Temperature controls randomness (0.0 - 2.0)
      # Lower = more focused, Higher = more creative
      temperature: 0.7

      # Top-p controls diversity via nucleus sampling (0.0 - 1.0)
      # Lower = more focused, Higher = more diverse
      top_p: 0.9

      # Maximum tokens to generate (min: 1)
      max_tokens: 2048

      # Maximum parallel requests (min: 1)
      max_parallel_requests: 4

      # Request timeout in seconds (optional)
      # timeout: 60

      # Extra parameters to pass to the model provider (optional)
      # extra_body:
      #   custom_param: value

  # Example: Code generation model with different parameters
  # - alias: codellama
  #   model: meta/codellama-70b-instruct
  #   inference_parameters:
  #     temperature: 0.2  # Lower for more deterministic code
  #     top_p: 0.95
  #     max_tokens: 4096

  # Example: Model with distribution-based temperature
  # Samples temperature from a uniform distribution on each request
  # - alias: creative-model
  #   model: meta/llama-3.3-70b-instruct
  #   inference_parameters:
  #     temperature:
  #       distribution_type: uniform
  #       params:
  #         low: 0.7
  #         high: 1.2
  #     top_p: 0.9
  #     max_tokens: 2048

  # Example: Model with manual distribution for temperature
  # Samples from discrete values with optional weights
  # - alias: varied-model
  #   model: meta/llama-3.3-70b-instruct
  #   inference_parameters:
  #     temperature:
  #       distribution_type: manual
  #       params:
  #         values: [0.5, 0.7, 0.9, 1.1]
  #         weights: [0.2, 0.3, 0.3, 0.2]  # Optional, defaults to uniform
  #     top_p: 0.9
  #     max_tokens: 2048
