{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83c\udfa8 NeMo Data Designer","text":"<p>\ud83d\udc4b Welcome! Data Designer is an orchestration framework for generating high-quality synthetic data. You provide LLM endpoints (NVIDIA, OpenAI, vLLM, etc.), and Data Designer handles batching, parallelism, validation, and more.</p> <p>Configure columns and models \u2192 Preview samples and iterate \u2192 Create your full dataset at scale.</p> <p>Unlike raw LLM calls, Data Designer gives you statistical diversity, field correlations, automated validation, and reproducible workflows. For details, see Architecture &amp; Performance.</p> <p>\ud83d\udcdd Want to hear from the team? Check out our Dev Notes for deep dives, best practices, and insights.</p>"},{"location":"#install","title":"Install","text":"<pre><code>pip install data-designer\n</code></pre>"},{"location":"#setup","title":"Setup","text":"<p>Get an API key from one of the default providers and set it as an environment variable:</p> <pre><code># NVIDIA (build.nvidia.com) - recommended\nexport NVIDIA_API_KEY=\"your-api-key-here\"\n\n# OpenAI (platform.openai.com)\nexport OPENAI_API_KEY=\"your-openai-api-key-here\"\n\n# OpenRouter (openrouter.ai)\nexport OPENROUTER_API_KEY=\"your-openrouter-api-key-here\"\n</code></pre> <p>Verify your configuration is ready:</p> <pre><code>data-designer config list\n</code></pre> <p>This displays the pre-configured model providers and models. See CLI Configuration to customize.</p>"},{"location":"#your-first-dataset","title":"Your First Dataset","text":"<p>Let's generate multilingual greetings to see Data Designer in action:</p> <pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\n# Initialize with default model providers\ndata_designer = DataDesigner()\nconfig_builder = dd.DataDesignerConfigBuilder()\n\n# Add a sampler column to randomly select a language\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"language\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\"English\", \"Spanish\", \"French\", \"German\", \"Italian\"],\n        ),\n    )\n)\n\n# Add an LLM text generation column\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"greeting\",\n        model_alias=\"nvidia-text\",\n        prompt=\"Write a casual and formal greeting in {{ language }}.\",\n    )\n)\n\n# Generate a preview\nresults = data_designer.preview(config_builder)\nresults.display_sample_record()\n</code></pre> <p>\ud83c\udf89 That's it! You've just designed your first synthetic dataset.</p>"},{"location":"#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ul> <li> <p> Tutorials</p> <p>Step-by-step notebooks covering core features</p> </li> <li> <p> Recipes</p> <p>Ready-to-use examples for common use cases</p> </li> <li> <p> Concepts</p> <p>Deep dive into columns, models, and configuration</p> </li> </ul>"},{"location":"#learn-more","title":"Learn More","text":"<ul> <li>Deployment Options \u2013 Library vs. NeMo Microservice</li> <li>Model Configuration \u2013 Configure LLM providers and models</li> <li>Architecture &amp; Performance \u2013 Optimize for throughput and scale</li> </ul>"},{"location":"CONTRIBUTING/","title":"\ud83c\udfa8\u2728 Contributing to NeMo Data Designer \ud83c\udfa8\u2728","text":"<p>Thank you for your interest in contributing to Data Designer!</p> <p>We welcome contributions from the community and sincerely appreciate your efforts to improve the project. Whether you're fixing a typo, reporting a bug, proposing a new feature, or implementing a major enhancement, your work helps make Data Designer better for everyone \ud83c\udf89.</p> <p>This guide will help you get started with the contribution process.</p>"},{"location":"CONTRIBUTING/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Getting Started</li> <li>Ways to Contribute</li> <li>Feature Requests</li> <li>Development Guide</li> <li>Submitting Changes</li> <li>Code of Conduct</li> <li>Signing off on your work</li> </ul>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>\ud83d\udc4b Welcome to the Data Designer community! We're excited to have you here.</p> <p>Whether you're new to the project or ready to dive in, the resources below will help you get oriented and productive quickly:</p> <ol> <li> <p>README.md \u2013\u00a0best place to start to learn the basics of the project</p> </li> <li> <p>AGENTS.md\u00a0\u2013 context and instructions to help AI coding agents work on Data Designer (it's also useful for human developers!)</p> </li> <li> <p>Documentation \u2013\u00a0detailed documentation on Data Designer's capabilities and usage</p> </li> </ol>"},{"location":"CONTRIBUTING/#ways-to-contribute","title":"Ways to Contribute","text":"<p>There are many ways to contribute to Data Designer:</p>"},{"location":"CONTRIBUTING/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<p>Found a bug? Before reporting, please 1. Verify you're using the latest version: <code>uv pip install --upgrade data-designer</code> 2. Search for duplicates in the issue tracker</p> <p>When creating a bug report, please include: - Data Designer version - Python version and operating system - Minimal reproducible example - Expected vs. actual behavior - Full error messages and stack traces</p> <p>If you are interested in fixing the bug yourself, that's AWESOME! Please follow the development guide to get started.</p>"},{"location":"CONTRIBUTING/#feature-implementation","title":"\u2728 Feature Implementation","text":"<p>Want to add new functionality? Great! Please review our development approach and open a feature request to discuss the idea and get feedback before investing significant time on the implementation.</p>"},{"location":"CONTRIBUTING/#documentation-improvements","title":"\ud83d\udcd6 Documentation Improvements","text":"<p>Documentation is crucial for user adoption. Contributions that clarify usage, add examples, or fix typos are highly valued.</p>"},{"location":"CONTRIBUTING/#examples-and-tutorials","title":"\ud83d\udca1 Examples and Tutorials","text":"<p>Share your use cases! Example notebooks and tutorials help others understand how to leverage Data Designer effectively.</p>"},{"location":"CONTRIBUTING/#test-coverage","title":"\ud83e\uddea Test Coverage","text":"<p>Help us improve test coverage by adding tests for untested code paths or edge cases.</p>"},{"location":"CONTRIBUTING/#feature-requests","title":"Feature Requests","text":"<p>Data Designer is designed to be as flexible and extensible as possible, and we welcome your ideas for pushing its capabilities even further! To keep the core library maintainable, while also supporting innovation, we take an incremental approach when adding new features \u2013 we explore what's already possible, extend through plugins when needed, and integrate the most broadly useful features into the core library:</p>"},{"location":"CONTRIBUTING/#how-we-grow-data-designer","title":"How We Grow Data Designer","text":"<ol> <li> <p>\ud83e\uddd7 Explore what's possible: Can your use case be achieved with current features? We've designed Data Designer to be composable \u2013 sometimes creative combinations of existing tools can accomplish what you need. Check out our examples or open an issue if you'd like help exploring this!</p> </li> <li> <p>\ud83d\udd0c Extend through plugins: If existing features aren't quite enough, consider implementing your idea as a plugin that extends the core library. Plugins let you experiment and share functionality while keeping the core library focused.</p> </li> <li> <p>\u2699\ufe0f Integrate into the core library: If your feature or plugin proves broadly useful and aligns with Data Designer's goals, we'd love to integrate it into the core library! We're happy to discuss whether it's a good fit and how to move forward together.</p> </li> </ol> <p>This approach helps us grow thoughtfully while keeping Data Designer focused and maintainable.</p>"},{"location":"CONTRIBUTING/#submitting-a-feature-request","title":"Submitting a Feature Request","text":"<p>Open a new issue with:</p> <ul> <li>Clear title: Concise description of the feature</li> <li>Use case: Explain what problem this solves and why it's important</li> <li>Proposed solution: Describe how you envision the feature working</li> <li>Alternatives considered: Other approaches you've thought about</li> <li>Examples: Code examples or mockups of how users would interact with the feature</li> <li>Willingness to implement: Are you interested in implementing this yourself?</li> </ul>"},{"location":"CONTRIBUTING/#development-guide","title":"Development Guide","text":"<p>Data Designer uses <code>uv</code> for dependency management. If you don't have uv installed, follow their installation instructions.</p>"},{"location":"CONTRIBUTING/#initial-setup","title":"Initial Setup","text":"<ol> <li> <p>Create or find an issue</p> <p>Before starting work, ensure there's an issue tracking your contribution:</p> <ul> <li>For bug fixes: Search existing issues or create a new one</li> <li>For new features: Open a feature request to discuss the approach first</li> <li>Comment on the issue to let maintainers know you're working on it</li> </ul> </li> <li> <p>Fork and clone the repository</p> <p>Start by forking the Data Designer repository, then clone your fork and add the upstream remote:</p> <pre><code>git clone https://github.com/YOUR_GITHUB_USERNAME/DataDesigner.git\n\ncd DataDesigner\n\ngit remote add upstream https://github.com/NVIDIA-NeMo/DataDesigner.git\n</code></pre> </li> <li> <p>Install dependencies</p> <pre><code># Install project with dev dependencies\nmake install-dev\n\n# Or, if you use Jupyter / IPython for development\nmake install-dev-notebooks\n</code></pre> </li> <li> <p>Verify your setup</p> <pre><code>make test &amp;&amp; make check-all\n</code></pre> <p>If no errors are reported, you're ready to develop \ud83d\ude80</p> </li> </ol>"},{"location":"CONTRIBUTING/#making-changes","title":"Making Changes","text":"<ol> <li> <p>Create a feature branch</p> <pre><code>git checkout main\ngit pull upstream main\ngit checkout -b &lt;username&gt;/&lt;type-of-change&gt;/&lt;issue-number&gt;-&lt;short-description&gt;\n</code></pre> <p>Example types of change:</p> <ul> <li><code>feat</code> for new features</li> <li><code>fix</code> for bug fixes</li> <li><code>docs</code> for documentation updates</li> <li><code>test</code> for testing changes</li> <li><code>refactor</code> for code refactoring</li> <li><code>chore</code> for chore tasks</li> <li><code>style</code> for style changes</li> <li><code>perf</code> for performance improvements</li> </ul> <p>Example branch name:</p> <ul> <li><code>johnnygreco/feat/123-add-xyz-generator</code> for a new feature by @johnnygreco, addressing issue #123</li> </ul> </li> <li> <p>Develop your changes</p> <p>Please follow the patterns and conventions used throughout the codebase, as well as those outlined in AGENTS.md.</p> </li> <li> <p>Test and validate</p> <pre><code>make check-all-fix  # Format code and fix linting issues\nmake test           # Run all tests\nmake coverage       # Check test coverage (must be &gt;90%)\n</code></pre> <p>Writing tests: Place tests in tests/ mirroring the source structure. Use fixtures from tests/conftest.py, mock external services with <code>unittest.mock</code> or <code>pytest-httpx</code>, and test both success and failure cases. See AGENTS.md for patterns and examples.</p> </li> <li> <p>Commit your work</p> <p>Write clear, descriptive commit messages, optionally including a brief summary (50 characters or less) and reference issue numbers when applicable (e.g., \"Fixes #123\").</p> <pre><code>git commit -m \"Add XYZ generator for synthetic data\" -m \"Fixes #123\"\n</code></pre> </li> <li> <p>Stay up to date</p> <p>Regularly sync your branch with upstream changes:</p> <pre><code>git fetch upstream\ngit merge upstream/main\n</code></pre> </li> </ol>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting Changes","text":""},{"location":"CONTRIBUTING/#before-submitting","title":"Before Submitting","text":"<p>Ensure your changes meet the following criteria:</p> <ul> <li>All tests pass (<code>make test</code>)</li> <li>Code is formatted and linted (<code>make check-all-fix</code>)</li> <li>New functionality includes tests</li> <li>Documentation is updated (README, docstrings, examples)</li> <li>License headers are present on all new files</li> <li>Commit messages are clear and descriptive</li> </ul>"},{"location":"CONTRIBUTING/#creating-a-pull-request","title":"Creating a Pull Request","text":"<ol> <li> <p>Push your changes to your fork:</p> <pre><code>git push origin &lt;username&gt;/&lt;type-of-change&gt;/&lt;issue-number&gt;-&lt;short-description&gt;\n</code></pre> </li> <li> <p>Open a pull request on GitHub from your fork to the main repository</p> </li> <li> <p>Respond to review feedback update your PR as needed</p> </li> </ol>"},{"location":"CONTRIBUTING/#pull-request-review-process","title":"Pull Request Review Process","text":"<ul> <li>Maintainers will review your PR and may request changes</li> <li>Address feedback by pushing additional commits to your branch</li> <li>Reply to the feedback comment with a link to the commit that addresses it.</li> <li>Once approved, a maintainer will merge your PR</li> <li>Your contribution will be included in the next release!</li> </ul>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>Data Designer follows the Contributor Covenant Code of Conduct. We are committed to providing a welcoming and inclusive environment for all contributors.</p> <p>Please read our complete Code of Conduct for full details on our standards and expectations.</p>"},{"location":"CONTRIBUTING/#license-file-headers","title":"License File Headers","text":"<p>All code files that are added to this repository must include the appropriate NVIDIA copyright header:</p> <pre><code># SPDX-FileCopyrightText: Copyright (c) {YEAR} NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n</code></pre> <p>Use <code>make update-license-headers</code> to add headers automatically.</p>"},{"location":"CONTRIBUTING/#signing-off-on-your-work","title":"Signing off on your work","text":"<p>When contributing to this project, you must agree that you have authored 100% of the content, that you have the necessary rights to the content and that the content you contribute may be provided under the project license. All contributors are asked to sign the Data Designer Developer Certificate of Origin (DCO) when submitting their first pull request. The process is automated by a bot that will comment on the pull request. Our DCO is the same as the Linux Foundation requires its contributors to sign.</p> <p>Thank you for contributing to NeMo Data Designer! Your efforts help make synthetic data generation more accessible and powerful for everyone. \ud83c\udfa8\u2728</p>"},{"location":"code_reference/analysis/","title":"Analysis","text":"<p>The <code>analysis</code> modules provide tools for profiling and analyzing generated datasets. It includes statistics tracking, column profiling, and reporting capabilities.</p>"},{"location":"code_reference/analysis/#column-statistics","title":"Column Statistics","text":"<p>Column statistics are automatically computed for every column after generation. They provide basic metrics specific to the column type. For example, LLM columns track token usage statistics, sampler columns track distribution information, and validation columns track validation success rates.</p> <p>The classes below are result objects that store the computed statistics for each column type and provide methods for formatting these results for display in reports.</p> <p>Classes:</p> Name Description <code>BaseColumnStatistics</code> <p>Abstract base class for all column statistics types.</p> <code>CategoricalDistribution</code> <p>Container for computed categorical distribution statistics.</p> <code>CategoricalHistogramData</code> <p>Container for categorical distribution histogram data.</p> <code>ExpressionColumnStatistics</code> <p>Container for statistics on expression-based derived columns.</p> <code>GeneralColumnStatistics</code> <p>Container for general statistics applicable to all column types.</p> <code>LLMCodeColumnStatistics</code> <p>Container for statistics on LLM-generated code columns.</p> <code>LLMJudgedColumnStatistics</code> <p>Container for statistics on LLM-as-a-judge quality assessment columns.</p> <code>LLMStructuredColumnStatistics</code> <p>Container for statistics on LLM-generated structured JSON columns.</p> <code>LLMTextColumnStatistics</code> <p>Container for statistics on LLM-generated text columns.</p> <code>NumericalDistribution</code> <p>Container for computed numerical distribution statistics.</p> <code>SamplerColumnStatistics</code> <p>Container for statistics on sampler-generated columns.</p> <code>SeedDatasetColumnStatistics</code> <p>Container for statistics on columns sourced from seed datasets.</p> <code>ValidationColumnStatistics</code> <p>Container for statistics on validation result columns.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.BaseColumnStatistics","title":"<code>BaseColumnStatistics</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract base class for all column statistics types.</p> <p>Serves as a container for computed statistics across different column types in Data-Designer-generated datasets. Subclasses hold column-specific statistical results and provide methods for formatting these results for display in reports.</p> <p>Methods:</p> Name Description <code>create_report_row_data</code> <p>Creates a formatted dictionary of statistics for display in reports.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.BaseColumnStatistics.create_report_row_data","title":"<code>create_report_row_data()</code>  <code>abstractmethod</code>","text":"<p>Creates a formatted dictionary of statistics for display in reports.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary mapping display labels to formatted statistic values.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/analysis/column_statistics.py</code> <pre><code>@abstractmethod\ndef create_report_row_data(self) -&gt; dict[str, str]:\n    \"\"\"Creates a formatted dictionary of statistics for display in reports.\n\n    Returns:\n        Dictionary mapping display labels to formatted statistic values.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.CategoricalDistribution","title":"<code>CategoricalDistribution</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for computed categorical distribution statistics.</p> <p>Attributes:</p> Name Type Description <code>most_common_value</code> <code>str | int</code> <p>The category value that appears most frequently in the data.</p> <code>least_common_value</code> <code>str | int</code> <p>The category value that appears least frequently in the data.</p> <code>histogram</code> <code>CategoricalHistogramData</code> <p>Complete frequency distribution showing all categories and their counts.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.CategoricalHistogramData","title":"<code>CategoricalHistogramData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for categorical distribution histogram data.</p> <p>Stores the computed frequency distribution of categorical values.</p> <p>Attributes:</p> Name Type Description <code>categories</code> <code>list[float | int | str]</code> <p>List of unique category values that appear in the data.</p> <code>counts</code> <code>list[int]</code> <p>List of occurrence counts for each category.</p> <p>Methods:</p> Name Description <code>ensure_python_types</code> <p>Ensure numerical values are Python objects rather than Numpy types.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.CategoricalHistogramData.ensure_python_types","title":"<code>ensure_python_types()</code>","text":"<p>Ensure numerical values are Python objects rather than Numpy types.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/analysis/column_statistics.py</code> <pre><code>@model_validator(mode=\"after\")\ndef ensure_python_types(self) -&gt; Self:\n    \"\"\"Ensure numerical values are Python objects rather than Numpy types.\"\"\"\n    self.categories = [(float(x) if is_float(x) else (int(x) if is_int(x) else str(x))) for x in self.categories]\n    self.counts = [int(i) for i in self.counts]\n    return self\n</code></pre>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.ExpressionColumnStatistics","title":"<code>ExpressionColumnStatistics</code>","text":"<p>               Bases: <code>GeneralColumnStatistics</code></p> <p>Container for statistics on expression-based derived columns.</p> <p>Inherits general statistics and stores statistics computed from columns that are derived from columns that are derived from Jinja2 expressions referencing other column values.</p> <p>Attributes:</p> Name Type Description <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"expression\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.GeneralColumnStatistics","title":"<code>GeneralColumnStatistics</code>","text":"<p>               Bases: <code>BaseColumnStatistics</code></p> <p>Container for general statistics applicable to all column types.</p> <p>Holds core statistical measures that apply universally across all column types, including null counts, unique values, and data type information. Serves as the base for more specialized column statistics classes that store additional column-specific metrics.</p> <p>Attributes:</p> Name Type Description <code>column_name</code> <code>str</code> <p>Name of the column being analyzed.</p> <code>num_records</code> <code>int | MissingValue</code> <p>Total number of records in the column.</p> <code>num_null</code> <code>int | MissingValue</code> <p>Number of null/missing values in the column.</p> <code>num_unique</code> <code>int | MissingValue</code> <p>Number of distinct values in the column. If a value is not hashable, it is converted to a string.</p> <code>pyarrow_dtype</code> <code>str</code> <p>PyArrow data type of the column as a string.</p> <code>simple_dtype</code> <code>str</code> <p>Simplified human-readable data type label.</p> <code>column_type</code> <code>Literal['general']</code> <p>Discriminator field, always \"general\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.LLMCodeColumnStatistics","title":"<code>LLMCodeColumnStatistics</code>","text":"<p>               Bases: <code>LLMTextColumnStatistics</code></p> <p>Container for statistics on LLM-generated code columns.</p> <p>Inherits all token usage metrics from LLMTextColumnStatistics. Stores statistics from columns that generate code snippets in specific programming languages.</p> <p>Attributes:</p> Name Type Description <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"llm-code\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.LLMJudgedColumnStatistics","title":"<code>LLMJudgedColumnStatistics</code>","text":"<p>               Bases: <code>LLMTextColumnStatistics</code></p> <p>Container for statistics on LLM-as-a-judge quality assessment columns.</p> <p>Inherits all token usage metrics from LLMTextColumnStatistics. Stores statistics from columns that evaluate and score other generated content based on defined criteria.</p> <p>Attributes:</p> Name Type Description <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"llm-judge\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.LLMStructuredColumnStatistics","title":"<code>LLMStructuredColumnStatistics</code>","text":"<p>               Bases: <code>LLMTextColumnStatistics</code></p> <p>Container for statistics on LLM-generated structured JSON columns.</p> <p>Inherits all token usage metrics from LLMTextColumnStatistics. Stores statistics from columns that generate structured data conforming to JSON schemas or Pydantic models.</p> <p>Attributes:</p> Name Type Description <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"llm-structured\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.LLMTextColumnStatistics","title":"<code>LLMTextColumnStatistics</code>","text":"<p>               Bases: <code>GeneralColumnStatistics</code></p> <p>Container for statistics on LLM-generated text columns.</p> <p>Inherits general statistics plus token usage metrics specific to LLM text generation. Stores both prompt and completion token consumption data.</p> <p>Attributes:</p> Name Type Description <code>output_tokens_mean</code> <code>float | MissingValue</code> <p>Mean number of output tokens generated per record.</p> <code>output_tokens_median</code> <code>float | MissingValue</code> <p>Median number of output tokens generated per record.</p> <code>output_tokens_stddev</code> <code>float | MissingValue</code> <p>Standard deviation of output tokens per record.</p> <code>input_tokens_mean</code> <code>float | MissingValue</code> <p>Mean number of input tokens used per record.</p> <code>input_tokens_median</code> <code>float | MissingValue</code> <p>Median number of input tokens used per record.</p> <code>input_tokens_stddev</code> <code>float | MissingValue</code> <p>Standard deviation of input tokens per record.</p> <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"llm-text\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.NumericalDistribution","title":"<code>NumericalDistribution</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for computed numerical distribution statistics.</p> <p>Attributes:</p> Name Type Description <code>min</code> <code>float | int</code> <p>Minimum value in the distribution.</p> <code>max</code> <code>float | int</code> <p>Maximum value in the distribution.</p> <code>mean</code> <code>float</code> <p>Arithmetic mean (average) of all values.</p> <code>stddev</code> <code>float</code> <p>Standard deviation measuring the spread of values around the mean.</p> <code>median</code> <code>float</code> <p>Median value of the distribution.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.SamplerColumnStatistics","title":"<code>SamplerColumnStatistics</code>","text":"<p>               Bases: <code>GeneralColumnStatistics</code></p> <p>Container for statistics on sampler-generated columns.</p> <p>Inherits general statistics plus sampler-specific information including the sampler type used and the empirical distribution of generated values. Stores both categorical and numerical distribution results.</p> <p>Attributes:</p> Name Type Description <code>sampler_type</code> <code>SamplerType</code> <p>Type of sampler used to generate this column (e.g., \"uniform\", \"category\", \"gaussian\", \"person\").</p> <code>distribution_type</code> <code>ColumnDistributionType</code> <p>Classification of the column's distribution (categorical, numerical, text, other, or unknown).</p> <code>distribution</code> <code>CategoricalDistribution | NumericalDistribution | MissingValue | None</code> <p>Empirical distribution statistics for the generated values. Can be CategoricalDistribution (for discrete values), NumericalDistribution (for continuous values), or MissingValue if distribution could not be computed.</p> <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"sampler\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.SeedDatasetColumnStatistics","title":"<code>SeedDatasetColumnStatistics</code>","text":"<p>               Bases: <code>GeneralColumnStatistics</code></p> <p>Container for statistics on columns sourced from seed datasets.</p> <p>Inherits general statistics and stores statistics computed from columns that originate from existing data provided via the seed dataset functionality.</p> <p>Attributes:</p> Name Type Description <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"seed-dataset\" for this statistics type.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_statistics.ValidationColumnStatistics","title":"<code>ValidationColumnStatistics</code>","text":"<p>               Bases: <code>GeneralColumnStatistics</code></p> <p>Container for statistics on validation result columns.</p> <p>Inherits general statistics plus validation-specific metrics including the count and percentage of records that passed validation. Stores results from validation logic (Python, SQL, or remote) executed against target columns.</p> <p>Attributes:</p> Name Type Description <code>num_valid_records</code> <code>int | MissingValue</code> <p>Number of records that passed validation.</p> <code>column_type</code> <code>Literal[value]</code> <p>Discriminator field, always \"validation\" for this statistics type.</p>"},{"location":"code_reference/analysis/#column-profilers","title":"Column Profilers","text":"<p>Column profilers are optional analysis tools that provide deeper insights into specific column types. Currently, the only column profiler available is the Judge Score Profiler.</p> <p>The classes below are result objects that store the computed profiler results and provide methods for formatting these results for display in reports.</p> <p>Classes:</p> Name Description <code>ColumnProfilerResults</code> <p>Abstract base class for column profiler results.</p> <code>JudgeScoreDistributions</code> <p>Container for computed distributions across all judge score dimensions.</p> <code>JudgeScoreProfilerConfig</code> <p>Configuration for the LLM-as-a-judge score profiler.</p> <code>JudgeScoreProfilerResults</code> <p>Container for complete judge score profiler analysis results.</p> <code>JudgeScoreSample</code> <p>Container for a single judge score and its associated reasoning.</p> <code>JudgeScoreSummary</code> <p>Container for an LLM-generated summary of a judge score dimension.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_profilers.ColumnProfilerResults","title":"<code>ColumnProfilerResults</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract base class for column profiler results.</p> <p>Stores results from column profiling operations. Subclasses hold profiler-specific analysis results and provide methods for generating formatted report sections for display.</p> <p>Methods:</p> Name Description <code>create_report_section</code> <p>Creates a Rich Panel containing the formatted profiler results for display.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_profilers.ColumnProfilerResults.create_report_section","title":"<code>create_report_section()</code>","text":"<p>Creates a Rich Panel containing the formatted profiler results for display.</p> <p>Returns:</p> Type Description <code>Panel</code> <p>A Rich Panel containing the formatted profiler results. Default implementation</p> <code>Panel</code> <p>returns a \"Not Implemented\" message; subclasses should override to provide</p> <code>Panel</code> <p>specific formatting.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/analysis/column_profilers.py</code> <pre><code>def create_report_section(self) -&gt; Panel:\n    \"\"\"Creates a Rich Panel containing the formatted profiler results for display.\n\n    Returns:\n        A Rich Panel containing the formatted profiler results. Default implementation\n        returns a \"Not Implemented\" message; subclasses should override to provide\n        specific formatting.\n    \"\"\"\n    return Panel(\n        f\"Report section generation not implemented for '{self.__class__.__name__}'.\",\n        title=\"Not Implemented\",\n        border_style=f\"bold {ColorPalette.YELLOW.value}\",\n        padding=(1, 2),\n    )\n</code></pre>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_profilers.JudgeScoreDistributions","title":"<code>JudgeScoreDistributions</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for computed distributions across all judge score dimensions.</p> <p>Stores the complete distribution analysis for all score dimensions in an LLM-as-a-judge column. Each score dimension (e.g., \"relevance\", \"fluency\") has its own distribution computed from the generated data.</p> <p>Attributes:</p> Name Type Description <code>scores</code> <code>dict[str, list[int | str]]</code> <p>Mapping of each score dimension name to its list of score values.</p> <code>reasoning</code> <code>dict[str, list[str]]</code> <p>Mapping of each score dimension name to its list of reasoning texts.</p> <code>distribution_types</code> <code>dict[str, ColumnDistributionType]</code> <p>Mapping of each score dimension name to its classification.</p> <code>distributions</code> <code>dict[str, CategoricalDistribution | NumericalDistribution | MissingValue]</code> <p>Mapping of each score dimension name to its computed distribution statistics.</p> <code>histograms</code> <code>dict[str, CategoricalHistogramData | MissingValue]</code> <p>Mapping of each score dimension name to its histogram data.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_profilers.JudgeScoreProfilerConfig","title":"<code>JudgeScoreProfilerConfig</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for the LLM-as-a-judge score profiler.</p> <p>Attributes:</p> Name Type Description <code>model_alias</code> <code>str</code> <p>Alias of the LLM model to use for generating score distribution summaries. Must match a model alias defined in the Data Designer configuration.</p> <code>summary_score_sample_size</code> <code>int | None</code> <p>Number of score samples to include when prompting the LLM to generate summaries. Larger sample sizes provide more context but increase token usage. Must be at least 1. Defaults to 20.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_profilers.JudgeScoreProfilerResults","title":"<code>JudgeScoreProfilerResults</code>","text":"<p>               Bases: <code>ColumnProfilerResults</code></p> <p>Container for complete judge score profiler analysis results.</p> <p>Attributes:</p> Name Type Description <code>column_name</code> <code>str</code> <p>Name of the judge column that was profiled.</p> <code>summaries</code> <code>dict[str, JudgeScoreSummary]</code> <p>Mapping of each score dimension name to its LLM-generated summary.</p> <code>score_distributions</code> <code>JudgeScoreDistributions | MissingValue</code> <p>Complete distribution analysis across all score dimensions.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_profilers.JudgeScoreSample","title":"<code>JudgeScoreSample</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for a single judge score and its associated reasoning.</p> <p>Stores a paired score-reasoning sample extracted from an LLM-as-a-judge column. Used when generating summaries to provide the LLM with examples of scoring patterns.</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>int | str</code> <p>The score value assigned by the judge. Can be numeric (int) or categorical (str).</p> <code>reasoning</code> <code>str</code> <p>The reasoning or explanation provided by the judge for this score.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.column_profilers.JudgeScoreSummary","title":"<code>JudgeScoreSummary</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for an LLM-generated summary of a judge score dimension.</p> <p>Stores the natural language summary and sample data for a single score dimension generated by the judge score profiler. The summary is created by an LLM analyzing the distribution and patterns in the score-reasoning pairs.</p> <p>Attributes:</p> Name Type Description <code>score_name</code> <code>str</code> <p>Name of the score dimension being summarized (e.g., \"relevance\", \"fluency\").</p> <code>summary</code> <code>str</code> <p>LLM-generated natural language summary describing the scoring patterns, distribution characteristics, and notable trends for this score dimension.</p> <code>score_samples</code> <code>list[JudgeScoreSample]</code> <p>List of score-reasoning pairs that were used to generate the summary. These are the examples of the scoring behavior that were used to generate the summary.</p>"},{"location":"code_reference/analysis/#dataset-profiler","title":"Dataset Profiler","text":"<p>The DatasetProfilerResults class contains complete profiling results for a generated dataset. It aggregates column-level statistics, metadata, and profiler results, and provides methods to:</p> <ul> <li>Compute dataset-level metrics (completion percentage, column type summary)</li> <li>Filter statistics by column type</li> <li>Generate formatted analysis reports via the <code>to_report()</code> method</li> </ul> <p>Reports can be displayed in the console or exported to HTML/SVG formats.</p> <p>Classes:</p> Name Description <code>DatasetProfilerResults</code> <p>Container for complete dataset profiling and analysis results.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.dataset_profiler.DatasetProfilerResults","title":"<code>DatasetProfilerResults</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for complete dataset profiling and analysis results.</p> <p>Stores profiling results for a generated dataset, including statistics for all columns, dataset-level metadata, and optional advanced profiler results. Provides methods for computing derived metrics and generating formatted reports.</p> <p>Attributes:</p> Name Type Description <code>num_records</code> <code>int</code> <p>Actual number of records successfully generated in the dataset.</p> <code>target_num_records</code> <code>int</code> <p>Target number of records that were requested to be generated.</p> <code>column_statistics</code> <code>list[Annotated[ColumnStatisticsT, Field(discriminator='column_type')]]</code> <p>List of statistics objects for all columns in the dataset. Each column has statistics appropriate to its type. Must contain at least one column.</p> <code>side_effect_column_names</code> <code>list[str] | None</code> <p>Column names that were generated as side effects of other columns.</p> <code>column_profiles</code> <code>list[ColumnProfilerResultsT] | None</code> <p>Column profiler results for specific columns when configured.</p> <p>Methods:</p> Name Description <code>get_column_statistics_by_type</code> <p>Filters column statistics to return only those of the specified type.</p> <code>to_report</code> <p>Generate and print an analysis report based on the dataset profiling results.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.dataset_profiler.DatasetProfilerResults.column_types","title":"<code>column_types</code>  <code>cached</code> <code>property</code>","text":"<p>Returns a sorted list of unique column types present in the dataset.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.dataset_profiler.DatasetProfilerResults.percent_complete","title":"<code>percent_complete</code>  <code>property</code>","text":"<p>Returns the completion percentage of the dataset.</p>"},{"location":"code_reference/analysis/#data_designer.config.analysis.dataset_profiler.DatasetProfilerResults.get_column_statistics_by_type","title":"<code>get_column_statistics_by_type(column_type)</code>","text":"<p>Filters column statistics to return only those of the specified type.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/analysis/dataset_profiler.py</code> <pre><code>def get_column_statistics_by_type(self, column_type: DataDesignerColumnType) -&gt; list[ColumnStatisticsT]:\n    \"\"\"Filters column statistics to return only those of the specified type.\"\"\"\n    return [c for c in self.column_statistics if c.column_type == column_type]\n</code></pre>"},{"location":"code_reference/analysis/#data_designer.config.analysis.dataset_profiler.DatasetProfilerResults.to_report","title":"<code>to_report(save_path=None, include_sections=None)</code>","text":"<p>Generate and print an analysis report based on the dataset profiling results.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str | Path | None</code> <p>Optional path to save the report. If provided, the report will be saved   as either HTML (.html) or SVG (.svg) format. If None, the report will   only be displayed in the console.</p> <code>None</code> <code>include_sections</code> <code>list[ReportSection | DataDesignerColumnType] | None</code> <p>Optional list of sections to include in the report. Choices are   any DataDesignerColumnType, \"overview\" (the dataset overview section),   and \"column_profilers\" (all column profilers in one section). If None,   all sections will be included.</p> <code>None</code> Source code in <code>packages/data-designer-config/src/data_designer/config/analysis/dataset_profiler.py</code> <pre><code>def to_report(\n    self,\n    save_path: str | Path | None = None,\n    include_sections: list[ReportSection | DataDesignerColumnType] | None = None,\n) -&gt; None:\n    \"\"\"Generate and print an analysis report based on the dataset profiling results.\n\n    Args:\n        save_path: Optional path to save the report. If provided, the report will be saved\n              as either HTML (.html) or SVG (.svg) format. If None, the report will\n              only be displayed in the console.\n        include_sections: Optional list of sections to include in the report. Choices are\n              any DataDesignerColumnType, \"overview\" (the dataset overview section),\n              and \"column_profilers\" (all column profilers in one section). If None,\n              all sections will be included.\n    \"\"\"\n    generate_analysis_report(self, save_path, include_sections=include_sections)\n</code></pre>"},{"location":"code_reference/column_configs/","title":"Column Configurations","text":"<p>The <code>column_configs</code> module defines configuration objects for all Data Designer column types. Each configuration inherits from SingleColumnConfig, which provides shared arguments like the column <code>name</code>, whether to <code>drop</code> the column after generation, and the <code>column_type</code>.</p> <p><code>column_type</code> is a discriminator field</p> <p>The <code>column_type</code> argument is used to identify column types when deserializing the Data Designer Config from JSON/YAML. It acts as the discriminator in a discriminated union, allowing Pydantic to automatically determine which column configuration class to instantiate.</p> <p>Classes:</p> Name Description <code>CustomColumnConfig</code> <p>Configuration for custom user-defined column generators.</p> <code>EmbeddingColumnConfig</code> <p>Configuration for embedding generation columns.</p> <code>ExpressionColumnConfig</code> <p>Configuration for derived columns using Jinja2 expressions.</p> <code>GenerationStrategy</code> <p>Strategy for custom column generation.</p> <code>LLMCodeColumnConfig</code> <p>Configuration for code generation columns using Large Language Models.</p> <code>LLMJudgeColumnConfig</code> <p>Configuration for LLM-as-a-judge quality assessment and scoring columns.</p> <code>LLMStructuredColumnConfig</code> <p>Configuration for structured JSON generation columns using Large Language Models.</p> <code>LLMTextColumnConfig</code> <p>Configuration for text generation columns using Large Language Models.</p> <code>SamplerColumnConfig</code> <p>Configuration for columns generated using numerical samplers.</p> <code>Score</code> <p>Configuration for a \"score\" in an LLM judge evaluation.</p> <code>SeedDatasetColumnConfig</code> <p>Configuration for columns sourced from seed datasets.</p> <code>ValidationColumnConfig</code> <p>Configuration for validation columns that validate existing columns.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.CustomColumnConfig","title":"<code>CustomColumnConfig</code>","text":"<p>               Bases: <code>SingleColumnConfig</code></p> <p>Configuration for custom user-defined column generators.</p> <p>Custom columns allow users to provide their own generation logic via a callable function decorated with <code>@custom_column_generator</code>. Two strategies are supported: cell_by_cell (default, row-based) and full_column (batch-based with DataFrame access).</p> <p>Attributes:</p> Name Type Description <code>generator_function</code> <code>Any</code> <p>A callable decorated with @custom_column_generator.</p> <code>generation_strategy</code> <code>GenerationStrategy</code> <p>\"cell_by_cell\" (row-based) or \"full_column\" (batch-based).</p> <code>generator_params</code> <code>BaseModel | None</code> <p>Optional typed configuration object (Pydantic BaseModel) passed as the second argument to the generator function.</p> <code>column_type</code> <code>Literal['custom']</code> <p>Discriminator field, always \"custom\" for this configuration type.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.CustomColumnConfig.model_aliases","title":"<code>model_aliases</code>  <code>property</code>","text":"<p>Returns model aliases for LLM access and health checks (from decorator metadata).</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.CustomColumnConfig.required_columns","title":"<code>required_columns</code>  <code>property</code>","text":"<p>Returns the columns required for custom generation (from decorator metadata).</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.CustomColumnConfig.side_effect_columns","title":"<code>side_effect_columns</code>  <code>property</code>","text":"<p>Returns additional columns created by this generator (from decorator metadata).</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.EmbeddingColumnConfig","title":"<code>EmbeddingColumnConfig</code>","text":"<p>               Bases: <code>SingleColumnConfig</code></p> <p>Configuration for embedding generation columns.</p> <p>Embedding columns generate embeddings for text input using a specified model.</p> <p>Attributes:</p> Name Type Description <code>target_column</code> <code>str</code> <p>The column to generate embeddings for. The column could be a single text string or a list of text strings in stringified JSON format. If it is a list of text strings in stringified JSON format, the embeddings will be generated for each text string.</p> <code>model_alias</code> <code>str</code> <p>The model to use for embedding generation.</p> <code>column_type</code> <code>Literal['embedding']</code> <p>Discriminator field, always \"embedding\" for this configuration type.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.ExpressionColumnConfig","title":"<code>ExpressionColumnConfig</code>","text":"<p>               Bases: <code>SingleColumnConfig</code></p> <p>Configuration for derived columns using Jinja2 expressions.</p> <p>Expression columns compute values by evaluating Jinja2 templates that reference other columns. Useful for transformations, concatenations, conditional logic, and derived features without requiring LLM generation. The expression is evaluated row-by-row.</p> <p>Attributes:</p> Name Type Description <code>expr</code> <code>str</code> <p>Jinja2 expression to evaluate. Can reference other column values using {{ column_name }} syntax. Supports filters, conditionals, and arithmetic. Must be a valid, non-empty Jinja2 template.</p> <code>dtype</code> <code>Literal['int', 'float', 'str', 'bool']</code> <p>Data type to cast the result to. Must be one of \"int\", \"float\", \"str\", or \"bool\". Defaults to \"str\". Type conversion is applied after expression evaluation.</p> <code>column_type</code> <code>Literal['expression']</code> <p>Discriminator field, always \"expression\" for this configuration type.</p> <p>Methods:</p> Name Description <code>assert_expression_valid_jinja</code> <p>Validate that the expression is a valid, non-empty Jinja2 template.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.ExpressionColumnConfig.required_columns","title":"<code>required_columns</code>  <code>property</code>","text":"<p>Returns the columns referenced in the expression template.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.ExpressionColumnConfig.assert_expression_valid_jinja","title":"<code>assert_expression_valid_jinja()</code>","text":"<p>Validate that the expression is a valid, non-empty Jinja2 template.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validated instance.</p> <p>Raises:</p> Type Description <code>InvalidConfigError</code> <p>If expression is empty or contains invalid Jinja2 syntax.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/column_configs.py</code> <pre><code>@model_validator(mode=\"after\")\ndef assert_expression_valid_jinja(self) -&gt; Self:\n    \"\"\"Validate that the expression is a valid, non-empty Jinja2 template.\n\n    Returns:\n        The validated instance.\n\n    Raises:\n        InvalidConfigError: If expression is empty or contains invalid Jinja2 syntax.\n    \"\"\"\n    if not self.expr.strip():\n        raise InvalidConfigError(\n            f\"\ud83d\uded1 Expression column '{self.name}' has an empty or whitespace-only expression. \"\n            f\"Please provide a valid Jinja2 expression (e.g., '{{ column_name }}' or '{{ col1 }} + {{ col2 }}') \"\n            \"or remove this column if not needed.\"\n        )\n    assert_valid_jinja2_template(self.expr)\n    return self\n</code></pre>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.GenerationStrategy","title":"<code>GenerationStrategy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Strategy for custom column generation.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMCodeColumnConfig","title":"<code>LLMCodeColumnConfig</code>","text":"<p>               Bases: <code>LLMTextColumnConfig</code></p> <p>Configuration for code generation columns using Large Language Models.</p> <p>Extends LLMTextColumnConfig to generate code snippets in specific programming languages or SQL dialects. The generated code is automatically extracted from markdown code blocks for the specified language. Inherits all prompt templating capabilities from LLMTextColumnConfig.</p> <p>Attributes:</p> Name Type Description <code>code_lang</code> <code>CodeLang</code> <p>Programming language or SQL dialect for code generation. Supported values include: \"python\", \"javascript\", \"typescript\", \"java\", \"kotlin\", \"go\", \"rust\", \"ruby\", \"scala\", \"swift\", \"sql:sqlite\", \"sql:postgres\", \"sql:mysql\", \"sql:tsql\", \"sql:bigquery\", \"sql:ansi\". See CodeLang enum for complete list.</p> <code>column_type</code> <code>Literal['llm-code']</code> <p>Discriminator field, always \"llm-code\" for this configuration type.</p> Inherited Attributes <p>prompt: Prompt template for code generation (supports Jinja2). model_alias: Alias of the model configuration to use. system_prompt: Optional system prompt (supports Jinja2). multi_modal_context: Optional image contexts for multi-modal generation. tool_alias: Optional tool configuration alias for MCP tool calls. with_trace: If True, creates a <code>{column_name}__trace</code> column with message history. extract_reasoning_content: If True, creates a <code>{column_name}__reasoning_content</code>     column containing the reasoning content from the final assistant response.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMJudgeColumnConfig","title":"<code>LLMJudgeColumnConfig</code>","text":"<p>               Bases: <code>LLMTextColumnConfig</code></p> <p>Configuration for LLM-as-a-judge quality assessment and scoring columns.</p> <p>Extends LLMTextColumnConfig to create judge columns that evaluate and score other generated content based on the defined criteria. Useful for quality assessment, preference ranking, and multi-dimensional evaluation of generated data. Inherits prompt templating capabilities from LLMTextColumnConfig.</p> <p>Attributes:</p> Name Type Description <code>scores</code> <code>list[Score]</code> <p>List of Score objects defining the evaluation dimensions. Each score represents a different aspect to evaluate (e.g., accuracy, relevance, fluency). Must contain at least one score.</p> <code>column_type</code> <code>Literal['llm-judge']</code> <p>Discriminator field, always \"llm-judge\" for this configuration type.</p> Inherited Attributes <p>prompt: Prompt template for the judge evaluation (supports Jinja2). model_alias: Alias of the model configuration to use. system_prompt: Optional system prompt (supports Jinja2). multi_modal_context: Optional image contexts for multi-modal generation. tool_alias: Optional tool configuration alias for MCP tool calls. with_trace: If True, creates a <code>{column_name}__trace</code> column with message history. extract_reasoning_content: If True, creates a <code>{column_name}__reasoning_content</code>     column containing the reasoning content from the final assistant response.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMStructuredColumnConfig","title":"<code>LLMStructuredColumnConfig</code>","text":"<p>               Bases: <code>LLMTextColumnConfig</code></p> <p>Configuration for structured JSON generation columns using Large Language Models.</p> <p>Extends LLMTextColumnConfig to generate structured data conforming to a specified schema. Uses JSON schema or Pydantic models to define the expected output structure, enabling type-safe and validated structured output generation. Inherits prompt templating capabilities from LLMTextColumnConfig.</p> <p>Attributes:</p> Name Type Description <code>output_format</code> <code>dict | type[BaseModel]</code> <p>The schema defining the expected output structure. Can be either: - A Pydantic BaseModel class (recommended) - A JSON schema dictionary</p> <code>column_type</code> <code>Literal['llm-structured']</code> <p>Discriminator field, always \"llm-structured\" for this configuration type.</p> Inherited Attributes <p>prompt: Prompt template for structured generation (supports Jinja2). model_alias: Alias of the model configuration to use. system_prompt: Optional system prompt (supports Jinja2). multi_modal_context: Optional image contexts for multi-modal generation. tool_alias: Optional tool configuration alias for MCP tool calls. with_trace: If True, creates a <code>{column_name}__trace</code> column with message history. extract_reasoning_content: If True, creates a <code>{column_name}__reasoning_content</code>     column containing the reasoning content from the final assistant response.</p> <p>Methods:</p> Name Description <code>validate_output_format</code> <p>Convert Pydantic model to JSON schema if needed.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMStructuredColumnConfig.validate_output_format","title":"<code>validate_output_format()</code>","text":"<p>Convert Pydantic model to JSON schema if needed.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validated instance with output_format as a JSON schema dict.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/column_configs.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_output_format(self) -&gt; Self:\n    \"\"\"Convert Pydantic model to JSON schema if needed.\n\n    Returns:\n        The validated instance with output_format as a JSON schema dict.\n    \"\"\"\n    if not isinstance(self.output_format, dict) and issubclass(self.output_format, BaseModel):\n        self.output_format = self.output_format.model_json_schema()\n    return self\n</code></pre>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMTextColumnConfig","title":"<code>LLMTextColumnConfig</code>","text":"<p>               Bases: <code>SingleColumnConfig</code></p> <p>Configuration for text generation columns using Large Language Models.</p> <p>LLM text columns generate free-form text content using language models via LiteLLM. Prompts support Jinja2 templating to reference values from other columns, enabling context-aware generation. The generated text can optionally include message traces capturing the full conversation history.</p> <p>Attributes:</p> Name Type Description <code>prompt</code> <code>str</code> <p>Prompt template for text generation. Supports Jinja2 syntax to reference other columns (e.g., \"Write a story about {{ character_name }}\"). Must be a valid Jinja2 template.</p> <code>model_alias</code> <code>str</code> <p>Alias of the model configuration to use for generation. Must match a model alias defined when initializing the DataDesignerConfigBuilder.</p> <code>system_prompt</code> <code>str | None</code> <p>Optional system prompt to set model behavior and constraints. Also supports Jinja2 templating. If provided, must be a valid Jinja2 template. Do not put any output parsing instructions in the system prompt. Instead, use the appropriate column type for the output you want to generate - e.g., <code>LLMStructuredColumnConfig</code> for structured output, <code>LLMCodeColumnConfig</code> for code.</p> <code>multi_modal_context</code> <code>list[ImageContext] | None</code> <p>Optional list of image contexts for multi-modal generation. Enables vision-capable models to generate text based on image inputs.</p> <code>tool_alias</code> <code>str | None</code> <p>Optional alias of the tool configuration to use for MCP tool calls. Must match a tool alias defined when initializing the DataDesignerConfigBuilder. When provided, the model may call permitted tools during generation.</p> <code>with_trace</code> <code>TraceType</code> <p>Specifies what trace information to capture in a <code>{column_name}__trace</code> column. Options are: - <code>TraceType.NONE</code> (default): No trace is captured. - <code>TraceType.LAST_MESSAGE</code>: Only the final assistant message is captured. - <code>TraceType.ALL_MESSAGES</code>: Full conversation history (system/user/assistant/tool).</p> <code>extract_reasoning_content</code> <code>bool</code> <p>If True, creates a <code>{column_name}__reasoning_content</code> column containing only the reasoning_content from the final assistant response. This is useful for models that expose chain-of-thought reasoning separately from the main response. Defaults to False.</p> <code>column_type</code> <code>Literal['llm-text']</code> <p>Discriminator field, always \"llm-text\" for this configuration type.</p> <p>Methods:</p> Name Description <code>assert_prompt_valid_jinja</code> <p>Validate that prompt and system_prompt are valid Jinja2 templates.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMTextColumnConfig.required_columns","title":"<code>required_columns</code>  <code>property</code>","text":"<p>Get columns referenced in the prompt and system_prompt templates.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of unique column names referenced in Jinja2 templates.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMTextColumnConfig.side_effect_columns","title":"<code>side_effect_columns</code>  <code>property</code>","text":"<p>Returns side-effect columns that may be generated alongside the main column.</p> <p>Side-effect columns include: - <code>{name}__trace</code>: Generated when <code>with_trace</code> is not <code>TraceType.NONE</code> on the column   config. - <code>{name}__reasoning_content</code>: Generated when <code>extract_reasoning_content=True</code>.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of side-effect column names.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.LLMTextColumnConfig.assert_prompt_valid_jinja","title":"<code>assert_prompt_valid_jinja()</code>","text":"<p>Validate that prompt and system_prompt are valid Jinja2 templates.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validated instance.</p> <p>Raises:</p> Type Description <code>InvalidConfigError</code> <p>If prompt or system_prompt contains invalid Jinja2 syntax.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/column_configs.py</code> <pre><code>@model_validator(mode=\"after\")\ndef assert_prompt_valid_jinja(self) -&gt; Self:\n    \"\"\"Validate that prompt and system_prompt are valid Jinja2 templates.\n\n    Returns:\n        The validated instance.\n\n    Raises:\n        InvalidConfigError: If prompt or system_prompt contains invalid Jinja2 syntax.\n    \"\"\"\n    assert_valid_jinja2_template(self.prompt)\n    if self.system_prompt:\n        assert_valid_jinja2_template(self.system_prompt)\n    return self\n</code></pre>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.SamplerColumnConfig","title":"<code>SamplerColumnConfig</code>","text":"<p>               Bases: <code>SingleColumnConfig</code></p> <p>Configuration for columns generated using numerical samplers.</p> <p>Sampler columns provide efficient data generation using numerical samplers for common data types and distributions. Supported samplers include UUID generation, datetime/timedelta sampling, person generation, category / subcategory sampling, and various statistical distributions (uniform, gaussian, binomial, poisson, scipy).</p> <p>Attributes:</p> Name Type Description <code>sampler_type</code> <code>SamplerType</code> <p>Type of sampler to use. Available types include: \"uuid\", \"category\", \"subcategory\", \"uniform\", \"gaussian\", \"bernoulli\", \"bernoulli_mixture\", \"binomial\", \"poisson\", \"scipy\", \"person\", \"datetime\", \"timedelta\".</p> <code>params</code> <code>Annotated[SamplerParamsT, Discriminator('sampler_type')]</code> <p>Parameters specific to the chosen sampler type. Type varies based on the <code>sampler_type</code> (e.g., <code>CategorySamplerParams</code>, <code>UniformSamplerParams</code>, <code>PersonSamplerParams</code>).</p> <code>conditional_params</code> <code>dict[str, Annotated[SamplerParamsT, Discriminator('sampler_type')]]</code> <p>Optional dictionary for conditional parameters. The dict keys are the conditions that must be met (e.g., \"age &gt; 21\") for the conditional parameters to be used. The values of dict are the parameters to use when the condition is met.</p> <code>convert_to</code> <code>str | None</code> <p>Optional type conversion to apply after sampling. Must be one of \"float\", \"int\", or \"str\". Useful for converting numerical samples to strings or other types.</p> <code>column_type</code> <code>Literal['sampler']</code> <p>Discriminator field, always \"sampler\" for this configuration type.</p> <p>Displaying available samplers and their parameters</p> <p>The config builder has an <code>info</code> attribute that can be used to display the available samplers and their parameters: <pre><code>config_builder.info.display(\"samplers\")\n</code></pre></p> <p>Methods:</p> Name Description <code>inject_sampler_type_into_params</code> <p>Inject sampler_type into params dict to enable discriminated union resolution.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.SamplerColumnConfig.inject_sampler_type_into_params","title":"<code>inject_sampler_type_into_params(data)</code>  <code>classmethod</code>","text":"<p>Inject sampler_type into params dict to enable discriminated union resolution.</p> <p>This allows users to pass params as a simple dict without the sampler_type field, which will be automatically added based on the outer sampler_type field.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/column_configs.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef inject_sampler_type_into_params(cls, data: dict) -&gt; dict:\n    \"\"\"Inject sampler_type into params dict to enable discriminated union resolution.\n\n    This allows users to pass params as a simple dict without the sampler_type field,\n    which will be automatically added based on the outer sampler_type field.\n    \"\"\"\n    if isinstance(data, dict):\n        sampler_type = data.get(\"sampler_type\")\n        params = data.get(\"params\")\n\n        # If params is a dict and doesn't have sampler_type, inject it\n        if sampler_type and isinstance(params, dict) and \"sampler_type\" not in params:\n            data[\"params\"] = {\"sampler_type\": sampler_type, **params}\n\n        # Handle conditional_params similarly\n        conditional_params = data.get(\"conditional_params\")\n        if conditional_params and isinstance(conditional_params, dict):\n            for condition, cond_params in conditional_params.items():\n                if isinstance(cond_params, dict) and \"sampler_type\" not in cond_params:\n                    data[\"conditional_params\"][condition] = {\"sampler_type\": sampler_type, **cond_params}\n\n    return data\n</code></pre>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.Score","title":"<code>Score</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for a \"score\" in an LLM judge evaluation.</p> <p>Defines a single scoring criterion with its possible values and descriptions. Multiple Score objects can be combined in an LLMJudgeColumnConfig to create multi-dimensional quality assessments.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>A clear, concise name for this scoring dimension (e.g., \"Relevance\", \"Fluency\").</p> <code>description</code> <code>str</code> <p>An informative and detailed assessment guide explaining how to evaluate this dimension. Should provide clear criteria for scoring.</p> <code>options</code> <code>dict[int | str, str]</code> <p>Dictionary mapping score values to their descriptions. Keys can be integers (e.g., 1-5 scale) or strings (e.g., \"Poor\", \"Good\", \"Excellent\"). Values are descriptions explaining what each score level means.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.SeedDatasetColumnConfig","title":"<code>SeedDatasetColumnConfig</code>","text":"<p>               Bases: <code>SingleColumnConfig</code></p> <p>Configuration for columns sourced from seed datasets.</p> <p>This config marks columns that come from seed data. It is typically created automatically when calling <code>with_seed_dataset()</code> on the builder, rather than being instantiated directly by users.</p> <p>Attributes:</p> Name Type Description <code>column_type</code> <code>Literal['seed-dataset']</code> <p>Discriminator field, always \"seed-dataset\" for this configuration type.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.ValidationColumnConfig","title":"<code>ValidationColumnConfig</code>","text":"<p>               Bases: <code>SingleColumnConfig</code></p> <p>Configuration for validation columns that validate existing columns.</p> <p>Validation columns execute validation logic against specified target columns and return structured results indicating pass/fail status with validation details. Supports multiple validation strategies: code execution (Python/SQL), local callable functions (library only), and remote HTTP endpoints.</p> <p>Attributes:</p> Name Type Description <code>target_columns</code> <code>list[str]</code> <p>List of column names to validate. These columns are passed to the validator for validation. All target columns must exist in the dataset before validation runs.</p> <code>validator_type</code> <code>ValidatorType</code> <p>The type of validator to use. Options: - \"code\": Execute code (Python or SQL) for validation. The code receives a   DataFrame with target columns and must return a DataFrame with validation results. - \"local_callable\": Call a local Python function with the data. Only supported   when running DataDesigner locally. - \"remote\": Send data to a remote HTTP endpoint for validation. Useful for</p> <code>validator_params</code> <code>ValidatorParamsT</code> <p>Parameters specific to the validator type. Type varies by validator: - CodeValidatorParams: Specifies code language (python or SQL dialect like   \"sql:postgres\", \"sql:mysql\"). - LocalCallableValidatorParams: Provides validation function (Callable[[pd.DataFrame],   pd.DataFrame]) and optional output schema for validation results. - RemoteValidatorParams: Configures endpoint URL, HTTP timeout, retry behavior   (max_retries, retry_backoff), and parallel request limits (max_parallel_requests).</p> <code>batch_size</code> <code>int</code> <p>Number of records to process in each validation batch. Defaults to 10. Larger batches are more efficient but use more memory. Adjust based on validator complexity and available resources.</p> <code>column_type</code> <code>Literal['validation']</code> <p>Discriminator field, always \"validation\" for this configuration type.</p>"},{"location":"code_reference/column_configs/#data_designer.config.column_configs.ValidationColumnConfig.required_columns","title":"<code>required_columns</code>  <code>property</code>","text":"<p>Returns the columns that need to be validated.</p>"},{"location":"code_reference/config_builder/","title":"Data Designer's Config Builder","text":"<p>The <code>config_builder</code> module provides a high-level interface for constructing Data Designer configurations through the DataDesignerConfigBuilder class, enabling programmatic creation of DataDesignerConfig objects by incrementally adding column configurations, constraints, processors, and profilers.</p> <p>You can use the builder to create Data Designer configurations from scratch or from existing configurations stored in YAML/JSON files via <code>from_config()</code>. The builder includes validation capabilities to catch configuration errors early and can work with seed datasets from local sources or external datastores. Once configured, use <code>build()</code> to generate the final configuration object or <code>write_config()</code> to serialize it to disk.</p> <p>Model configs are required</p> <p>DataDesignerConfigBuilder requires a list of model configurations at initialization. This tells the builder which model aliases can be referenced by LLM-generated columns (such as <code>LLMTextColumnConfig</code>, <code>LLMCodeColumnConfig</code>, <code>LLMStructuredColumnConfig</code>, and <code>LLMJudgeColumnConfig</code>). Each model configuration specifies the model alias, model provider, model ID, and inference parameters that will be used during data generation.</p> <p>Classes:</p> Name Description <code>BuilderConfig</code> <p>Configuration container for Data Designer builder.</p> <code>DataDesignerConfigBuilder</code> <p>Config builder for Data Designer configurations.</p>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.BuilderConfig","title":"<code>BuilderConfig</code>","text":"<p>               Bases: <code>ExportableConfigBase</code></p> <p>Configuration container for Data Designer builder.</p> <p>This class holds the main Data Designer configuration along with optional datastore settings needed for seed dataset operations.</p> <p>Attributes:</p> Name Type Description <code>data_designer</code> <code>DataDesignerConfig</code> <p>The main Data Designer configuration containing columns, constraints, profilers, and other settings.</p>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder","title":"<code>DataDesignerConfigBuilder(model_configs=None, tool_configs=None)</code>","text":"<p>Config builder for Data Designer configurations.</p> <p>This class provides a high-level interface for building Data Designer configurations.</p> <p>Initialize a new DataDesignerConfigBuilder instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_configs</code> <code>list[ModelConfig] | str | Path | None</code> <p>Model configurations. Can be: - None to use default model configurations in local mode - A list of ModelConfig objects - A string or Path to a model configuration file</p> <code>None</code> <code>tool_configs</code> <code>list[ToolConfig] | None</code> <p>Tool configurations for MCP tool calling. Can be: - None if no tool configs are needed - A list of ToolConfig objects</p> <code>None</code> <p>Methods:</p> Name Description <code>add_column</code> <p>Add a Data Designer column configuration to the current Data Designer configuration.</p> <code>add_constraint</code> <p>Add a constraint to the current Data Designer configuration.</p> <code>add_model_config</code> <p>Add a model configuration to the current Data Designer configuration.</p> <code>add_processor</code> <p>Add a processor to the current Data Designer configuration.</p> <code>add_profiler</code> <p>Add a profiler to the current Data Designer configuration.</p> <code>add_tool_config</code> <p>Add a tool configuration to the current Data Designer configuration.</p> <code>build</code> <p>Build a DataDesignerConfig instance based on the current builder configuration.</p> <code>delete_column</code> <p>Delete the column with the given name.</p> <code>delete_constraints</code> <p>Delete all constraints for the given target column.</p> <code>delete_model_config</code> <p>Delete a model configuration from the current Data Designer configuration by alias.</p> <code>delete_tool_config</code> <p>Delete a tool configuration from the current Data Designer configuration by alias.</p> <code>from_config</code> <p>Create a DataDesignerConfigBuilder from an existing configuration.</p> <code>get_builder_config</code> <p>Get the builder config for the current Data Designer configuration.</p> <code>get_column_config</code> <p>Get a column configuration by name.</p> <code>get_column_configs</code> <p>Get all column configurations.</p> <code>get_columns_excluding_type</code> <p>Get all column configurations excluding the specified type.</p> <code>get_columns_of_type</code> <p>Get all column configurations of the specified type.</p> <code>get_constraints</code> <p>Get all constraints for the given target column.</p> <code>get_processor_configs</code> <p>Get processor configuration objects.</p> <code>get_profilers</code> <p>Get all profilers.</p> <code>get_seed_config</code> <p>Get the seed config for the current Data Designer configuration.</p> <code>get_tool_config</code> <p>Get a tool configuration by alias.</p> <code>num_columns_of_type</code> <p>Get the count of columns of the specified type.</p> <code>with_seed_dataset</code> <p>Add a seed dataset to the current Data Designer configuration.</p> <code>write_config</code> <p>Write the current configuration to a file.</p> <p>Attributes:</p> Name Type Description <code>allowed_references</code> <code>list[str]</code> <p>Get all referenceable variables allowed in prompt templates and expressions.</p> <code>info</code> <code>ConfigBuilderInfo</code> <p>Get the ConfigBuilderInfo object for this builder.</p> <code>model_configs</code> <code>list[ModelConfig]</code> <p>Get the model configurations for this builder.</p> <code>tool_configs</code> <code>list[ToolConfig]</code> <p>Get the tool configurations for this builder.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def __init__(\n    self,\n    model_configs: list[ModelConfig] | str | Path | None = None,\n    tool_configs: list[ToolConfig] | None = None,\n):\n    \"\"\"Initialize a new DataDesignerConfigBuilder instance.\n\n    Args:\n        model_configs: Model configurations. Can be:\n            - None to use default model configurations in local mode\n            - A list of ModelConfig objects\n            - A string or Path to a model configuration file\n        tool_configs: Tool configurations for MCP tool calling. Can be:\n            - None if no tool configs are needed\n            - A list of ToolConfig objects\n    \"\"\"\n    self._column_configs = {}\n    self._model_configs = _load_model_configs(model_configs)\n    self._tool_configs: list[ToolConfig] = tool_configs or []\n    self._processor_configs: list[ProcessorConfigT] = []\n    self._seed_config: SeedConfig | None = None\n    self._constraints: list[ColumnConstraintT] = []\n    self._profilers: list[ColumnProfilerConfigT] = []\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.allowed_references","title":"<code>allowed_references</code>  <code>property</code>","text":"<p>Get all referenceable variables allowed in prompt templates and expressions.</p> <p>This includes all column names and their side effect columns that can be referenced in prompt templates and expressions within the configuration.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of variable names that can be referenced in templates and expressions.</p>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.info","title":"<code>info</code>  <code>property</code>","text":"<p>Get the ConfigBuilderInfo object for this builder.</p> <p>Returns:</p> Type Description <code>ConfigBuilderInfo</code> <p>An object containing information about the configuration.</p>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.model_configs","title":"<code>model_configs</code>  <code>property</code>","text":"<p>Get the model configurations for this builder.</p> <p>Returns:</p> Type Description <code>list[ModelConfig]</code> <p>A list of ModelConfig objects used for data generation.</p>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.tool_configs","title":"<code>tool_configs</code>  <code>property</code>","text":"<p>Get the tool configurations for this builder.</p> <p>Returns:</p> Type Description <code>list[ToolConfig]</code> <p>A list of ToolConfig objects used for MCP tool calling.</p>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.add_column","title":"<code>add_column(column_config=None, *, name=None, column_type=None, **kwargs)</code>","text":"<p>Add a Data Designer column configuration to the current Data Designer configuration.</p> <p>If no column config object is provided, you must provide the <code>name</code>, <code>column_type</code>, and any additional keyword arguments that are required by the column config constructor.</p> <p>Parameters:</p> Name Type Description Default <code>column_config</code> <code>ColumnConfigT | None</code> <p>Data Designer column config object to add.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Name of the column to add. This is only used if <code>column_config</code> is not provided.</p> <code>None</code> <code>column_type</code> <code>DataDesignerColumnType | None</code> <p>Column type to add. This is only used if <code>column_config</code> is not provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the column constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> <p>Raises:</p> Type Description <code>BuilderConfigurationError</code> <p>If the column name collides with an existing seed dataset column.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def add_column(\n    self,\n    column_config: ColumnConfigT | None = None,\n    *,\n    name: str | None = None,\n    column_type: DataDesignerColumnType | None = None,\n    **kwargs,\n) -&gt; Self:\n    \"\"\"Add a Data Designer column configuration to the current Data Designer configuration.\n\n    If no column config object is provided, you must provide the `name`, `column_type`, and any\n    additional keyword arguments that are required by the column config constructor.\n\n    Args:\n        column_config: Data Designer column config object to add.\n        name: Name of the column to add. This is only used if `column_config` is not provided.\n        column_type: Column type to add. This is only used if `column_config` is not provided.\n        **kwargs: Additional keyword arguments to pass to the column constructor.\n\n    Returns:\n        The current Data Designer config builder instance.\n\n    Raises:\n        BuilderConfigurationError: If the column name collides with an existing seed dataset column.\n    \"\"\"\n    if column_config is None:\n        if name is None or column_type is None:\n            raise BuilderConfigurationError(\n                \"\ud83d\uded1 You must provide either a 'column_config' object or 'name' *and* 'column_type' \"\n                f\"with additional keyword arguments. You provided {column_config=}, {name=}, and {column_type=}.\"\n            )\n        column_config = get_column_config_from_kwargs(name=name, column_type=column_type, **kwargs)\n\n    allowed_column_configs = ColumnConfigT.__args__\n    if not any(isinstance(column_config, t) for t in allowed_column_configs):\n        raise InvalidColumnTypeError(\n            f\"\ud83d\uded1 Invalid column config object: '{column_config}'. Valid column config options are: \"\n            f\"{', '.join([t.__name__ for t in allowed_column_configs])}\"\n        )\n\n    self._column_configs[column_config.name] = column_config\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.add_constraint","title":"<code>add_constraint(constraint=None, *, constraint_type=None, **kwargs)</code>","text":"<p>Add a constraint to the current Data Designer configuration.</p> <p>Currently, constraints are only supported for numerical samplers.</p> <p>You can either provide a constraint object directly, or provide a constraint type and additional keyword arguments to construct the constraint object. Valid constraint types are:     - \"scalar_inequality\": Constraint between a column and a scalar value.     - \"column_inequality\": Constraint between two columns.</p> <p>Parameters:</p> Name Type Description Default <code>constraint</code> <code>ColumnConstraintT | None</code> <p>Constraint object to add.</p> <code>None</code> <code>constraint_type</code> <code>ConstraintType | None</code> <p>Constraint type to add. Ignored when <code>constraint</code> is provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the constraint constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def add_constraint(\n    self,\n    constraint: ColumnConstraintT | None = None,\n    *,\n    constraint_type: ConstraintType | None = None,\n    **kwargs,\n) -&gt; Self:\n    \"\"\"Add a constraint to the current Data Designer configuration.\n\n    Currently, constraints are only supported for numerical samplers.\n\n    You can either provide a constraint object directly, or provide a constraint type and\n    additional keyword arguments to construct the constraint object. Valid constraint types are:\n        - \"scalar_inequality\": Constraint between a column and a scalar value.\n        - \"column_inequality\": Constraint between two columns.\n\n    Args:\n        constraint: Constraint object to add.\n        constraint_type: Constraint type to add. Ignored when `constraint` is provided.\n        **kwargs: Additional keyword arguments to pass to the constraint constructor.\n\n    Returns:\n        The current Data Designer config builder instance.\n    \"\"\"\n    if constraint is None:\n        if constraint_type is None:\n            raise BuilderConfigurationError(\n                \"\ud83d\uded1 You must provide either a 'constraint' object or 'constraint_type' \"\n                \"with additional keyword arguments.\"\n            )\n        try:\n            constraint_type = ConstraintType(constraint_type)\n        except Exception:\n            raise BuilderConfigurationError(\n                f\"\ud83d\uded1 Invalid constraint type: {constraint_type}. Valid options are: \"\n                f\"{', '.join([t.value for t in ConstraintType])}\"\n            )\n        if constraint_type == ConstraintType.SCALAR_INEQUALITY:\n            constraint = ScalarInequalityConstraint(**kwargs)\n        elif constraint_type == ConstraintType.COLUMN_INEQUALITY:\n            constraint = ColumnInequalityConstraint(**kwargs)\n\n    allowed_constraint_types = ColumnConstraintT.__args__\n    if not any(isinstance(constraint, t) for t in allowed_constraint_types):\n        raise BuilderConfigurationError(\n            \"\ud83d\uded1 Invalid constraint object. Valid constraint options are: \"\n            f\"{', '.join([t.__name__ for t in allowed_constraint_types])}\"\n        )\n\n    self._constraints.append(constraint)\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.add_model_config","title":"<code>add_model_config(model_config)</code>","text":"<p>Add a model configuration to the current Data Designer configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>ModelConfig</code> <p>The model configuration to add.</p> required Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def add_model_config(self, model_config: ModelConfig) -&gt; Self:\n    \"\"\"Add a model configuration to the current Data Designer configuration.\n\n    Args:\n        model_config: The model configuration to add.\n    \"\"\"\n    if model_config.alias in [mc.alias for mc in self._model_configs]:\n        raise BuilderConfigurationError(\n            f\"\ud83d\uded1 Model configuration with alias {model_config.alias} already exists. Please delete the existing model configuration or choose a different alias.\"\n        )\n    self._model_configs.append(model_config)\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.add_processor","title":"<code>add_processor(processor_config=None, *, processor_type=None, **kwargs)</code>","text":"<p>Add a processor to the current Data Designer configuration.</p> <p>You can either provide a processor config object directly, or provide a processor type and additional keyword arguments to construct the processor config object.</p> <p>Parameters:</p> Name Type Description Default <code>processor_config</code> <code>ProcessorConfigT | None</code> <p>The processor configuration object to add.</p> <code>None</code> <code>processor_type</code> <code>ProcessorType | None</code> <p>The type of processor to add.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the processor constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def add_processor(\n    self,\n    processor_config: ProcessorConfigT | None = None,\n    *,\n    processor_type: ProcessorType | None = None,\n    **kwargs,\n) -&gt; Self:\n    \"\"\"Add a processor to the current Data Designer configuration.\n\n    You can either provide a processor config object directly, or provide a processor type and\n    additional keyword arguments to construct the processor config object.\n\n    Args:\n        processor_config: The processor configuration object to add.\n        processor_type: The type of processor to add.\n        **kwargs: Additional keyword arguments to pass to the processor constructor.\n\n    Returns:\n        The current Data Designer config builder instance.\n    \"\"\"\n    if processor_config is None:\n        if processor_type is None:\n            raise BuilderConfigurationError(\n                \"\ud83d\uded1 You must provide either a 'processor_config' object or 'processor_type' \"\n                \"with additional keyword arguments.\"\n            )\n        processor_config = get_processor_config_from_kwargs(processor_type=processor_type, **kwargs)\n\n    # Checks elsewhere fail if DropColumnsProcessor drops a column but it is not marked for drop\n    if processor_config.processor_type == ProcessorType.DROP_COLUMNS:\n        for column in processor_config.column_names:\n            if column in self._column_configs:\n                self._column_configs[column].drop = True\n\n    self._processor_configs.append(processor_config)\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.add_profiler","title":"<code>add_profiler(profiler_config)</code>","text":"<p>Add a profiler to the current Data Designer configuration.</p> <p>Parameters:</p> Name Type Description Default <code>profiler_config</code> <code>ColumnProfilerConfigT</code> <p>The profiler configuration object to add.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> <p>Raises:</p> Type Description <code>BuilderConfigurationError</code> <p>If the profiler configuration is of an invalid type.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def add_profiler(self, profiler_config: ColumnProfilerConfigT) -&gt; Self:\n    \"\"\"Add a profiler to the current Data Designer configuration.\n\n    Args:\n        profiler_config: The profiler configuration object to add.\n\n    Returns:\n        The current Data Designer config builder instance.\n\n    Raises:\n        BuilderConfigurationError: If the profiler configuration is of an invalid type.\n    \"\"\"\n    if not isinstance(profiler_config, ColumnProfilerConfigT):\n        if hasattr(ColumnProfilerConfigT, \"__args__\"):\n            valid_options = \", \".join([t.__name__ for t in ColumnProfilerConfigT.__args__])\n        else:\n            valid_options = ColumnProfilerConfigT.__name__\n        raise BuilderConfigurationError(f\"\ud83d\uded1 Invalid profiler object. Valid profiler options are: {valid_options}\")\n    self._profilers.append(profiler_config)\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.add_tool_config","title":"<code>add_tool_config(tool_config)</code>","text":"<p>Add a tool configuration to the current Data Designer configuration.</p> <p>Parameters:</p> Name Type Description Default <code>tool_config</code> <code>ToolConfig</code> <p>The tool configuration to add.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> <p>Raises:</p> Type Description <code>BuilderConfigurationError</code> <p>If a tool configuration with the same alias already exists.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def add_tool_config(self, tool_config: ToolConfig) -&gt; Self:\n    \"\"\"Add a tool configuration to the current Data Designer configuration.\n\n    Args:\n        tool_config: The tool configuration to add.\n\n    Returns:\n        The current Data Designer config builder instance.\n\n    Raises:\n        BuilderConfigurationError: If a tool configuration with the same alias already exists.\n    \"\"\"\n    if tool_config.tool_alias in {tc.tool_alias for tc in self._tool_configs}:\n        raise BuilderConfigurationError(\n            f\"Tool configuration with alias {tool_config.tool_alias} already exists. \"\n            \"Please delete the existing tool configuration or choose a different alias.\"\n        )\n    self._tool_configs.append(tool_config)\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.build","title":"<code>build()</code>","text":"<p>Build a DataDesignerConfig instance based on the current builder configuration.</p> <p>Returns:</p> Type Description <code>DataDesignerConfig</code> <p>The current Data Designer config object.</p> <p>Raises:</p> Type Description <code>BuilderConfigurationError</code> <p>If any ToolConfig has duplicate tool names in its allow_tools list.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def build(self) -&gt; DataDesignerConfig:\n    \"\"\"Build a DataDesignerConfig instance based on the current builder configuration.\n\n    Returns:\n        The current Data Designer config object.\n\n    Raises:\n        BuilderConfigurationError: If any ToolConfig has duplicate tool names in its allow_tools list.\n    \"\"\"\n    self._validate_tool_configs_no_duplicates()\n    return DataDesignerConfig(\n        model_configs=self._model_configs,\n        tool_configs=self._tool_configs,\n        seed_config=self._seed_config,\n        columns=list(self._column_configs.values()),\n        constraints=self._constraints or None,\n        profilers=self._profilers or None,\n        processors=self._processor_configs or None,\n    )\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.delete_column","title":"<code>delete_column(column_name)</code>","text":"<p>Delete the column with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>Name of the column to delete.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> <p>Raises:</p> Type Description <code>BuilderConfigurationError</code> <p>If trying to delete a seed dataset column.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def delete_column(self, column_name: str) -&gt; Self:\n    \"\"\"Delete the column with the given name.\n\n    Args:\n        column_name: Name of the column to delete.\n\n    Returns:\n        The current Data Designer config builder instance.\n\n    Raises:\n        BuilderConfigurationError: If trying to delete a seed dataset column.\n    \"\"\"\n    if isinstance(self._column_configs.get(column_name), SeedDatasetColumnConfig):\n        raise BuilderConfigurationError(\"Seed columns cannot be deleted. Please update the seed dataset instead.\")\n    self._column_configs.pop(column_name, None)\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.delete_constraints","title":"<code>delete_constraints(target_column)</code>","text":"<p>Delete all constraints for the given target column.</p> <p>Parameters:</p> Name Type Description Default <code>target_column</code> <code>str</code> <p>Name of the column to remove constraints for.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def delete_constraints(self, target_column: str) -&gt; Self:\n    \"\"\"Delete all constraints for the given target column.\n\n    Args:\n        target_column: Name of the column to remove constraints for.\n\n    Returns:\n        The current Data Designer config builder instance.\n    \"\"\"\n    self._constraints = [c for c in self._constraints if c.target_column != target_column]\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.delete_model_config","title":"<code>delete_model_config(alias)</code>","text":"<p>Delete a model configuration from the current Data Designer configuration by alias.</p> <p>Parameters:</p> Name Type Description Default <code>alias</code> <code>str</code> <p>The alias of the model configuration to delete.</p> required Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def delete_model_config(self, alias: str) -&gt; Self:\n    \"\"\"Delete a model configuration from the current Data Designer configuration by alias.\n\n    Args:\n        alias: The alias of the model configuration to delete.\n    \"\"\"\n    self._model_configs = [mc for mc in self._model_configs if mc.alias != alias]\n    if len(self._model_configs) == 0:\n        logger.warning(\n            f\"\u26a0\ufe0f No model configurations found after deleting model configuration with alias {alias}. Please add a model configuration before building the configuration.\"\n        )\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.delete_tool_config","title":"<code>delete_tool_config(alias)</code>","text":"<p>Delete a tool configuration from the current Data Designer configuration by alias.</p> <p>Parameters:</p> Name Type Description Default <code>alias</code> <code>str</code> <p>The alias of the tool configuration to delete.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def delete_tool_config(self, alias: str) -&gt; Self:\n    \"\"\"Delete a tool configuration from the current Data Designer configuration by alias.\n\n    Args:\n        alias: The alias of the tool configuration to delete.\n\n    Returns:\n        The current Data Designer config builder instance.\n    \"\"\"\n    self._tool_configs = [tc for tc in self._tool_configs if tc.tool_alias != alias]\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create a DataDesignerConfigBuilder from an existing configuration.</p> <p>Accepts both the full <code>BuilderConfig</code> format (with a top-level <code>data_designer</code> key) and the shorthand <code>DataDesignerConfig</code> format (<code>columns</code>, <code>model_configs</code>, etc. at the top level). When the shorthand format is detected it is automatically normalized into a full <code>BuilderConfig</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict | str | Path | BuilderConfig</code> <p>Configuration source. Can be: - A dictionary containing the configuration - A string or Path to a local YAML/JSON configuration file - An HTTP(S) URL string to a YAML/JSON configuration file - A BuilderConfig object</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A new instance populated with the configuration from the provided source.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the config format is invalid.</p> <code>ValidationError</code> <p>If the builder config loaded from the config is invalid.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>@classmethod\ndef from_config(cls, config: dict | str | Path | BuilderConfig) -&gt; Self:\n    \"\"\"Create a DataDesignerConfigBuilder from an existing configuration.\n\n    Accepts both the full ``BuilderConfig`` format (with a top-level\n    ``data_designer`` key) and the shorthand ``DataDesignerConfig`` format\n    (``columns``, ``model_configs``, etc. at the top level). When the\n    shorthand format is detected it is automatically normalized into a\n    full ``BuilderConfig``.\n\n    Args:\n        config: Configuration source. Can be:\n            - A dictionary containing the configuration\n            - A string or Path to a local YAML/JSON configuration file\n            - An HTTP(S) URL string to a YAML/JSON configuration file\n            - A BuilderConfig object\n\n    Returns:\n        A new instance populated with the configuration from the provided source.\n\n    Raises:\n        ValueError: If the config format is invalid.\n        ValidationError: If the builder config loaded from the config is invalid.\n    \"\"\"\n    if isinstance(config, BuilderConfig):\n        builder_config = config\n    else:\n        json_config = json.loads(serialize_data(smart_load_yaml(config)))\n        # Normalize shorthand DataDesignerConfig into full BuilderConfig\n        if \"columns\" in json_config and \"data_designer\" not in json_config:\n            json_config = {\"data_designer\": json_config}\n        builder_config = BuilderConfig.model_validate(json_config)\n\n    builder = cls(\n        model_configs=builder_config.data_designer.model_configs,\n        tool_configs=builder_config.data_designer.tool_configs,\n    )\n    data_designer_config = builder_config.data_designer\n\n    for col in data_designer_config.columns:\n        builder.add_column(col)\n\n    for constraint in data_designer_config.constraints or []:\n        builder.add_constraint(constraint=constraint)\n\n    if (seed_config := data_designer_config.seed_config) is not None:\n        builder.with_seed_dataset(\n            seed_config.source,\n            sampling_strategy=seed_config.sampling_strategy,\n            selection_strategy=seed_config.selection_strategy,\n        )\n\n    return builder\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_builder_config","title":"<code>get_builder_config()</code>","text":"<p>Get the builder config for the current Data Designer configuration.</p> <p>Returns:</p> Type Description <code>BuilderConfig</code> <p>The builder config.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def get_builder_config(self) -&gt; BuilderConfig:\n    \"\"\"Get the builder config for the current Data Designer configuration.\n\n    Returns:\n        The builder config.\n    \"\"\"\n    return BuilderConfig(data_designer=self.build())\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_column_config","title":"<code>get_column_config(name)</code>","text":"<p>Get a column configuration by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the column to retrieve the config for.</p> required <p>Returns:</p> Type Description <code>ColumnConfigT</code> <p>The column configuration object.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no column with the given name exists.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def get_column_config(self, name: str) -&gt; ColumnConfigT:\n    \"\"\"Get a column configuration by name.\n\n    Args:\n        name: Name of the column to retrieve the config for.\n\n    Returns:\n        The column configuration object.\n\n    Raises:\n        KeyError: If no column with the given name exists.\n    \"\"\"\n    return self._column_configs[name]\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_column_configs","title":"<code>get_column_configs()</code>","text":"<p>Get all column configurations.</p> <p>Returns:</p> Type Description <code>list[ColumnConfigT]</code> <p>A list of all column configuration objects.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def get_column_configs(self) -&gt; list[ColumnConfigT]:\n    \"\"\"Get all column configurations.\n\n    Returns:\n        A list of all column configuration objects.\n    \"\"\"\n    return list(self._column_configs.values())\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_columns_excluding_type","title":"<code>get_columns_excluding_type(column_type)</code>","text":"<p>Get all column configurations excluding the specified type.</p> <p>Parameters:</p> Name Type Description Default <code>column_type</code> <code>DataDesignerColumnType</code> <p>The type of columns to exclude.</p> required <p>Returns:</p> Type Description <code>list[ColumnConfigT]</code> <p>A list of column configurations that do not match the specified type.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def get_columns_excluding_type(self, column_type: DataDesignerColumnType) -&gt; list[ColumnConfigT]:\n    \"\"\"Get all column configurations excluding the specified type.\n\n    Args:\n        column_type: The type of columns to exclude.\n\n    Returns:\n        A list of column configurations that do not match the specified type.\n    \"\"\"\n    column_type = resolve_string_enum(column_type, DataDesignerColumnType)\n    return [c for c in self._column_configs.values() if c.column_type != column_type]\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_columns_of_type","title":"<code>get_columns_of_type(column_type)</code>","text":"<p>Get all column configurations of the specified type.</p> <p>Parameters:</p> Name Type Description Default <code>column_type</code> <code>DataDesignerColumnType</code> <p>The type of columns to filter by.</p> required <p>Returns:</p> Type Description <code>list[ColumnConfigT]</code> <p>A list of column configurations matching the specified type.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def get_columns_of_type(self, column_type: DataDesignerColumnType) -&gt; list[ColumnConfigT]:\n    \"\"\"Get all column configurations of the specified type.\n\n    Args:\n        column_type: The type of columns to filter by.\n\n    Returns:\n        A list of column configurations matching the specified type.\n    \"\"\"\n    column_type = resolve_string_enum(column_type, DataDesignerColumnType)\n    return [c for c in self._column_configs.values() if c.column_type == column_type]\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_constraints","title":"<code>get_constraints(target_column)</code>","text":"<p>Get all constraints for the given target column.</p> <p>Parameters:</p> Name Type Description Default <code>target_column</code> <code>str</code> <p>Name of the column to get constraints for.</p> required <p>Returns:</p> Type Description <code>list[ColumnConstraintT]</code> <p>A list of constraint objects targeting the specified column.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def get_constraints(self, target_column: str) -&gt; list[ColumnConstraintT]:\n    \"\"\"Get all constraints for the given target column.\n\n    Args:\n        target_column: Name of the column to get constraints for.\n\n    Returns:\n        A list of constraint objects targeting the specified column.\n    \"\"\"\n    return [c for c in self._constraints if c.target_column == target_column]\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_processor_configs","title":"<code>get_processor_configs()</code>","text":"<p>Get processor configuration objects.</p> <p>Returns:</p> Type Description <code>dict[BuildStage, list[ProcessorConfigT]]</code> <p>A dictionary of processor configuration objects by dataset builder stage.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def get_processor_configs(self) -&gt; dict[BuildStage, list[ProcessorConfigT]]:\n    \"\"\"Get processor configuration objects.\n\n    Returns:\n        A dictionary of processor configuration objects by dataset builder stage.\n    \"\"\"\n    return self._processor_configs\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_profilers","title":"<code>get_profilers()</code>","text":"<p>Get all profilers.</p> <p>Returns:</p> Type Description <code>list[ColumnProfilerConfigT]</code> <p>A list of profiler configuration objects.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def get_profilers(self) -&gt; list[ColumnProfilerConfigT]:\n    \"\"\"Get all profilers.\n\n    Returns:\n        A list of profiler configuration objects.\n    \"\"\"\n    return self._profilers\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_seed_config","title":"<code>get_seed_config()</code>","text":"<p>Get the seed config for the current Data Designer configuration.</p> <p>Returns:</p> Type Description <code>SeedConfig | None</code> <p>The seed config if configured, None otherwise.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def get_seed_config(self) -&gt; SeedConfig | None:\n    \"\"\"Get the seed config for the current Data Designer configuration.\n\n    Returns:\n        The seed config if configured, None otherwise.\n    \"\"\"\n    return self._seed_config\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.get_tool_config","title":"<code>get_tool_config(alias)</code>","text":"<p>Get a tool configuration by alias.</p> <p>Parameters:</p> Name Type Description Default <code>alias</code> <code>str</code> <p>The alias of the tool configuration to retrieve.</p> required <p>Returns:</p> Type Description <code>ToolConfig</code> <p>The tool configuration object.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no tool configuration with the given alias exists.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def get_tool_config(self, alias: str) -&gt; ToolConfig:\n    \"\"\"Get a tool configuration by alias.\n\n    Args:\n        alias: The alias of the tool configuration to retrieve.\n\n    Returns:\n        The tool configuration object.\n\n    Raises:\n        KeyError: If no tool configuration with the given alias exists.\n    \"\"\"\n    for tc in self._tool_configs:\n        if tc.tool_alias == alias:\n            return tc\n    raise KeyError(f\"No tool configuration with alias {alias!r} found\")\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.num_columns_of_type","title":"<code>num_columns_of_type(column_type)</code>","text":"<p>Get the count of columns of the specified type.</p> <p>Parameters:</p> Name Type Description Default <code>column_type</code> <code>DataDesignerColumnType</code> <p>The type of columns to count.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of columns matching the specified type.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def num_columns_of_type(self, column_type: DataDesignerColumnType) -&gt; int:\n    \"\"\"Get the count of columns of the specified type.\n\n    Args:\n        column_type: The type of columns to count.\n\n    Returns:\n        The number of columns matching the specified type.\n    \"\"\"\n    return len(self.get_columns_of_type(column_type))\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.with_seed_dataset","title":"<code>with_seed_dataset(seed_source, *, sampling_strategy=SamplingStrategy.ORDERED, selection_strategy=None)</code>","text":"<p>Add a seed dataset to the current Data Designer configuration.</p> <p>This method sets the seed dataset for the configuration, but columns are not resolved until compilation (including validation) is performed by the engine using a SeedReader.</p> <p>Parameters:</p> Name Type Description Default <code>seed_source</code> <code>SeedSourceT</code> <p>The pointer to the seed dataset.</p> required <code>sampling_strategy</code> <code>SamplingStrategy</code> <p>The sampling strategy to use when generating data from the seed dataset. Defaults to ORDERED sampling.</p> <code>ORDERED</code> <code>selection_strategy</code> <code>IndexRange | PartitionBlock | None</code> <p>An optional selection strategy to use when generating data from the seed dataset. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>The current Data Designer config builder instance.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def with_seed_dataset(\n    self,\n    seed_source: SeedSourceT,\n    *,\n    sampling_strategy: SamplingStrategy = SamplingStrategy.ORDERED,\n    selection_strategy: IndexRange | PartitionBlock | None = None,\n) -&gt; Self:\n    \"\"\"Add a seed dataset to the current Data Designer configuration.\n\n    This method sets the seed dataset for the configuration, but columns are not resolved until\n    compilation (including validation) is performed by the engine using a SeedReader.\n\n    Args:\n        seed_source: The pointer to the seed dataset.\n        sampling_strategy: The sampling strategy to use when generating data from the seed dataset.\n            Defaults to ORDERED sampling.\n        selection_strategy: An optional selection strategy to use when generating data from the seed dataset.\n            Defaults to None.\n\n    Returns:\n        The current Data Designer config builder instance.\n    \"\"\"\n    self._seed_config = SeedConfig(\n        source=seed_source,\n        sampling_strategy=sampling_strategy,\n        selection_strategy=selection_strategy,\n    )\n    return self\n</code></pre>"},{"location":"code_reference/config_builder/#data_designer.config.config_builder.DataDesignerConfigBuilder.write_config","title":"<code>write_config(path, indent=2, **kwargs)</code>","text":"<p>Write the current configuration to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file to write the configuration to.</p> required <code>indent</code> <code>int | None</code> <p>Indentation level for the output file (default: 2).</p> <code>2</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the serialization methods used.</p> <code>{}</code> <p>Raises:</p> Type Description <code>BuilderConfigurationError</code> <p>If the file format is unsupported.</p> <code>BuilderSerializationError</code> <p>If the configuration cannot be serialized.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/config_builder.py</code> <pre><code>def write_config(self, path: str | Path, indent: int | None = 2, **kwargs) -&gt; None:\n    \"\"\"Write the current configuration to a file.\n\n    Args:\n        path: Path to the file to write the configuration to.\n        indent: Indentation level for the output file (default: 2).\n        **kwargs: Additional keyword arguments passed to the serialization methods used.\n\n    Raises:\n        BuilderConfigurationError: If the file format is unsupported.\n        BuilderSerializationError: If the configuration cannot be serialized.\n    \"\"\"\n    if (seed_config := self.get_seed_config()) is not None and isinstance(seed_config.source, DataFrameSeedSource):\n        raise BuilderSerializationError(\n            \"This builder was configured with a DataFrame seed dataset. \"\n            \"DataFrame seeds cannot be serialized to config files. \"\n            \"To serialize this configuration, change your seed dataset to a more persistent, serializable source format. \"\n            \"For example, you could make a local file seed source from the dataframe:\\n\\n\"\n            \"LocalFileSeedSource.from_dataframe(my_dataframe, '/path/to/data.parquet')\"\n        )\n\n    cfg = self.get_builder_config()\n    suffix = Path(path).suffix\n    if suffix in {\".yaml\", \".yml\"}:\n        cfg.to_yaml(path, indent=indent, **kwargs)\n    elif suffix == \".json\":\n        cfg.to_json(path, indent=indent, **kwargs)\n    else:\n        raise BuilderConfigurationError(f\"\ud83d\uded1 Unsupported file type: {suffix}. Must be `.yaml`, `.yml` or `.json`.\")\n</code></pre>"},{"location":"code_reference/data_designer_config/","title":"Data Designer Configuration","text":"<p>DataDesignerConfig is the main configuration object for builder datasets with Data Designer. It is a declarative configuration for defining the dataset you want to generate column-by-column, including options for dataset post-processing, validation, and profiling.</p> <p>Generally, you should use the DataDesignerConfigBuilder to build your configuration, but you can also build it manually by instantiating the DataDesignerConfig class directly.</p> <p>Classes:</p> Name Description <code>DataDesignerConfig</code> <p>Configuration for NeMo Data Designer.</p>"},{"location":"code_reference/data_designer_config/#data_designer.config.data_designer_config.DataDesignerConfig","title":"<code>DataDesignerConfig</code>","text":"<p>               Bases: <code>ExportableConfigBase</code></p> <p>Configuration for NeMo Data Designer.</p> <p>This class defines the main configuration structure for NeMo Data Designer, which orchestrates the generation of synthetic data.</p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>list[Annotated[ColumnConfigT, Field(discriminator='column_type')]]</code> <p>Required list of column configurations defining how each column should be generated. Must contain at least one column.</p> <code>model_configs</code> <code>list[ModelConfig] | None</code> <p>Optional list of model configurations for LLM-based generation. Each model config defines the model, provider, and inference parameters.</p> <code>tool_configs</code> <code>list[ToolConfig] | None</code> <p>Optional list of tool configurations for MCP tool calling. Each tool config defines the provider, allowed tools, and execution limits.</p> <code>seed_config</code> <code>SeedConfig | None</code> <p>Optional seed dataset settings to use for generation.</p> <code>constraints</code> <code>list[ColumnConstraintT] | None</code> <p>Optional list of column constraints.</p> <code>profilers</code> <code>list[ColumnProfilerConfigT] | None</code> <p>Optional list of column profilers for analyzing generated data characteristics.</p>"},{"location":"code_reference/mcp/","title":"MCP (Model Context Protocol)","text":"<p>The <code>mcp</code> module defines configuration and execution classes for tool use via MCP (Model Context Protocol).</p>"},{"location":"code_reference/mcp/#configuration-classes","title":"Configuration Classes","text":"<p>MCPProvider configures remote MCP servers via SSE transport. LocalStdioMCPProvider configures local MCP servers as subprocesses via stdio transport. ToolConfig defines which tools are available for LLM columns and how they are constrained.</p> <p>For user-facing guides, see:</p> <ul> <li>MCP Providers - Configure local or remote MCP providers</li> <li>Tool Configs - Define tool permissions and limits</li> <li>Enabling Tools - Use tools in LLM columns</li> <li>Traces - Capture full conversation history</li> </ul>"},{"location":"code_reference/mcp/#internal-architecture","title":"Internal Architecture","text":""},{"location":"code_reference/mcp/#parallel-structure","title":"Parallel Structure","text":"Model Layer MCP Layer Purpose <code>ModelProviderRegistry</code> <code>MCPProviderRegistry</code> Holds provider configurations <code>ModelRegistry</code> <code>MCPRegistry</code> Manages configs by alias, lazy facade creation <code>ModelFacade</code> <code>MCPFacade</code> Lightweight facade scoped to specific config <code>ModelConfig.alias</code> <code>ToolConfig.tool_alias</code> Alias for referencing in column configs"},{"location":"code_reference/mcp/#mcpproviderregistry","title":"MCPProviderRegistry","text":"<p>Holds MCP provider configurations. Can be empty (MCP is optional). Created first during resource initialization.</p>"},{"location":"code_reference/mcp/#mcpregistry","title":"MCPRegistry","text":"<p>The central registry for tool configurations:</p> <ul> <li>Holds <code>ToolConfig</code> instances by <code>tool_alias</code></li> <li>Lazily creates <code>MCPFacade</code> instances via <code>get_mcp(tool_alias)</code></li> <li>Manages shared connection pool and tool cache across all facades</li> <li>Validates that tool configs reference valid providers</li> </ul>"},{"location":"code_reference/mcp/#mcpfacade","title":"MCPFacade","text":"<p>A lightweight facade scoped to a specific <code>ToolConfig</code>. Key methods:</p> Method Description <code>tool_call_count(response)</code> Count tool calls in a completion response <code>has_tool_calls(response)</code> Check if response contains tool calls <code>get_tool_schemas()</code> Get OpenAI-format tool schemas for this config <code>process_completion_response(response)</code> Execute tool calls and return messages <code>refuse_completion_response(response)</code> Refuse tool calls gracefully (budget exhaustion) <p>Properties: <code>tool_alias</code>, <code>providers</code>, <code>max_tool_call_turns</code>, <code>allow_tools</code>, <code>timeout_sec</code></p>"},{"location":"code_reference/mcp/#io-layer-mcpiopy","title":"I/O Layer (mcp/io.py)","text":"<p>The <code>io.py</code> module provides low-level MCP communication with performance optimizations:</p> <p>Single event loop architecture: All MCP operations funnel through a dedicated background daemon thread running an asyncio event loop. This allows:</p> <ul> <li>Efficient concurrent I/O without per-thread event loop overhead</li> <li>Natural session sharing across all worker threads</li> <li>Clean async implementation for parallel tool calls</li> </ul> <p>Session pooling: MCP sessions are created lazily and kept alive for the program's duration:</p> <ul> <li>One session per provider (keyed by serialized config)</li> <li>No per-call connection/handshake overhead</li> <li>Graceful cleanup on program exit via <code>atexit</code> handler</li> </ul> <p>Request coalescing: The <code>list_tools</code> operation uses request coalescing to prevent thundering herd:</p> <ul> <li>When multiple workers request tools from the same provider simultaneously</li> <li>Only one request is made; others wait for the cached result</li> <li>Uses asyncio.Lock per provider key</li> </ul> <p>Parallel tool execution: The <code>call_tools_parallel()</code> function executes multiple tool calls concurrently via <code>asyncio.gather()</code>. This is used by MCPFacade when the model returns parallel tool calls in a single response.</p>"},{"location":"code_reference/mcp/#integration-with-modelfacadegenerate","title":"Integration with ModelFacade.generate()","text":"<p>The <code>ModelFacade.generate()</code> method accepts an optional <code>tool_alias</code> parameter:</p> <pre><code>output, messages = model_facade.generate(\n    prompt=\"Search and answer...\",\n    parser=my_parser,\n    tool_alias=\"my-tools\",  # Enables tool calling for this generation\n)\n</code></pre> <p>When <code>tool_alias</code> is provided:</p> <ol> <li><code>ModelFacade</code> looks up the <code>MCPFacade</code> from <code>MCPRegistry</code></li> <li>Tool schemas are fetched and passed to the LLM</li> <li>After each completion, <code>MCPFacade</code> processes tool calls</li> <li>Turn counting tracks iterations; refusal kicks in when budget exhausted</li> <li>Messages (including tool results) are returned for trace capture</li> </ol>"},{"location":"code_reference/mcp/#config-module","title":"Config Module","text":"<p>Classes:</p> Name Description <code>LocalStdioMCPProvider</code> <p>Configuration for launching a local MCP server via stdio transport.</p> <code>MCPProvider</code> <p>Configuration for a remote MCP server connection.</p> <code>ToolConfig</code> <p>Configuration for permitting MCP tools on an LLM column.</p>"},{"location":"code_reference/mcp/#data_designer.config.mcp.LocalStdioMCPProvider","title":"<code>LocalStdioMCPProvider</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for launching a local MCP server via stdio transport.</p> <p>LocalStdioMCPProvider is used to launch MCP servers as subprocesses using stdio for communication. For connecting to remote/pre-existing MCP servers, use MCPProvider instead.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique name used to reference this MCP provider.</p> <code>command</code> <code>str</code> <p>Executable to launch the MCP server via stdio transport.</p> <code>args</code> <code>list[str]</code> <p>Arguments passed to the MCP server executable. Defaults to [].</p> <code>env</code> <code>dict[str, str]</code> <p>Environment variables passed to the MCP server subprocess. Defaults to {}.</p> <code>provider_type</code> <code>Literal['stdio']</code> <p>Transport type discriminator, always \"stdio\".</p> <p>Examples:</p> <p>Stdio (subprocess) transport:</p> <pre><code>&gt;&gt;&gt; LocalStdioMCPProvider(\n...     name=\"demo-mcp\",\n...     command=\"python\",\n...     args=[\"-m\", \"data_designer_e2e_tests.mcp_demo_server\"],\n...     env={\"PYTHONPATH\": \"/path/to/project\"},\n... )\n</code></pre>"},{"location":"code_reference/mcp/#data_designer.config.mcp.MCPProvider","title":"<code>MCPProvider</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for a remote MCP server connection.</p> <p>MCPProvider is used to connect to pre-existing MCP servers via SSE (Server-Sent Events) transport. For local subprocess-based MCP servers, use LocalStdioMCPProvider instead.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique name used to reference this MCP provider.</p> <code>endpoint</code> <code>str</code> <p>SSE endpoint URL for connecting to the remote MCP server.</p> <code>api_key</code> <code>str | None</code> <p>Optional API key for authentication. Defaults to None.</p> <code>provider_type</code> <code>Literal['sse']</code> <p>Transport type discriminator, always \"sse\".</p> <p>Examples:</p> <p>Remote SSE transport:</p> <pre><code>&gt;&gt;&gt; MCPProvider(\n...     name=\"remote-mcp\",\n...     endpoint=\"http://localhost:8080/sse\",\n...     api_key=\"your-api-key\",\n... )\n</code></pre>"},{"location":"code_reference/mcp/#data_designer.config.mcp.ToolConfig","title":"<code>ToolConfig</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for permitting MCP tools on an LLM column.</p> <p>ToolConfig defines which tools are available for use during LLM generation. It references one or more MCP providers by name and can optionally restrict which tools from those providers are permitted.</p> <p>Attributes:</p> Name Type Description <code>tool_alias</code> <code>str</code> <p>User-defined alias to reference this tool configuration in column configs.</p> <code>providers</code> <code>list[str]</code> <p>Names of the MCP providers to use for tool calls. Tools can be drawn from multiple providers.</p> <code>allow_tools</code> <code>list[str] | None</code> <p>Optional allowlist of tool names that restricts which tools are permitted. If None, all tools from the specified providers are allowed. Defaults to None.</p> <code>max_tool_call_turns</code> <code>int</code> <p>Maximum number of tool-calling turns permitted in a single generation. A turn is one iteration where the LLM requests tool calls. With parallel tool calling, a single turn may execute multiple tools simultaneously. Defaults to 5.</p> <code>timeout_sec</code> <code>float | None</code> <p>Timeout in seconds for MCP tool calls. Defaults to None (no timeout).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ToolConfig(\n...     tool_alias=\"search-tools\",\n...     providers=[\"doc-search-mcp\", \"web-search-mcp\"],\n...     allow_tools=[\"search_docs\", \"list_docs\"],\n...     max_tool_call_turns=10,\n...     timeout_sec=30.0,\n... )\n</code></pre>"},{"location":"code_reference/models/","title":"Models","text":"<p>The <code>models</code> module defines configuration objects for model-based generation. ModelProvider, specifies connection and authentication details for custom providers. ModelConfig encapsulates model details including the model alias, identifier, and inference parameters. Inference Parameters controls model behavior through settings like <code>temperature</code>, <code>top_p</code>, and <code>max_tokens</code>, with support for both fixed values and distribution-based sampling. The module includes ImageContext for providing image inputs to multimodal models.</p> <p>For more information on how they are used, see below:</p> <ul> <li>Model Providers</li> <li>Model Configs</li> <li>Image Context</li> </ul> <p>Classes:</p> Name Description <code>BaseInferenceParams</code> <p>Base configuration for inference parameters.</p> <code>ChatCompletionInferenceParams</code> <p>Configuration for LLM inference parameters.</p> <code>DistributionType</code> <p>Types of distributions for sampling inference parameters.</p> <code>EmbeddingInferenceParams</code> <p>Configuration for embedding generation parameters.</p> <code>ImageContext</code> <p>Configuration for providing image context to multimodal models.</p> <code>ImageFormat</code> <p>Supported image formats for image modality.</p> <code>ManualDistribution</code> <p>Manual (discrete) distribution for sampling inference parameters.</p> <code>ManualDistributionParams</code> <p>Parameters for manual distribution sampling.</p> <code>Modality</code> <p>Supported modality types for multimodal model data.</p> <code>ModalityDataType</code> <p>Data type formats for multimodal data.</p> <code>ModelConfig</code> <p>Configuration for a model used for generation.</p> <code>ModelProvider</code> <p>Configuration for a custom model provider.</p> <code>UniformDistribution</code> <p>Uniform distribution for sampling inference parameters.</p> <code>UniformDistributionParams</code> <p>Parameters for uniform distribution sampling.</p>"},{"location":"code_reference/models/#data_designer.config.models.BaseInferenceParams","title":"<code>BaseInferenceParams</code>","text":"<p>               Bases: <code>ConfigBase</code>, <code>ABC</code></p> <p>Base configuration for inference parameters.</p> <p>Attributes:</p> Name Type Description <code>generation_type</code> <code>GenerationType</code> <p>Type of generation (chat-completion or embedding). Acts as discriminator.</p> <code>max_parallel_requests</code> <code>int</code> <p>Maximum number of parallel requests to the model API.</p> <code>timeout</code> <code>int | None</code> <p>Timeout in seconds for each request.</p> <code>extra_body</code> <code>dict[str, Any] | None</code> <p>Additional parameters to pass to the model API.</p> <p>Methods:</p> Name Description <code>format_for_display</code> <p>Format inference parameters for display as a single line.</p> <code>get_formatted_params</code> <p>Get a list of formatted parameter strings.</p>"},{"location":"code_reference/models/#data_designer.config.models.BaseInferenceParams.generate_kwargs","title":"<code>generate_kwargs</code>  <code>property</code>","text":"<p>Get the generate kwargs for the inference parameters.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary of the generate kwargs.</p>"},{"location":"code_reference/models/#data_designer.config.models.BaseInferenceParams.format_for_display","title":"<code>format_for_display()</code>","text":"<p>Format inference parameters for display as a single line.</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted string of inference parameters</p> Source code in <code>packages/data-designer-config/src/data_designer/config/models.py</code> <pre><code>def format_for_display(self) -&gt; str:\n    \"\"\"Format inference parameters for display as a single line.\n\n    Returns:\n        Formatted string of inference parameters\n    \"\"\"\n    parts = self.get_formatted_params()\n    if not parts:\n        return \"(none)\"\n    return \", \".join(parts)\n</code></pre>"},{"location":"code_reference/models/#data_designer.config.models.BaseInferenceParams.get_formatted_params","title":"<code>get_formatted_params()</code>","text":"<p>Get a list of formatted parameter strings.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of formatted parameter strings (e.g., [\"temperature=0.70\", \"max_tokens=100\"])</p> Source code in <code>packages/data-designer-config/src/data_designer/config/models.py</code> <pre><code>def get_formatted_params(self) -&gt; list[str]:\n    \"\"\"Get a list of formatted parameter strings.\n\n    Returns:\n        List of formatted parameter strings (e.g., [\"temperature=0.70\", \"max_tokens=100\"])\n    \"\"\"\n    params_dict = self.model_dump(exclude_none=True, mode=\"json\")\n    parts = []\n    for key, value in params_dict.items():\n        formatted_value = self._format_value(key, value)\n        parts.append(f\"{key}={formatted_value}\")\n    return parts\n</code></pre>"},{"location":"code_reference/models/#data_designer.config.models.ChatCompletionInferenceParams","title":"<code>ChatCompletionInferenceParams</code>","text":"<p>               Bases: <code>BaseInferenceParams</code></p> <p>Configuration for LLM inference parameters.</p> <p>Attributes:</p> Name Type Description <code>generation_type</code> <code>Literal[CHAT_COMPLETION]</code> <p>Type of generation, always \"chat-completion\" for this class.</p> <code>temperature</code> <code>float | DistributionT | None</code> <p>Sampling temperature (0.0-2.0). Can be a fixed value or a distribution for dynamic sampling.</p> <code>top_p</code> <code>float | DistributionT | None</code> <p>Nucleus sampling probability (0.0-1.0). Can be a fixed value or a distribution for dynamic sampling.</p> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate in the response.</p>"},{"location":"code_reference/models/#data_designer.config.models.DistributionType","title":"<code>DistributionType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of distributions for sampling inference parameters.</p>"},{"location":"code_reference/models/#data_designer.config.models.EmbeddingInferenceParams","title":"<code>EmbeddingInferenceParams</code>","text":"<p>               Bases: <code>BaseInferenceParams</code></p> <p>Configuration for embedding generation parameters.</p> <p>Attributes:</p> Name Type Description <code>generation_type</code> <code>Literal[EMBEDDING]</code> <p>Type of generation, always \"embedding\" for this class.</p> <code>encoding_format</code> <code>Literal['float', 'base64']</code> <p>Format of the embedding encoding (\"float\" or \"base64\").</p> <code>dimensions</code> <code>int | None</code> <p>Number of dimensions for the embedding.</p>"},{"location":"code_reference/models/#data_designer.config.models.ImageContext","title":"<code>ImageContext</code>","text":"<p>               Bases: <code>ModalityContext</code></p> <p>Configuration for providing image context to multimodal models.</p> <p>Attributes:</p> Name Type Description <code>modality</code> <code>Modality</code> <p>The modality type (always \"image\").</p> <code>column_name</code> <code>str</code> <p>Name of the column containing image data.</p> <code>data_type</code> <code>ModalityDataType</code> <p>Format of the image data (\"url\" or \"base64\").</p> <code>image_format</code> <code>ImageFormat | None</code> <p>Image format (required for base64 data).</p> <p>Methods:</p> Name Description <code>get_contexts</code> <p>Get the contexts for the image modality.</p>"},{"location":"code_reference/models/#data_designer.config.models.ImageContext.get_contexts","title":"<code>get_contexts(record)</code>","text":"<p>Get the contexts for the image modality.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>dict</code> <p>The record containing the image data. The data can be: - A JSON serialized list of strings - A list of strings - A single string</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A list of image contexts.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/models.py</code> <pre><code>def get_contexts(self, record: dict) -&gt; list[dict[str, Any]]:\n    \"\"\"Get the contexts for the image modality.\n\n    Args:\n        record: The record containing the image data. The data can be:\n            - A JSON serialized list of strings\n            - A list of strings\n            - A single string\n\n    Returns:\n        A list of image contexts.\n    \"\"\"\n    raw_value = record[self.column_name]\n\n    # Normalize to list of strings\n    if isinstance(raw_value, str):\n        # Try to parse as JSON first\n        try:\n            parsed_value = json.loads(raw_value)\n            if isinstance(parsed_value, list):\n                context_values = parsed_value\n            else:\n                context_values = [raw_value]\n        except (json.JSONDecodeError, TypeError):\n            context_values = [raw_value]\n    elif isinstance(raw_value, list):\n        context_values = raw_value\n    elif hasattr(raw_value, \"__iter__\") and not isinstance(raw_value, (str, bytes, dict)):\n        # Handle array-like objects (numpy arrays, pandas Series, etc.)\n        context_values = list(raw_value)\n    else:\n        context_values = [raw_value]\n\n    # Build context list\n    contexts = []\n    for context_value in context_values:\n        context = dict(type=\"image_url\")\n        if self.data_type == ModalityDataType.URL:\n            context[\"image_url\"] = context_value\n        else:\n            context[\"image_url\"] = {\n                \"url\": f\"data:image/{self.image_format.value};base64,{context_value}\",\n                \"format\": self.image_format.value,\n            }\n        contexts.append(context)\n\n    return contexts\n</code></pre>"},{"location":"code_reference/models/#data_designer.config.models.ImageFormat","title":"<code>ImageFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported image formats for image modality.</p>"},{"location":"code_reference/models/#data_designer.config.models.ManualDistribution","title":"<code>ManualDistribution</code>","text":"<p>               Bases: <code>Distribution[ManualDistributionParams]</code></p> <p>Manual (discrete) distribution for sampling inference parameters.</p> <p>Samples from a discrete set of values with optional weights. Useful for testing specific values or creating custom probability distributions for temperature or top_p.</p> <p>Attributes:</p> Name Type Description <code>distribution_type</code> <code>DistributionType | None</code> <p>Type of distribution (\"manual\").</p> <code>params</code> <code>ManualDistributionParams</code> <p>Distribution parameters (values, weights).</p> <p>Methods:</p> Name Description <code>sample</code> <p>Sample a value from the manual distribution.</p>"},{"location":"code_reference/models/#data_designer.config.models.ManualDistribution.sample","title":"<code>sample()</code>","text":"<p>Sample a value from the manual distribution.</p> <p>Returns:</p> Type Description <code>float</code> <p>A float value sampled from the manual distribution.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/models.py</code> <pre><code>def sample(self) -&gt; float:\n    \"\"\"Sample a value from the manual distribution.\n\n    Returns:\n        A float value sampled from the manual distribution.\n    \"\"\"\n    return float(np.random.choice(self.params.values, p=self.params.weights))\n</code></pre>"},{"location":"code_reference/models/#data_designer.config.models.ManualDistributionParams","title":"<code>ManualDistributionParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for manual distribution sampling.</p> <p>Attributes:</p> Name Type Description <code>values</code> <code>list[float]</code> <p>List of possible values to sample from.</p> <code>weights</code> <code>list[float] | None</code> <p>Optional list of weights for each value. If not provided, all values have equal probability.</p>"},{"location":"code_reference/models/#data_designer.config.models.Modality","title":"<code>Modality</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported modality types for multimodal model data.</p>"},{"location":"code_reference/models/#data_designer.config.models.ModalityDataType","title":"<code>ModalityDataType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Data type formats for multimodal data.</p>"},{"location":"code_reference/models/#data_designer.config.models.ModelConfig","title":"<code>ModelConfig</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for a model used for generation.</p> <p>Attributes:</p> Name Type Description <code>alias</code> <code>str</code> <p>User-defined alias to reference in column configurations.</p> <code>model</code> <code>str</code> <p>Model identifier (e.g., from build.nvidia.com or other providers).</p> <code>inference_parameters</code> <code>InferenceParamsT</code> <p>Inference parameters for the model (temperature, top_p, max_tokens, etc.). The generation_type is determined by the type of inference_parameters.</p> <code>provider</code> <code>str | None</code> <p>Optional model provider name if using custom providers.</p> <code>skip_health_check</code> <code>bool</code> <p>Whether to skip the health check for this model. Defaults to False.</p>"},{"location":"code_reference/models/#data_designer.config.models.ModelConfig.generation_type","title":"<code>generation_type</code>  <code>property</code>","text":"<p>Get the generation type from the inference parameters.</p>"},{"location":"code_reference/models/#data_designer.config.models.ModelProvider","title":"<code>ModelProvider</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for a custom model provider.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the model provider.</p> <code>endpoint</code> <code>str</code> <p>API endpoint URL for the provider.</p> <code>provider_type</code> <code>str</code> <p>Provider type (default: \"openai\"). Determines the API format to use.</p> <code>api_key</code> <code>str | None</code> <p>Optional API key for authentication.</p> <code>extra_body</code> <code>dict[str, Any] | None</code> <p>Additional parameters to pass in API requests.</p> <code>extra_headers</code> <code>dict[str, str] | None</code> <p>Additional headers to pass in API requests.</p>"},{"location":"code_reference/models/#data_designer.config.models.UniformDistribution","title":"<code>UniformDistribution</code>","text":"<p>               Bases: <code>Distribution[UniformDistributionParams]</code></p> <p>Uniform distribution for sampling inference parameters.</p> <p>Samples values uniformly between low and high bounds. Useful for exploring a continuous range of values for temperature or top_p.</p> <p>Attributes:</p> Name Type Description <code>distribution_type</code> <code>DistributionType | None</code> <p>Type of distribution (\"uniform\").</p> <code>params</code> <code>UniformDistributionParams</code> <p>Distribution parameters (low, high).</p> <p>Methods:</p> Name Description <code>sample</code> <p>Sample a value from the uniform distribution.</p>"},{"location":"code_reference/models/#data_designer.config.models.UniformDistribution.sample","title":"<code>sample()</code>","text":"<p>Sample a value from the uniform distribution.</p> <p>Returns:</p> Type Description <code>float</code> <p>A float value sampled from the uniform distribution.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/models.py</code> <pre><code>def sample(self) -&gt; float:\n    \"\"\"Sample a value from the uniform distribution.\n\n    Returns:\n        A float value sampled from the uniform distribution.\n    \"\"\"\n    return float(np.random.uniform(low=self.params.low, high=self.params.high, size=1)[0])\n</code></pre>"},{"location":"code_reference/models/#data_designer.config.models.UniformDistributionParams","title":"<code>UniformDistributionParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for uniform distribution sampling.</p> <p>Attributes:</p> Name Type Description <code>low</code> <code>float</code> <p>Lower bound (inclusive).</p> <code>high</code> <code>float</code> <p>Upper bound (exclusive).</p>"},{"location":"code_reference/processors/","title":"Processors","text":"<p>The <code>processors</code> module defines configuration objects for post-generation data transformations. Processors run after column generation and can modify the dataset schema or content before output.</p> <p>Classes:</p> Name Description <code>DropColumnsProcessorConfig</code> <p>Configuration for dropping columns from the output dataset.</p> <code>ProcessorConfig</code> <p>Abstract base class for all processor configuration types.</p> <code>ProcessorType</code> <p>Enumeration of available processor types.</p> <code>SchemaTransformProcessorConfig</code> <p>Configuration for transforming the dataset schema using Jinja2 templates.</p> <p>Functions:</p> Name Description <code>get_processor_config_from_kwargs</code> <p>Create a processor configuration from a processor type and keyword arguments.</p>"},{"location":"code_reference/processors/#data_designer.config.processors.DropColumnsProcessorConfig","title":"<code>DropColumnsProcessorConfig</code>","text":"<p>               Bases: <code>ProcessorConfig</code></p> <p>Configuration for dropping columns from the output dataset.</p> <p>This processor removes specified columns from the generated dataset. The dropped columns are saved separately in a <code>dropped-columns</code> directory for reference. When this processor is added via the config builder, the corresponding column configs are automatically marked with <code>drop = True</code>.</p> <p>Alternatively, you can set <code>drop = True</code> when configuring a column.</p> <p>Attributes:</p> Name Type Description <code>column_names</code> <code>list[str]</code> <p>List of column names to remove from the output dataset.</p> <code>processor_type</code> <code>Literal[DROP_COLUMNS]</code> <p>Discriminator field, always <code>ProcessorType.DROP_COLUMNS</code> for this configuration type.</p>"},{"location":"code_reference/processors/#data_designer.config.processors.ProcessorConfig","title":"<code>ProcessorConfig</code>","text":"<p>               Bases: <code>ConfigBase</code>, <code>ABC</code></p> <p>Abstract base class for all processor configuration types.</p> <p>Processors are transformations that run before or after columns are generated. They can modify, reshape, or augment the dataset before it's saved.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique name of the processor, used to identify the processor in results and to name output artifacts on disk.</p> <code>build_stage</code> <code>BuildStage</code> <p>The stage at which the processor runs. Currently only <code>POST_BATCH</code> is supported, meaning processors run after each batch of columns is generated.</p>"},{"location":"code_reference/processors/#data_designer.config.processors.ProcessorType","title":"<code>ProcessorType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of available processor types.</p> <p>Attributes:</p> Name Type Description <code>DROP_COLUMNS</code> <p>Processor that removes specified columns from the output dataset.</p> <code>SCHEMA_TRANSFORM</code> <p>Processor that creates a new dataset with a transformed schema using Jinja2 templates.</p>"},{"location":"code_reference/processors/#data_designer.config.processors.SchemaTransformProcessorConfig","title":"<code>SchemaTransformProcessorConfig</code>","text":"<p>               Bases: <code>ProcessorConfig</code></p> <p>Configuration for transforming the dataset schema using Jinja2 templates.</p> <p>This processor creates a new dataset with a transformed schema. Each key in the template becomes a column in the output, and values are Jinja2 templates that can reference any column in the batch. The transformed dataset is written to a <code>processors-outputs/{processor_name}/</code> directory alongside the main dataset.</p> <p>Attributes:</p> Name Type Description <code>template</code> <code>dict[str, Any]</code> <p>Dictionary defining the output schema. Keys are new column names, values are Jinja2 templates (strings, lists, or nested structures). Must be JSON-serializable.</p> <code>processor_type</code> <code>Literal[SCHEMA_TRANSFORM]</code> <p>Discriminator field, always <code>ProcessorType.SCHEMA_TRANSFORM</code> for this configuration type.</p>"},{"location":"code_reference/processors/#data_designer.config.processors.get_processor_config_from_kwargs","title":"<code>get_processor_config_from_kwargs(processor_type, **kwargs)</code>","text":"<p>Create a processor configuration from a processor type and keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>processor_type</code> <code>ProcessorType</code> <p>The type of processor to create.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the processor constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ProcessorConfig</code> <p>A processor configuration object of the specified type.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/processors.py</code> <pre><code>def get_processor_config_from_kwargs(processor_type: ProcessorType, **kwargs: Any) -&gt; ProcessorConfig:\n    \"\"\"Create a processor configuration from a processor type and keyword arguments.\n\n    Args:\n        processor_type: The type of processor to create.\n        **kwargs: Additional keyword arguments passed to the processor constructor.\n\n    Returns:\n        A processor configuration object of the specified type.\n    \"\"\"\n    if processor_type == ProcessorType.DROP_COLUMNS:\n        return DropColumnsProcessorConfig(**kwargs)\n    elif processor_type == ProcessorType.SCHEMA_TRANSFORM:\n        return SchemaTransformProcessorConfig(**kwargs)\n</code></pre>"},{"location":"code_reference/run_config/","title":"Run Config","text":"<p>The <code>run_config</code> module defines runtime settings that control dataset generation behavior, including early shutdown thresholds, batch sizing, and non-inference worker concurrency.</p>"},{"location":"code_reference/run_config/#usage","title":"Usage","text":"<pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\ndata_designer = DataDesigner()\ndata_designer.set_run_config(dd.RunConfig(\n    buffer_size=500,\n    max_conversation_restarts=3,\n))\n</code></pre>"},{"location":"code_reference/run_config/#api-reference","title":"API Reference","text":"<p>Classes:</p> Name Description <code>RunConfig</code> <p>Runtime configuration for dataset generation.</p>"},{"location":"code_reference/run_config/#data_designer.config.run_config.RunConfig","title":"<code>RunConfig</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Runtime configuration for dataset generation.</p> <p>Groups configuration options that control generation behavior but aren't part of the dataset configuration itself.</p> <p>Attributes:</p> Name Type Description <code>disable_early_shutdown</code> <code>bool</code> <p>If True, disables the executor's early-shutdown behavior entirely. Generation will continue regardless of error rate, and the early-shutdown exception will never be raised. Error counts and summaries are still collected. Default is False.</p> <code>shutdown_error_rate</code> <code>float</code> <p>Error rate threshold (0.0-1.0) that triggers early shutdown when early shutdown is enabled. Default is 0.5.</p> <code>shutdown_error_window</code> <code>int</code> <p>Minimum number of completed tasks before error rate monitoring begins. Must be &gt;= 0. Default is 10.</p> <code>buffer_size</code> <code>int</code> <p>Number of records to process in each batch during dataset generation. A batch is processed end-to-end (column generation, post-batch processors, and writing the batch to artifact storage) before moving on to the next batch. Must be &gt; 0. Default is 1000.</p> <code>non_inference_max_parallel_workers</code> <code>int</code> <p>Maximum number of worker threads used for non-inference cell-by-cell generators. Must be &gt;= 1. Default is 4.</p> <code>max_conversation_restarts</code> <code>int</code> <p>Maximum number of full conversation restarts permitted when generation tasks call <code>ModelFacade.generate(...)</code>. Must be &gt;= 0. Default is 5.</p> <code>max_conversation_correction_steps</code> <code>int</code> <p>Maximum number of correction rounds permitted within a single conversation when generation tasks call <code>ModelFacade.generate(...)</code>. Must be &gt;= 0. Default is 0.</p> <p>Methods:</p> Name Description <code>normalize_shutdown_settings</code> <p>Normalize shutdown settings for compatibility.</p>"},{"location":"code_reference/run_config/#data_designer.config.run_config.RunConfig.normalize_shutdown_settings","title":"<code>normalize_shutdown_settings()</code>","text":"<p>Normalize shutdown settings for compatibility.</p> Source code in <code>packages/data-designer-config/src/data_designer/config/run_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef normalize_shutdown_settings(self) -&gt; Self:\n    \"\"\"Normalize shutdown settings for compatibility.\"\"\"\n    if self.disable_early_shutdown:\n        self.shutdown_error_rate = 1.0\n    return self\n</code></pre>"},{"location":"code_reference/sampler_params/","title":"Sampler Parameters","text":"<p>The <code>sampler_params</code> module defines parameter configuration objects for all Data Designer sampler types. Sampler parameters are used within the SamplerColumnConfig to specify how values should be generated for sampled columns.</p> <p>Displaying available samplers and their parameters</p> <p>The config builder has an <code>info</code> attribute that can be used to display the available sampler types and their parameters: <pre><code>config_builder.info.display(\"samplers\")\n</code></pre></p> <p>Classes:</p> Name Description <code>BernoulliMixtureSamplerParams</code> <p>Parameters for sampling from a Bernoulli mixture distribution.</p> <code>BernoulliSamplerParams</code> <p>Parameters for sampling from a Bernoulli distribution.</p> <code>BinomialSamplerParams</code> <p>Parameters for sampling from a Binomial distribution.</p> <code>CategorySamplerParams</code> <p>Parameters for categorical sampling with optional probability weighting.</p> <code>DatetimeSamplerParams</code> <p>Parameters for uniform datetime sampling within a specified range.</p> <code>GaussianSamplerParams</code> <p>Parameters for sampling from a Gaussian (Normal) distribution.</p> <code>PersonFromFakerSamplerParams</code> <p>Parameters for sampling synthetic person data with demographic attributes from Faker.</p> <code>PersonSamplerParams</code> <p>Parameters for sampling synthetic person data with demographic attributes.</p> <code>PoissonSamplerParams</code> <p>Parameters for sampling from a Poisson distribution.</p> <code>ScipySamplerParams</code> <p>Parameters for sampling from any scipy.stats continuous or discrete distribution.</p> <code>SubcategorySamplerParams</code> <p>Parameters for subcategory sampling conditioned on a parent category column.</p> <code>TimeDeltaSamplerParams</code> <p>Parameters for sampling time deltas relative to a reference datetime column.</p> <code>UUIDSamplerParams</code> <p>Parameters for generating UUID (Universally Unique Identifier) values.</p> <code>UniformSamplerParams</code> <p>Parameters for sampling from a continuous Uniform distribution.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.BernoulliMixtureSamplerParams","title":"<code>BernoulliMixtureSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling from a Bernoulli mixture distribution.</p> <p>Combines a Bernoulli distribution with another continuous distribution, creating a mixture where values are either 0 (with probability 1-p) or sampled from the specified distribution (with probability p). This is useful for modeling scenarios with many zero values mixed with a continuous distribution of non-zero values.</p> <p>Common use cases include modeling sparse events, zero-inflated data, or situations where an outcome either doesn't occur (0) or follows a specific distribution when it does occur.</p> <p>Attributes:</p> Name Type Description <code>p</code> <code>float</code> <p>Probability of sampling from the mixture distribution (non-zero outcome). Must be between 0.0 and 1.0 (inclusive). With probability 1-p, the sample is 0.</p> <code>dist_name</code> <code>str</code> <p>Name of the scipy.stats distribution to sample from when outcome is non-zero. Must be a valid scipy.stats distribution name (e.g., \"norm\", \"gamma\", \"expon\").</p> <code>dist_params</code> <code>dict</code> <p>Parameters for the specified scipy.stats distribution.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.BernoulliSamplerParams","title":"<code>BernoulliSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling from a Bernoulli distribution.</p> <p>Samples binary values (0 or 1) representing the outcome of a single trial with a fixed probability of success. This is the simplest discrete probability distribution, useful for modeling binary outcomes like success/failure, yes/no, or true/false.</p> <p>Attributes:</p> Name Type Description <code>p</code> <code>float</code> <p>Probability of success (sampling 1). Must be between 0.0 and 1.0 (inclusive). The probability of failure (sampling 0) is automatically 1 - p.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.BinomialSamplerParams","title":"<code>BinomialSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling from a Binomial distribution.</p> <p>Samples integer values representing the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success. Commonly used to model the number of successful outcomes in repeated experiments.</p> <p>Attributes:</p> Name Type Description <code>n</code> <code>int</code> <p>Number of independent trials. Must be a positive integer.</p> <code>p</code> <code>float</code> <p>Probability of success on each trial. Must be between 0.0 and 1.0 (inclusive).</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.CategorySamplerParams","title":"<code>CategorySamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for categorical sampling with optional probability weighting.</p> <p>Samples values from a discrete set of categories. When weights are provided, values are sampled according to their assigned probabilities. Without weights, uniform sampling is used.</p> <p>Attributes:</p> Name Type Description <code>values</code> <code>list[str | int | float]</code> <p>List of possible categorical values to sample from. Can contain strings, integers, or floats. Must contain at least one value.</p> <code>weights</code> <code>list[float] | None</code> <p>Optional unnormalized probability weights for each value. If provided, must be the same length as <code>values</code>. Weights are automatically normalized to sum to 1.0. Larger weights result in higher sampling probability for the corresponding value.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.DatetimeSamplerParams","title":"<code>DatetimeSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for uniform datetime sampling within a specified range.</p> <p>Samples datetime values uniformly between a start and end date with a specified granularity. The sampling unit determines the smallest possible time interval between consecutive samples.</p> <p>Attributes:</p> Name Type Description <code>start</code> <code>str</code> <p>Earliest possible datetime for the sampling range (inclusive). Must be a valid datetime string parseable by pandas.to_datetime().</p> <code>end</code> <code>str</code> <p>Latest possible datetime for the sampling range (inclusive). Must be a valid datetime string parseable by pandas.to_datetime().</p> <code>unit</code> <code>Literal['Y', 'M', 'D', 'h', 'm', 's']</code> <p>Time unit for sampling granularity. Options: - \"Y\": Years - \"M\": Months - \"D\": Days (default) - \"h\": Hours - \"m\": Minutes - \"s\": Seconds</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.GaussianSamplerParams","title":"<code>GaussianSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling from a Gaussian (Normal) distribution.</p> <p>Samples continuous values from a normal distribution characterized by its mean and standard deviation. The Gaussian distribution is one of the most commonly used probability distributions, appearing naturally in many real-world phenomena due to the Central Limit Theorem.</p> <p>Attributes:</p> Name Type Description <code>mean</code> <code>float</code> <p>Mean (center) of the Gaussian distribution. This is the expected value and the location of the distribution's peak.</p> <code>stddev</code> <code>float</code> <p>Standard deviation of the Gaussian distribution. Controls the spread or width of the distribution. Must be positive.</p> <code>decimal_places</code> <code>int | None</code> <p>Optional number of decimal places to round sampled values to. If None, values are not rounded.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.PersonFromFakerSamplerParams","title":"<code>PersonFromFakerSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling synthetic person data with demographic attributes from Faker.</p> <p>Uses the Faker library to generate random personal information. The data is basic and not demographically accurate, but is useful for quick testing, prototyping, or when realistic demographic distributions are not relevant for your use case. For demographically accurate person data, use the <code>PersonSamplerParams</code> sampler.</p> <p>Attributes:</p> Name Type Description <code>locale</code> <code>str</code> <p>Locale string determining the language and geographic region for synthetic people. Can be any locale supported by Faker.</p> <code>sex</code> <code>SexT | None</code> <p>If specified, filters to only sample people of the specified sex. Options: \"Male\" or \"Female\". If None, samples both sexes.</p> <code>city</code> <code>str | list[str] | None</code> <p>If specified, filters to only sample people from the specified city or cities. Can be a single city name (string) or a list of city names.</p> <code>age_range</code> <code>list[int]</code> <p>Two-element list [min_age, max_age] specifying the age range to sample from (inclusive). Defaults to a standard age range. Both values must be between the minimum and maximum allowed ages.</p> <code>sampler_type</code> <code>Literal[PERSON_FROM_FAKER]</code> <p>Discriminator for the sampler type. Must be <code>SamplerType.PERSON_FROM_FAKER</code>.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.PersonFromFakerSamplerParams.generator_kwargs","title":"<code>generator_kwargs</code>  <code>property</code>","text":"<p>Keyword arguments to pass to the person generator.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.PersonSamplerParams","title":"<code>PersonSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling synthetic person data with demographic attributes.</p> <p>Generates realistic synthetic person data including names, addresses, phone numbers, and other demographic information. Data can be sampled from managed datasets (when available) or generated using Faker. The sampler supports filtering by locale, sex, age, geographic location, and can optionally include synthetic persona descriptions.</p> <p>Attributes:</p> Name Type Description <code>locale</code> <code>str</code> <p>Locale string determining the language and geographic region for synthetic people. Must be a locale supported by a managed Nemotron Personas dataset. The dataset must be downloaded and available in the managed assets directory.</p> <code>sex</code> <code>SexT | None</code> <p>If specified, filters to only sample people of the specified sex. Options: \"Male\" or \"Female\". If None, samples both sexes.</p> <code>city</code> <code>str | list[str] | None</code> <p>If specified, filters to only sample people from the specified city or cities. Can be a single city name (string) or a list of city names.</p> <code>age_range</code> <code>list[int]</code> <p>Two-element list [min_age, max_age] specifying the age range to sample from (inclusive). Defaults to a standard age range. Both values must be between minimum and maximum allowed ages.</p> <code>with_synthetic_personas</code> <code>bool</code> <p>If True, appends additional synthetic persona columns including personality traits, interests, and background descriptions. Only supported for certain locales with managed datasets.</p> <code>sample_dataset_when_available</code> <code>bool</code> <p>If True, samples from curated managed datasets when available for the specified locale. If False or unavailable, falls back to Faker-generated data. Managed datasets typically provide more realistic and diverse synthetic people.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.PersonSamplerParams.generator_kwargs","title":"<code>generator_kwargs</code>  <code>property</code>","text":"<p>Keyword arguments to pass to the person generator.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.PoissonSamplerParams","title":"<code>PoissonSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling from a Poisson distribution.</p> <p>Samples non-negative integer values representing the number of events occurring in a fixed interval of time or space. The Poisson distribution is commonly used to model count data like the number of arrivals, occurrences, or events per time period.</p> <p>The distribution is characterized by a single parameter (mean/rate), and both the mean and variance equal this parameter value.</p> <p>Attributes:</p> Name Type Description <code>mean</code> <code>float</code> <p>Mean number of events in the fixed interval (also called rate parameter \u03bb). Must be positive. This represents both the expected value and the variance of the distribution.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.ScipySamplerParams","title":"<code>ScipySamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling from any scipy.stats continuous or discrete distribution.</p> <p>Provides a flexible interface to sample from the wide range of probability distributions available in scipy.stats. This enables advanced statistical sampling beyond the built-in distribution types (Gaussian, Uniform, etc.).</p> <p>See: scipy.stats documentation</p> <p>Attributes:</p> Name Type Description <code>dist_name</code> <code>str</code> <p>Name of the scipy.stats distribution to sample from (e.g., \"beta\", \"gamma\", \"lognorm\", \"expon\"). Must be a valid distribution name from scipy.stats.</p> <code>dist_params</code> <code>dict</code> <p>Dictionary of parameters for the specified distribution. Parameter names and values must match the scipy.stats distribution specification (e.g., {\"a\": 2, \"b\": 5} for beta distribution, {\"scale\": 1.5} for exponential).</p> <code>decimal_places</code> <code>int | None</code> <p>Optional number of decimal places to round sampled values to. If None, values are not rounded.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.SubcategorySamplerParams","title":"<code>SubcategorySamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for subcategory sampling conditioned on a parent category column.</p> <p>Samples subcategory values based on the value of a parent category column. Each parent category value maps to its own list of possible subcategory values, enabling hierarchical or conditional sampling patterns.</p> <p>Attributes:</p> Name Type Description <code>category</code> <code>str</code> <p>Name of the parent category column that this subcategory depends on. The parent column must be generated before this subcategory column.</p> <code>values</code> <code>dict[str, list[str | int | float]]</code> <p>Mapping from each parent category value to a list of possible subcategory values. Each key must correspond to a value that appears in the parent category column.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.TimeDeltaSamplerParams","title":"<code>TimeDeltaSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling time deltas relative to a reference datetime column.</p> <p>Samples time offsets within a specified range and adds them to values from a reference datetime column. This is useful for generating related datetime columns like order dates and delivery dates, or event start times and end times.</p> Note <p>Years and months are not supported as timedelta units because they have variable lengths. See: pandas timedelta documentation</p> <p>Attributes:</p> Name Type Description <code>dt_min</code> <code>int</code> <p>Minimum time-delta value (inclusive). Must be non-negative and less than <code>dt_max</code>. Specified in units defined by the <code>unit</code> parameter.</p> <code>dt_max</code> <code>int</code> <p>Maximum time-delta value (exclusive). Must be positive and greater than <code>dt_min</code>. Specified in units defined by the <code>unit</code> parameter.</p> <code>reference_column_name</code> <code>str</code> <p>Name of an existing datetime column to add the time-delta to. This column must be generated before the timedelta column.</p> <code>unit</code> <code>Literal['D', 'h', 'm', 's']</code> <p>Time unit for the delta values. Options: - \"D\": Days (default) - \"h\": Hours - \"m\": Minutes - \"s\": Seconds</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.UUIDSamplerParams","title":"<code>UUIDSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for generating UUID (Universally Unique Identifier) values.</p> <p>Generates UUID4 (random) identifiers with optional formatting options. UUIDs are useful for creating unique identifiers for records, entities, or transactions.</p> <p>Attributes:</p> Name Type Description <code>prefix</code> <code>str | None</code> <p>Optional string to prepend to each UUID. Useful for creating namespaced or typed identifiers (e.g., \"user-\", \"order-\", \"txn-\").</p> <code>short_form</code> <code>bool</code> <p>If True, truncates UUIDs to 8 characters (first segment only). Default is False for full 32-character UUIDs (excluding hyphens).</p> <code>uppercase</code> <code>bool</code> <p>If True, converts all hexadecimal letters to uppercase. Default is False for lowercase UUIDs.</p>"},{"location":"code_reference/sampler_params/#data_designer.config.sampler_params.UniformSamplerParams","title":"<code>UniformSamplerParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Parameters for sampling from a continuous Uniform distribution.</p> <p>Samples continuous values uniformly from a specified range, where every value in the range has equal probability of being sampled. This is useful when all values within a range are equally likely, such as random percentages, proportions, or unbiased measurements.</p> <p>Attributes:</p> Name Type Description <code>low</code> <code>float</code> <p>Lower bound of the uniform distribution (inclusive). Can be any real number.</p> <code>high</code> <code>float</code> <p>Upper bound of the uniform distribution (inclusive). Must be greater than <code>low</code>.</p> <code>decimal_places</code> <code>int | None</code> <p>Optional number of decimal places to round sampled values to. If None, values are not rounded and may have many decimal places.</p>"},{"location":"code_reference/validator_params/","title":"Validator Parameters","text":"<p>When creating a <code>ValidationColumnConfig</code>, two parameters are used to define the validator: <code>validator_type</code> and <code>validator_config</code>. The <code>validator_type</code> parameter can be set to either <code>code</code>, <code>local_callable</code> or <code>remote</code>. The <code>validator_config</code> accompanying each of these is, respectively:</p> <p>Classes:</p> Name Description <code>CodeValidatorParams</code> <p>Configuration for code validation. Supports Python and SQL code validation.</p> <code>LocalCallableValidatorParams</code> <p>Configuration for local callable validation. Expects a function to be passed that validates the data.</p> <code>RemoteValidatorParams</code> <p>Configuration for remote validation. Sends data to a remote endpoint for validation.</p>"},{"location":"code_reference/validator_params/#data_designer.config.validator_params.CodeValidatorParams","title":"<code>CodeValidatorParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for code validation. Supports Python and SQL code validation.</p> <p>Attributes:</p> Name Type Description <code>code_lang</code> <code>CodeLang</code> <p>The language of the code to validate. Supported values include: <code>python</code>, <code>sql:sqlite</code>, <code>sql:postgres</code>, <code>sql:mysql</code>, <code>sql:tsql</code>, <code>sql:bigquery</code>, <code>sql:ansi</code>.</p>"},{"location":"code_reference/validator_params/#data_designer.config.validator_params.LocalCallableValidatorParams","title":"<code>LocalCallableValidatorParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for local callable validation. Expects a function to be passed that validates the data.</p> <p>Attributes:</p> Name Type Description <code>validation_function</code> <code>Any</code> <p>Function (<code>Callable[[pd.DataFrame], pd.DataFrame]</code>) to validate the data. Output must contain a column <code>is_valid</code> of type <code>bool</code>.</p> <code>output_schema</code> <code>dict[str, Any] | None</code> <p>The JSON schema for the local callable validator's output. If not provided, the output will not be validated.</p>"},{"location":"code_reference/validator_params/#data_designer.config.validator_params.RemoteValidatorParams","title":"<code>RemoteValidatorParams</code>","text":"<p>               Bases: <code>ConfigBase</code></p> <p>Configuration for remote validation. Sends data to a remote endpoint for validation.</p> <p>Attributes:</p> Name Type Description <code>endpoint_url</code> <code>str</code> <p>The URL of the remote endpoint.</p> <code>output_schema</code> <code>dict[str, Any] | None</code> <p>The JSON schema for the remote validator's output. If not provided, the output will not be validated.</p> <code>timeout</code> <code>float</code> <p>The timeout for the HTTP request in seconds. Defaults to 30.0.</p> <code>max_retries</code> <code>int</code> <p>The maximum number of retry attempts. Defaults to 3.</p> <code>retry_backoff</code> <code>float</code> <p>The backoff factor for the retry delay in seconds. Defaults to 2.0.</p> <code>max_parallel_requests</code> <code>int</code> <p>The maximum number of parallel requests to make. Defaults to 4.</p>"},{"location":"colab_notebooks/1-the-basics/","title":"1 the basics","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install -U data-designer\n</pre> %%capture !pip install -U data-designer In\u00a0[\u00a0]: Copied! <pre>import getpass\nimport os\n\nfrom google.colab import userdata\n\ntry:\n    os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\nexcept userdata.SecretNotFoundError:\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")\n</pre> import getpass import os  from google.colab import userdata  try:     os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\") except userdata.SecretNotFoundError:     os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \") In\u00a0[\u00a0]: Copied! <pre>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n</pre> import data_designer.config as dd from data_designer.interface import DataDesigner In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    dd.ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     dd.ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=dd.ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[\u00a0]: Copied! <pre>config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre>config_builder.info.display(\"samplers\")\n</pre> config_builder.info.display(\"samplers\") <p>Let's start designing our product review dataset by adding product category and subcategory columns.</p> In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"product_category\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\n                \"Electronics\",\n                \"Clothing\",\n                \"Home &amp; Kitchen\",\n                \"Books\",\n                \"Home Office\",\n            ],\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"product_subcategory\",\n        sampler_type=dd.SamplerType.SUBCATEGORY,\n        params=dd.SubcategorySamplerParams(\n            category=\"product_category\",\n            values={\n                \"Electronics\": [\n                    \"Smartphones\",\n                    \"Laptops\",\n                    \"Headphones\",\n                    \"Cameras\",\n                    \"Accessories\",\n                ],\n                \"Clothing\": [\n                    \"Men's Clothing\",\n                    \"Women's Clothing\",\n                    \"Winter Coats\",\n                    \"Activewear\",\n                    \"Accessories\",\n                ],\n                \"Home &amp; Kitchen\": [\n                    \"Appliances\",\n                    \"Cookware\",\n                    \"Furniture\",\n                    \"Decor\",\n                    \"Organization\",\n                ],\n                \"Books\": [\n                    \"Fiction\",\n                    \"Non-Fiction\",\n                    \"Self-Help\",\n                    \"Textbooks\",\n                    \"Classics\",\n                ],\n                \"Home Office\": [\n                    \"Desks\",\n                    \"Chairs\",\n                    \"Storage\",\n                    \"Office Supplies\",\n                    \"Lighting\",\n                ],\n            },\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"target_age_range\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),\n    )\n)\n\n# Optionally validate that the columns are configured correctly.\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     dd.SamplerColumnConfig(         name=\"product_category\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(             values=[                 \"Electronics\",                 \"Clothing\",                 \"Home &amp; Kitchen\",                 \"Books\",                 \"Home Office\",             ],         ),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"product_subcategory\",         sampler_type=dd.SamplerType.SUBCATEGORY,         params=dd.SubcategorySamplerParams(             category=\"product_category\",             values={                 \"Electronics\": [                     \"Smartphones\",                     \"Laptops\",                     \"Headphones\",                     \"Cameras\",                     \"Accessories\",                 ],                 \"Clothing\": [                     \"Men's Clothing\",                     \"Women's Clothing\",                     \"Winter Coats\",                     \"Activewear\",                     \"Accessories\",                 ],                 \"Home &amp; Kitchen\": [                     \"Appliances\",                     \"Cookware\",                     \"Furniture\",                     \"Decor\",                     \"Organization\",                 ],                 \"Books\": [                     \"Fiction\",                     \"Non-Fiction\",                     \"Self-Help\",                     \"Textbooks\",                     \"Classics\",                 ],                 \"Home Office\": [                     \"Desks\",                     \"Chairs\",                     \"Storage\",                     \"Office Supplies\",                     \"Lighting\",                 ],             },         ),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"target_age_range\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),     ) )  # Optionally validate that the columns are configured correctly. data_designer.validate(config_builder) <p>Next, let's add samplers to generate data related to the customer and their review.</p> In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=dd.SamplerType.PERSON_FROM_FAKER,\n        params=dd.PersonFromFakerSamplerParams(age_range=[18, 70], locale=\"en_US\"),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"number_of_stars\",\n        sampler_type=dd.SamplerType.UNIFORM,\n        params=dd.UniformSamplerParams(low=1, high=5),\n        convert_to=\"int\",  # Convert the sampled float to an integer.\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"review_style\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],\n            weights=[1, 2, 2, 1],\n        ),\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     dd.SamplerColumnConfig(         name=\"customer\",         sampler_type=dd.SamplerType.PERSON_FROM_FAKER,         params=dd.PersonFromFakerSamplerParams(age_range=[18, 70], locale=\"en_US\"),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"number_of_stars\",         sampler_type=dd.SamplerType.UNIFORM,         params=dd.UniformSamplerParams(low=1, high=5),         convert_to=\"int\",  # Convert the sampled float to an integer.     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"review_style\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(             values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],             weights=[1, 2, 2, 1],         ),     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"product_name\",\n        prompt=(\n            \"You are a helpful assistant that generates product names. DO NOT add quotes around the product name.\\n\\n\"\n            \"Come up with a creative product name for a product in the '{{ product_category }}' category, focusing \"\n            \"on products related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"\n            \"{{ target_age_range }} years old. Respond with only the product name, no other text.\"\n        ),\n        model_alias=MODEL_ALIAS,\n    )\n)\n\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"customer_review\",\n        prompt=(\n            \"You are a customer named {{ customer.first_name }} from {{ customer.city }}, {{ customer.state }}. \"\n            \"You are {{ customer.age }} years old and recently purchased a product called {{ product_name }}. \"\n            \"Write a review of this product, which you gave a rating of {{ number_of_stars }} stars. \"\n            \"The style of the review should be '{{ review_style }}'. \"\n            \"Respond with only the review, no other text.\"\n        ),\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     dd.LLMTextColumnConfig(         name=\"product_name\",         prompt=(             \"You are a helpful assistant that generates product names. DO NOT add quotes around the product name.\\n\\n\"             \"Come up with a creative product name for a product in the '{{ product_category }}' category, focusing \"             \"on products related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"             \"{{ target_age_range }} years old. Respond with only the product name, no other text.\"         ),         model_alias=MODEL_ALIAS,     ) )  config_builder.add_column(     dd.LLMTextColumnConfig(         name=\"customer_review\",         prompt=(             \"You are a customer named {{ customer.first_name }} from {{ customer.city }}, {{ customer.state }}. \"             \"You are {{ customer.age }} years old and recently purchased a product called {{ product_name }}. \"             \"Write a review of this product, which you gave a rating of {{ number_of_stars }} stars. \"             \"The style of the review should be '{{ review_style }}'. \"             \"Respond with only the review, no other text.\"         ),         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-1\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-1\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"colab_notebooks/1-the-basics/#data-designer-tutorial-the-basics","title":"\ud83c\udfa8 Data Designer Tutorial: The Basics\u00b6","text":""},{"location":"colab_notebooks/1-the-basics/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>This notebook demonstrates the basics of Data Designer by generating a simple product review dataset.</p>"},{"location":"colab_notebooks/1-the-basics/#import-data-designer","title":"\ud83d\udce6 Import Data Designer\u00b6","text":"<ul> <li><p><code>data_designer.config</code> provides access to the configuration API.</p> </li> <li><p><code>DataDesigner</code> is the main interface for data generation.</p> </li> </ul>"},{"location":"colab_notebooks/1-the-basics/#colab-setup","title":"\u26a1 Colab Setup\u00b6","text":"<p>Run the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from build.nvidia.com.</p>"},{"location":"colab_notebooks/1-the-basics/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"colab_notebooks/1-the-basics/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"colab_notebooks/1-the-basics/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"colab_notebooks/1-the-basics/#getting-started-with-sampler-columns","title":"\ud83c\udfb2 Getting started with sampler columns\u00b6","text":"<ul> <li><p>Sampler columns offer non-LLM based generation of synthetic data.</p> </li> <li><p>They are particularly useful for steering the diversity of the generated data, as we demonstrate below.</p> </li> </ul> <p>You can view available samplers using the config builder's <code>info</code> property:</p>"},{"location":"colab_notebooks/1-the-basics/#llm-generated-columns","title":"\ud83e\udd9c LLM-generated columns\u00b6","text":"<ul> <li><p>The real power of Data Designer comes from leveraging LLMs to generate text, code, and structured data.</p> </li> <li><p>When prompting the LLM, we can use Jinja templating to reference other columns in the dataset.</p> </li> <li><p>As we see below, nested json fields can be accessed using dot notation.</p> </li> </ul>"},{"location":"colab_notebooks/1-the-basics/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"colab_notebooks/1-the-basics/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"colab_notebooks/1-the-basics/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"colab_notebooks/1-the-basics/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Now that you've seen the basics of Data Designer, check out the following notebooks to learn more about:</p> <ul> <li><p>Structured outputs and jinja expressions</p> </li> <li><p>Seeding synthetic data generation with an external dataset</p> </li> <li><p>Providing images as context</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/","title":"2 structured outputs and jinja expressions","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install -U data-designer\n</pre> %%capture !pip install -U data-designer In\u00a0[\u00a0]: Copied! <pre>import getpass\nimport os\n\nfrom google.colab import userdata\n\ntry:\n    os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\nexcept userdata.SecretNotFoundError:\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")\n</pre> import getpass import os  from google.colab import userdata  try:     os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\") except userdata.SecretNotFoundError:     os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \") In\u00a0[\u00a0]: Copied! <pre>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n</pre> import data_designer.config as dd from data_designer.interface import DataDesigner In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    dd.ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     dd.ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=dd.ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[\u00a0]: Copied! <pre>config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre>from decimal import Decimal\nfrom typing import Literal\n\nfrom pydantic import BaseModel, Field\n\n\n# We define a Product schema so that the name, description, and price are generated\n# in one go, with the types and constraints specified.\nclass Product(BaseModel):\n    name: str = Field(description=\"The name of the product\")\n    description: str = Field(description=\"A description of the product\")\n    price: Decimal = Field(description=\"The price of the product\", ge=10, le=1000, decimal_places=2)\n\n\nclass ProductReview(BaseModel):\n    rating: int = Field(description=\"The rating of the product\", ge=1, le=5)\n    customer_mood: Literal[\"irritated\", \"mad\", \"happy\", \"neutral\", \"excited\"] = Field(\n        description=\"The mood of the customer\"\n    )\n    review: str = Field(description=\"A review of the product\")\n</pre> from decimal import Decimal from typing import Literal  from pydantic import BaseModel, Field   # We define a Product schema so that the name, description, and price are generated # in one go, with the types and constraints specified. class Product(BaseModel):     name: str = Field(description=\"The name of the product\")     description: str = Field(description=\"A description of the product\")     price: Decimal = Field(description=\"The price of the product\", ge=10, le=1000, decimal_places=2)   class ProductReview(BaseModel):     rating: int = Field(description=\"The rating of the product\", ge=1, le=5)     customer_mood: Literal[\"irritated\", \"mad\", \"happy\", \"neutral\", \"excited\"] = Field(         description=\"The mood of the customer\"     )     review: str = Field(description=\"A review of the product\") <p>Next, let's design our product review dataset using a few more tricks compared to the previous notebook.</p> In\u00a0[\u00a0]: Copied! <pre># Since we often only want a few attributes from Person objects, we can\n# set drop=True in the column config to drop the column from the final dataset.\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=dd.SamplerType.PERSON_FROM_FAKER,\n        params=dd.PersonFromFakerSamplerParams(),\n        drop=True,\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"product_category\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\n                \"Electronics\",\n                \"Clothing\",\n                \"Home &amp; Kitchen\",\n                \"Books\",\n                \"Home Office\",\n            ],\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"product_subcategory\",\n        sampler_type=dd.SamplerType.SUBCATEGORY,\n        params=dd.SubcategorySamplerParams(\n            category=\"product_category\",\n            values={\n                \"Electronics\": [\n                    \"Smartphones\",\n                    \"Laptops\",\n                    \"Headphones\",\n                    \"Cameras\",\n                    \"Accessories\",\n                ],\n                \"Clothing\": [\n                    \"Men's Clothing\",\n                    \"Women's Clothing\",\n                    \"Winter Coats\",\n                    \"Activewear\",\n                    \"Accessories\",\n                ],\n                \"Home &amp; Kitchen\": [\n                    \"Appliances\",\n                    \"Cookware\",\n                    \"Furniture\",\n                    \"Decor\",\n                    \"Organization\",\n                ],\n                \"Books\": [\n                    \"Fiction\",\n                    \"Non-Fiction\",\n                    \"Self-Help\",\n                    \"Textbooks\",\n                    \"Classics\",\n                ],\n                \"Home Office\": [\n                    \"Desks\",\n                    \"Chairs\",\n                    \"Storage\",\n                    \"Office Supplies\",\n                    \"Lighting\",\n                ],\n            },\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"target_age_range\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),\n    )\n)\n\n# Sampler columns support conditional params, which are used if the condition is met.\n# In this example, we set the review style to rambling if the target age range is 18-25.\n# Note conditional parameters are only supported for Sampler column types.\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"review_style\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],\n            weights=[1, 2, 2, 1],\n        ),\n        conditional_params={\n            \"target_age_range == '18-25'\": dd.CategorySamplerParams(values=[\"rambling\"]),\n        },\n    )\n)\n\n# Optionally validate that the columns are configured correctly.\ndata_designer.validate(config_builder)\n</pre> # Since we often only want a few attributes from Person objects, we can # set drop=True in the column config to drop the column from the final dataset. config_builder.add_column(     dd.SamplerColumnConfig(         name=\"customer\",         sampler_type=dd.SamplerType.PERSON_FROM_FAKER,         params=dd.PersonFromFakerSamplerParams(),         drop=True,     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"product_category\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(             values=[                 \"Electronics\",                 \"Clothing\",                 \"Home &amp; Kitchen\",                 \"Books\",                 \"Home Office\",             ],         ),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"product_subcategory\",         sampler_type=dd.SamplerType.SUBCATEGORY,         params=dd.SubcategorySamplerParams(             category=\"product_category\",             values={                 \"Electronics\": [                     \"Smartphones\",                     \"Laptops\",                     \"Headphones\",                     \"Cameras\",                     \"Accessories\",                 ],                 \"Clothing\": [                     \"Men's Clothing\",                     \"Women's Clothing\",                     \"Winter Coats\",                     \"Activewear\",                     \"Accessories\",                 ],                 \"Home &amp; Kitchen\": [                     \"Appliances\",                     \"Cookware\",                     \"Furniture\",                     \"Decor\",                     \"Organization\",                 ],                 \"Books\": [                     \"Fiction\",                     \"Non-Fiction\",                     \"Self-Help\",                     \"Textbooks\",                     \"Classics\",                 ],                 \"Home Office\": [                     \"Desks\",                     \"Chairs\",                     \"Storage\",                     \"Office Supplies\",                     \"Lighting\",                 ],             },         ),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"target_age_range\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),     ) )  # Sampler columns support conditional params, which are used if the condition is met. # In this example, we set the review style to rambling if the target age range is 18-25. # Note conditional parameters are only supported for Sampler column types. config_builder.add_column(     dd.SamplerColumnConfig(         name=\"review_style\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(             values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],             weights=[1, 2, 2, 1],         ),         conditional_params={             \"target_age_range == '18-25'\": dd.CategorySamplerParams(values=[\"rambling\"]),         },     ) )  # Optionally validate that the columns are configured correctly. data_designer.validate(config_builder) <p>Next, we will use more advanced Jinja expressions to create new columns.</p> <p>Jinja expressions let you:</p> <ul> <li><p>Access nested attributes: <code>{{ customer.first_name }}</code></p> </li> <li><p>Combine values: <code>{{ customer.first_name }} {{ customer.last_name }}</code></p> </li> <li><p>Use conditional logic: <code>{% if condition %}...{% endif %}</code></p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre># We can create new columns using Jinja expressions that reference\n# existing columns, including attributes of nested objects.\nconfig_builder.add_column(\n    dd.ExpressionColumnConfig(name=\"customer_name\", expr=\"{{ customer.first_name }} {{ customer.last_name }}\")\n)\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"customer_age\", expr=\"{{ customer.age }}\"))\n\nconfig_builder.add_column(\n    dd.LLMStructuredColumnConfig(\n        name=\"product\",\n        prompt=(\n            \"Create a product in the '{{ product_category }}' category, focusing on products  \"\n            \"related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"\n            \"{{ target_age_range }} years old. The product should be priced between $10 and $1000.\"\n        ),\n        output_format=Product,\n        model_alias=MODEL_ALIAS,\n    )\n)\n\n# We can even use if/else logic in our Jinja expressions to create more complex prompt patterns.\nconfig_builder.add_column(\n    dd.LLMStructuredColumnConfig(\n        name=\"customer_review\",\n        prompt=(\n            \"Your task is to write a review for the following product:\\n\\n\"\n            \"Product Name: {{ product.name }}\\n\"\n            \"Product Description: {{ product.description }}\\n\"\n            \"Price: {{ product.price }}\\n\\n\"\n            \"Imagine your name is {{ customer_name }} and you are from {{ customer.city }}, {{ customer.state }}. \"\n            \"Write the review in a style that is '{{ review_style }}'.\"\n            \"{% if target_age_range == '18-25' %}\"\n            \"Make sure the review is more informal and conversational.\\n\"\n            \"{% else %}\"\n            \"Make sure the review is more formal and structured.\\n\"\n            \"{% endif %}\"\n            \"The review field should contain only the review, no other text.\"\n        ),\n        output_format=ProductReview,\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> # We can create new columns using Jinja expressions that reference # existing columns, including attributes of nested objects. config_builder.add_column(     dd.ExpressionColumnConfig(name=\"customer_name\", expr=\"{{ customer.first_name }} {{ customer.last_name }}\") )  config_builder.add_column(dd.ExpressionColumnConfig(name=\"customer_age\", expr=\"{{ customer.age }}\"))  config_builder.add_column(     dd.LLMStructuredColumnConfig(         name=\"product\",         prompt=(             \"Create a product in the '{{ product_category }}' category, focusing on products  \"             \"related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"             \"{{ target_age_range }} years old. The product should be priced between $10 and $1000.\"         ),         output_format=Product,         model_alias=MODEL_ALIAS,     ) )  # We can even use if/else logic in our Jinja expressions to create more complex prompt patterns. config_builder.add_column(     dd.LLMStructuredColumnConfig(         name=\"customer_review\",         prompt=(             \"Your task is to write a review for the following product:\\n\\n\"             \"Product Name: {{ product.name }}\\n\"             \"Product Description: {{ product.description }}\\n\"             \"Price: {{ product.price }}\\n\\n\"             \"Imagine your name is {{ customer_name }} and you are from {{ customer.city }}, {{ customer.state }}. \"             \"Write the review in a style that is '{{ review_style }}'.\"             \"{% if target_age_range == '18-25' %}\"             \"Make sure the review is more informal and conversational.\\n\"             \"{% else %}\"             \"Make sure the review is more formal and structured.\\n\"             \"{% endif %}\"             \"The review field should contain only the review, no other text.\"         ),         output_format=ProductReview,         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-2\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-2\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#data-designer-tutorial-structured-outputs-and-jinja-expressions","title":"\ud83c\udfa8 Data Designer Tutorial: Structured Outputs and Jinja Expressions\u00b6","text":""},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>In this notebook, we will continue our exploration of Data Designer, demonstrating more advanced data generation using structured outputs and Jinja expressions.</p> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#import-data-designer","title":"\ud83d\udce6 Import Data Designer\u00b6","text":"<ul> <li><p><code>data_designer.config</code> provides access to the configuration API.</p> </li> <li><p><code>DataDesigner</code> is the main interface for data generation.</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#colab-setup","title":"\u26a1 Colab Setup\u00b6","text":"<p>Run the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from build.nvidia.com.</p>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object that is used to interface with the library.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#designing-our-data","title":"\ud83e\uddd1\u200d\ud83c\udfa8 Designing our data\u00b6","text":"<ul> <li><p>We will again create a product review dataset, but this time we will use structured outputs and Jinja expressions.</p> </li> <li><p>Structured outputs let you specify the exact schema of the data you want to generate.</p> </li> <li><p>Data Designer supports schemas specified using either json schema or Pydantic data models (recommended).</p> </li> </ul> <p>We'll define our structured outputs using Pydantic data models</p> <p>\ud83d\udca1 Why Pydantic?</p> <ul> <li><p>Pydantic models provide better IDE support and type validation.</p> </li> <li><p>They are more Pythonic than raw JSON schemas.</p> </li> <li><p>They integrate seamlessly with Data Designer's structured output system.</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"colab_notebooks/2-structured-outputs-and-jinja-expressions/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Check out the following notebook to learn more about:</p> <ul> <li><p>Seeding synthetic data generation with an external dataset</p> </li> <li><p>Providing images as context</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/","title":"3 seeding with a dataset","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install -U data-designer\n</pre> %%capture !pip install -U data-designer In\u00a0[\u00a0]: Copied! <pre>import getpass\nimport os\n\nfrom google.colab import userdata\n\ntry:\n    os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\nexcept userdata.SecretNotFoundError:\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")\n</pre> import getpass import os  from google.colab import userdata  try:     os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\") except userdata.SecretNotFoundError:     os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \") In\u00a0[\u00a0]: Copied! <pre>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n</pre> import data_designer.config as dd from data_designer.interface import DataDesigner In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    dd.ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     dd.ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=dd.ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[\u00a0]: Copied! <pre>config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre># Download sample dataset from Github\nimport urllib.request\n\nurl = \"https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/refs/heads/main/nemo/NeMo-Data-Designer/data/gretelai_symptom_to_diagnosis.csv\"\nlocal_filename, _ = urllib.request.urlretrieve(url, \"gretelai_symptom_to_diagnosis.csv\")\n\n# Seed datasets are passed as reference objects to the config builder.\nseed_source = dd.LocalFileSeedSource(path=local_filename)\n\nconfig_builder.with_seed_dataset(seed_source)\n</pre> # Download sample dataset from Github import urllib.request  url = \"https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/refs/heads/main/nemo/NeMo-Data-Designer/data/gretelai_symptom_to_diagnosis.csv\" local_filename, _ = urllib.request.urlretrieve(url, \"gretelai_symptom_to_diagnosis.csv\")  # Seed datasets are passed as reference objects to the config builder. seed_source = dd.LocalFileSeedSource(path=local_filename)  config_builder.with_seed_dataset(seed_source) In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"patient_sampler\",\n        sampler_type=dd.SamplerType.PERSON_FROM_FAKER,\n        params=dd.PersonFromFakerSamplerParams(),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"doctor_sampler\",\n        sampler_type=dd.SamplerType.PERSON_FROM_FAKER,\n        params=dd.PersonFromFakerSamplerParams(),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"patient_id\",\n        sampler_type=dd.SamplerType.UUID,\n        params=dd.UUIDSamplerParams(\n            prefix=\"PT-\",\n            short_form=True,\n            uppercase=True,\n        ),\n    )\n)\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"first_name\", expr=\"{{ patient_sampler.first_name }}\"))\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"last_name\", expr=\"{{ patient_sampler.last_name }}\"))\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"dob\", expr=\"{{ patient_sampler.birth_date }}\"))\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"symptom_onset_date\",\n        sampler_type=dd.SamplerType.DATETIME,\n        params=dd.DatetimeSamplerParams(start=\"2024-01-01\", end=\"2024-12-31\"),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"date_of_visit\",\n        sampler_type=dd.SamplerType.TIMEDELTA,\n        params=dd.TimeDeltaSamplerParams(dt_min=1, dt_max=30, reference_column_name=\"symptom_onset_date\"),\n    )\n)\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"physician\", expr=\"Dr. {{ doctor_sampler.last_name }}\"))\n\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"physician_notes\",\n        prompt=\"\"\"\\\nYou are a primary-care physician who just had an appointment with {{ first_name }} {{ last_name }},\nwho has been struggling with symptoms from {{ diagnosis }} since {{ symptom_onset_date }}.\nThe date of today's visit is {{ date_of_visit }}.\n\n{{ patient_summary }}\n\nWrite careful notes about your visit with {{ first_name }},\nas Dr. {{ doctor_sampler.first_name }} {{ doctor_sampler.last_name }}.\n\nFormat the notes as a busy doctor might.\nRespond with only the notes, no other text.\n\"\"\",\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     dd.SamplerColumnConfig(         name=\"patient_sampler\",         sampler_type=dd.SamplerType.PERSON_FROM_FAKER,         params=dd.PersonFromFakerSamplerParams(),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"doctor_sampler\",         sampler_type=dd.SamplerType.PERSON_FROM_FAKER,         params=dd.PersonFromFakerSamplerParams(),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"patient_id\",         sampler_type=dd.SamplerType.UUID,         params=dd.UUIDSamplerParams(             prefix=\"PT-\",             short_form=True,             uppercase=True,         ),     ) )  config_builder.add_column(dd.ExpressionColumnConfig(name=\"first_name\", expr=\"{{ patient_sampler.first_name }}\"))  config_builder.add_column(dd.ExpressionColumnConfig(name=\"last_name\", expr=\"{{ patient_sampler.last_name }}\"))  config_builder.add_column(dd.ExpressionColumnConfig(name=\"dob\", expr=\"{{ patient_sampler.birth_date }}\"))  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"symptom_onset_date\",         sampler_type=dd.SamplerType.DATETIME,         params=dd.DatetimeSamplerParams(start=\"2024-01-01\", end=\"2024-12-31\"),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"date_of_visit\",         sampler_type=dd.SamplerType.TIMEDELTA,         params=dd.TimeDeltaSamplerParams(dt_min=1, dt_max=30, reference_column_name=\"symptom_onset_date\"),     ) )  config_builder.add_column(dd.ExpressionColumnConfig(name=\"physician\", expr=\"Dr. {{ doctor_sampler.last_name }}\"))  config_builder.add_column(     dd.LLMTextColumnConfig(         name=\"physician_notes\",         prompt=\"\"\"\\ You are a primary-care physician who just had an appointment with {{ first_name }} {{ last_name }}, who has been struggling with symptoms from {{ diagnosis }} since {{ symptom_onset_date }}. The date of today's visit is {{ date_of_visit }}.  {{ patient_summary }}  Write careful notes about your visit with {{ first_name }}, as Dr. {{ doctor_sampler.first_name }} {{ doctor_sampler.last_name }}.  Format the notes as a busy doctor might. Respond with only the notes, no other text. \"\"\",         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-3\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-3\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#data-designer-tutorial-seeding-synthetic-data-generation-with-an-external-dataset","title":"\ud83c\udfa8 Data Designer Tutorial: Seeding Synthetic Data Generation with an External Dataset\u00b6","text":""},{"location":"colab_notebooks/3-seeding-with-a-dataset/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>In this notebook, we will demonstrate how to seed synthetic data generation in Data Designer with an external dataset.</p> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#import-data-designer","title":"\ud83d\udce6 Import Data Designer\u00b6","text":"<ul> <li><p><code>data_designer.config</code> provides access to the configuration API.</p> </li> <li><p><code>DataDesigner</code> is the main interface for data generation.</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#colab-setup","title":"\u26a1 Colab Setup\u00b6","text":"<p>Run the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from build.nvidia.com.</p>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#prepare-a-seed-dataset","title":"\ud83c\udfe5 Prepare a seed dataset\u00b6","text":"<ul> <li><p>For this notebook, we'll create a synthetic dataset of patient notes.</p> </li> <li><p>We will seed the generation process with a symptom-to-diagnosis dataset.</p> </li> <li><p>We already have the dataset downloaded in the data directory of this repository.</p> </li> </ul> <p>\ud83c\udf31 Why use a seed dataset?</p> <ul> <li><p>Seed datasets let you steer the generation process by providing context that is specific to your use case.</p> </li> <li><p>Seed datasets are also an excellent way to inject real-world diversity into your synthetic data.</p> </li> <li><p>During generation, prompt templates can reference any of the seed dataset fields.</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#designing-our-synthetic-patient-notes-dataset","title":"\ud83c\udfa8 Designing our synthetic patient notes dataset\u00b6","text":"<ul> <li>The prompt template can reference fields from our seed dataset:<ul> <li><code>{{ diagnosis }}</code> - the medical diagnosis from the seed data</li> <li><code>{{ patient_summary }}</code> - the symptom description from the seed data</li> </ul> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"colab_notebooks/3-seeding-with-a-dataset/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Check out the following notebook to learn more about:</p> <ul> <li>Providing images as context</li> </ul>"},{"location":"colab_notebooks/4-providing-images-as-context/","title":"4 providing images as context","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install -U data-designer \"pillow&gt;=12.0.0,&lt;13\" \"datasets&gt;=4.0.0,&lt;5\"\n</pre> %%capture !pip install -U data-designer \"pillow&gt;=12.0.0,&lt;13\" \"datasets&gt;=4.0.0,&lt;5\" In\u00a0[\u00a0]: Copied! <pre>import getpass\nimport os\n\nfrom google.colab import userdata\n\ntry:\n    os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\nexcept userdata.SecretNotFoundError:\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")\n</pre> import getpass import os  from google.colab import userdata  try:     os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\") except userdata.SecretNotFoundError:     os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \") In\u00a0[\u00a0]: Copied! <pre># Standard library imports\nimport base64\nimport io\nimport uuid\n\n# Third-party imports\nimport pandas as pd\nimport rich\nfrom datasets import load_dataset\nfrom IPython.display import display\nfrom rich.panel import Panel\n\n# Data Designer imports\nimport data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n</pre> # Standard library imports import base64 import io import uuid  # Third-party imports import pandas as pd import rich from datasets import load_dataset from IPython.display import display from rich.panel import Panel  # Data Designer imports import data_designer.config as dd from data_designer.interface import DataDesigner In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\nmodel_configs = [\n    dd.ModelConfig(\n        alias=\"vision\",\n        model=\"meta/llama-4-scout-17b-16e-instruct\",\n        provider=MODEL_PROVIDER,\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=0.60,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  model_configs = [     dd.ModelConfig(         alias=\"vision\",         model=\"meta/llama-4-scout-17b-16e-instruct\",         provider=MODEL_PROVIDER,         inference_parameters=dd.ChatCompletionInferenceParams(             temperature=0.60,             top_p=0.95,             max_tokens=2048,         ),     ), ] In\u00a0[\u00a0]: Copied! <pre>config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre># Dataset processing configuration\nIMG_COUNT = 512  # Number of images to process\nBASE64_IMAGE_HEIGHT = 512  # Standardized height for model input\n\n# Load ColPali dataset for visual documents\nimg_dataset_cfg = {\"path\": \"vidore/colpali_train_set\", \"split\": \"train\", \"streaming\": True}\n</pre> # Dataset processing configuration IMG_COUNT = 512  # Number of images to process BASE64_IMAGE_HEIGHT = 512  # Standardized height for model input  # Load ColPali dataset for visual documents img_dataset_cfg = {\"path\": \"vidore/colpali_train_set\", \"split\": \"train\", \"streaming\": True} In\u00a0[\u00a0]: Copied! <pre>def resize_image(image, height: int):\n    \"\"\"\n    Resize image while maintaining aspect ratio.\n\n    Args:\n        image: PIL Image object\n        height: Target height in pixels\n\n    Returns:\n        Resized PIL Image object\n    \"\"\"\n    original_width, original_height = image.size\n    width = int(original_width * (height / original_height))\n    return image.resize((width, height))\n\n\ndef convert_image_to_chat_format(record, height: int) -&gt; dict:\n    \"\"\"\n    Convert PIL image to base64 format for chat template usage.\n\n    Args:\n        record: Dataset record containing image and metadata\n        height: Target height for image resizing\n\n    Returns:\n        Updated record with base64_image and uuid fields\n    \"\"\"\n    # Resize image for consistent processing\n    image = resize_image(record[\"image\"], height)\n\n    # Convert to base64 string\n    img_buffer = io.BytesIO()\n    image.save(img_buffer, format=\"PNG\")\n    byte_data = img_buffer.getvalue()\n    base64_encoded_data = base64.b64encode(byte_data)\n    base64_string = base64_encoded_data.decode(\"utf-8\")\n\n    # Return updated record\n    return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())}\n</pre> def resize_image(image, height: int):     \"\"\"     Resize image while maintaining aspect ratio.      Args:         image: PIL Image object         height: Target height in pixels      Returns:         Resized PIL Image object     \"\"\"     original_width, original_height = image.size     width = int(original_width * (height / original_height))     return image.resize((width, height))   def convert_image_to_chat_format(record, height: int) -&gt; dict:     \"\"\"     Convert PIL image to base64 format for chat template usage.      Args:         record: Dataset record containing image and metadata         height: Target height for image resizing      Returns:         Updated record with base64_image and uuid fields     \"\"\"     # Resize image for consistent processing     image = resize_image(record[\"image\"], height)      # Convert to base64 string     img_buffer = io.BytesIO()     image.save(img_buffer, format=\"PNG\")     byte_data = img_buffer.getvalue()     base64_encoded_data = base64.b64encode(byte_data)     base64_string = base64_encoded_data.decode(\"utf-8\")      # Return updated record     return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())} In\u00a0[\u00a0]: Copied! <pre># Load and process the visual document dataset\nprint(\"\ud83d\udce5 Loading and processing document images...\")\n\nimg_dataset_iter = iter(\n    load_dataset(**img_dataset_cfg).map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT})\n)\nimg_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])\n\nprint(f\"\u2705 Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\")\n</pre> # Load and process the visual document dataset print(\"\ud83d\udce5 Loading and processing document images...\")  img_dataset_iter = iter(     load_dataset(**img_dataset_cfg).map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT}) ) img_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])  print(f\"\u2705 Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\") In\u00a0[\u00a0]: Copied! <pre>img_dataset.head()\n</pre> img_dataset.head() In\u00a0[\u00a0]: Copied! <pre># Add the seed dataset containing our processed images\ndf_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]]\nconfig_builder.with_seed_dataset(dd.DataFrameSeedSource(df=df_seed))\n</pre> # Add the seed dataset containing our processed images df_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]] config_builder.with_seed_dataset(dd.DataFrameSeedSource(df=df_seed)) In\u00a0[\u00a0]: Copied! <pre># Add a column to generate detailed document summaries\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"summary\",\n        model_alias=\"vision\",\n        prompt=(\n            \"Provide a detailed summary of the content in this image in Markdown format. \"\n            \"Start from the top of the image and then describe it from top to bottom. \"\n            \"Place a summary at the bottom.\"\n        ),\n        multi_modal_context=[\n            dd.ImageContext(\n                column_name=\"base64_image\",\n                data_type=dd.ModalityDataType.BASE64,\n                image_format=dd.ImageFormat.PNG,\n            )\n        ],\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> # Add a column to generate detailed document summaries config_builder.add_column(     dd.LLMTextColumnConfig(         name=\"summary\",         model_alias=\"vision\",         prompt=(             \"Provide a detailed summary of the content in this image in Markdown format. \"             \"Start from the top of the image and then describe it from top to bottom. \"             \"Place a summary at the bottom.\"         ),         multi_modal_context=[             dd.ImageContext(                 column_name=\"base64_image\",                 data_type=dd.ModalityDataType.BASE64,                 image_format=dd.ImageFormat.PNG,             )         ],     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre># Compare original document with generated summary\nindex = 0  # Change this to view different examples\n\n# Merge preview data with original images for comparison\ncomparison_dataset = preview.dataset.merge(pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\")\n\n# Extract the record for display\nrecord = comparison_dataset.iloc[index]\n\nprint(\"\ud83d\udcc4 Original Document Image:\")\ndisplay(resize_image(record.image, BASE64_IMAGE_HEIGHT))\n\nprint(\"\\n\ud83d\udcdd Generated Summary:\")\nrich.print(Panel(record.summary, title=\"Document Summary\", title_align=\"left\"))\n</pre> # Compare original document with generated summary index = 0  # Change this to view different examples  # Merge preview data with original images for comparison comparison_dataset = preview.dataset.merge(pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\")  # Extract the record for display record = comparison_dataset.iloc[index]  print(\"\ud83d\udcc4 Original Document Image:\") display(resize_image(record.image, BASE64_IMAGE_HEIGHT))  print(\"\\n\ud83d\udcdd Generated Summary:\") rich.print(Panel(record.summary, title=\"Document Summary\", title_align=\"left\")) In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-4\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-4\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"colab_notebooks/4-providing-images-as-context/#data-designer-tutorial-providing-images-as-context-for-vision-based-data-generation","title":"\ud83c\udfa8 Data Designer Tutorial: Providing Images as Context for Vision-Based Data Generation\u00b6","text":""},{"location":"colab_notebooks/4-providing-images-as-context/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>This notebook demonstrates how to provide images as context to generate text descriptions using vision-language models.</p> <ul> <li>\u2728 Visual Document Processing: Converting images to chat-ready format for model consumption</li> <li>\ud83d\udd0d Vision-Language Generation: Using vision models to generate detailed summaries from images</li> </ul> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"colab_notebooks/4-providing-images-as-context/#import-data-designer","title":"\ud83d\udce6 Import Data Designer\u00b6","text":"<ul> <li><p><code>data_designer.config</code> provides access to the configuration API.</p> </li> <li><p><code>DataDesigner</code> is the main interface for data generation.</p> </li> </ul>"},{"location":"colab_notebooks/4-providing-images-as-context/#colab-setup","title":"\u26a1 Colab Setup\u00b6","text":"<p>Run the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from build.nvidia.com.</p>"},{"location":"colab_notebooks/4-providing-images-as-context/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"colab_notebooks/4-providing-images-as-context/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"colab_notebooks/4-providing-images-as-context/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"colab_notebooks/4-providing-images-as-context/#seed-dataset-creation","title":"\ud83c\udf31 Seed Dataset Creation\u00b6","text":"<p>In this section, we'll prepare our visual documents as a seed dataset for summarization:</p> <ul> <li>Loading Visual Documents: We use the ColPali dataset containing document images</li> <li>Image Processing: Convert images to base64 format for vision model consumption</li> <li>Metadata Extraction: Preserve relevant document information (filename, page number, source, etc.)</li> </ul> <p>The seed dataset will be used to generate detailed text summaries of each document image.</p>"},{"location":"colab_notebooks/4-providing-images-as-context/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013 preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"colab_notebooks/4-providing-images-as-context/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"colab_notebooks/4-providing-images-as-context/#visual-inspection","title":"\ud83d\udd0e Visual Inspection\u00b6","text":"<p>Let's compare the original document image with the generated summary to validate quality:</p>"},{"location":"colab_notebooks/4-providing-images-as-context/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"colab_notebooks/4-providing-images-as-context/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Now that you've learned how to use visual context for image summarization in Data Designer, explore more:</p> <ul> <li>Experiment with different vision models for specific document types</li> <li>Try different prompt variations to generate specialized descriptions (e.g., technical details, key findings)</li> <li>Combine vision-based summaries with other column types for multi-modal workflows</li> <li>Apply this pattern to other vision tasks like image captioning, OCR validation, or visual question answering</li> </ul>"},{"location":"concepts/architecture-and-performance/","title":"\ud83c\udfd7\ufe0f Architecture &amp; Performance","text":"<p>Data Designer is an orchestration framework that coordinates synthetic data generation workflows. It is a client of LLM inference servers\u2014it does not host models itself.</p> <p>This guide explains the architecture, execution model, and how to tune performance for your specific use case.</p>"},{"location":"concepts/architecture-and-performance/#separation-of-concerns","title":"Separation of Concerns","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Data Designer               \u2502          \u2502       Inference Server(s)           \u2502\n\u2502         (Orchestration)             \u2502  HTTP    \u2502       (LLM Hosting)                 \u2502\n\u2502                                     \u2502  \u2500\u2500\u2500\u2500\u2500\u25ba  \u2502                                     \u2502\n\u2502  \u2022 Dataset workflow management      \u2502          \u2502  \u2022 Model weights and execution      \u2502\n\u2502  \u2022 Column dependency resolution     \u2502          \u2502  \u2022 GPU allocation and scheduling    \u2502\n\u2502  \u2022 Batching and parallelism         \u2502          \u2502  \u2022 Request queuing                  \u2502\n\u2502  \u2022 Retry and error handling         \u2502          \u2502  \u2022 Token generation                 \u2502\n\u2502  \u2022 Data validation and quality      \u2502          \u2502  \u2022 Rate limiting (optional)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u25b2                                                    \u25b2\n              \u2502                                                    \u2502\n        Your workflow                                    Your infrastructure\n         configuration                                    (or cloud API)\n</code></pre>"},{"location":"concepts/architecture-and-performance/#what-data-designer-does","title":"What Data Designer Does","text":"<ul> <li>Orchestrates the generation workflow across multiple columns</li> <li>Resolves dependencies between columns (DAG-based execution)</li> <li>Batches work into manageable chunks (<code>buffer_size</code>)</li> <li>Parallelizes LLM calls within batches (<code>max_parallel_requests</code>)</li> <li>Handles errors with retries and early shutdown logic</li> <li>Validates generated data against schemas and constraints</li> </ul>"},{"location":"concepts/architecture-and-performance/#what-data-designer-does-not-do","title":"What Data Designer Does NOT Do","text":"<ul> <li>Host models: You must provide LLM endpoints</li> <li>Manage GPUs: Your inference server handles GPU allocation</li> <li>Scale inference: You must provision sufficient capacity</li> <li>Rate limit: Your server or API gateway handles this</li> </ul>"},{"location":"concepts/architecture-and-performance/#execution-model","title":"Execution Model","text":"<p>Column-Wise Generator</p> <p>This describes Data Designer's current column-wise dataset generator. Other dataset generation strategies are in development.</p> <p>Data Designer processes datasets in batches, with parallel operations within each batch.</p>"},{"location":"concepts/architecture-and-performance/#how-it-works","title":"How It Works","text":"<p>Step 1: Split into batches</p> <p>Your dataset is divided into batches of <code>buffer_size</code> records. Each batch is processed completely before moving to the next.</p> <p>Step 2: Process columns sequentially</p> <p>Within a batch, columns are generated one at a time following the dependency graph. The order depends on column dependencies\u2014expression columns may come before LLM columns if the LLM columns depend on them.</p> <p>Example workflow:</p> <pre><code>Batch 1 (100 records)\n\u2502\n\u251c\u2500\u25ba Column 1: category (Sampler)      \u2500\u2500\u2500\u2500 All 100 values generated\n\u251c\u2500\u25ba Column 2: prompt (LLM Text)       \u2500\u2500\u2500\u2500 All 100 values generated\n\u251c\u2500\u25ba Column 3: response (LLM Text)     \u2500\u2500\u2500\u2500 All 100 values generated\n\u251c\u2500\u25ba Column 4: score (Expression)      \u2500\u2500\u2500\u2500 All 100 values computed\n\u2502\n\u2514\u2500\u25ba Write batch to disk\n    \u2502\n    \u25bc\nBatch 2 (100 records)\n    ...repeat...\n</code></pre> <p>Step 3: Generate cells in parallel</p> <p>Within each column, cells are processed in parallel up to the configured limit:</p> Column Type Parallelism Control Sampler <code>non_inference_max_parallel_workers</code> LLM (Text, Code, Structured, Judge) <code>max_parallel_requests</code> Expression Sequential (fast, CPU-bound)"},{"location":"concepts/architecture-and-performance/#key-concepts","title":"Key Concepts","text":"Concept Description Batching Records are split into batches of <code>buffer_size</code>. Each batch completes entirely before the next begins. Sequential columns Within a batch, columns are generated one at a time, respecting the dependency graph. Parallel cells Within a column, individual cells (records) are generated in parallel up to the configured limit."},{"location":"concepts/architecture-and-performance/#concurrency-formula","title":"Concurrency Formula","text":"<p>At any moment, the number of concurrent LLM requests is:</p> <pre><code>concurrent_requests = min(\n    buffer_size,                # Records in current batch\n    max_parallel_requests,      # Per-model limit\n    remaining_cells_in_column   # Cells left to generate\n)\n</code></pre> <p>Example: With <code>buffer_size=100</code> and <code>max_parallel_requests=8</code>, Data Designer sends up to 8 LLM requests at a time until all 100 cells in the column are complete.</p>"},{"location":"concepts/architecture-and-performance/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"concepts/architecture-and-performance/#buffer_size-runconfig","title":"<code>buffer_size</code> (RunConfig)","text":"<p>Controls how many records are processed per batch.</p> <pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\nrun_config = dd.RunConfig(buffer_size=2000)\n\ndesigner = DataDesigner()\ndesigner.set_run_config(run_config)\n</code></pre> Value Memory Usage Throughput Error Feedback Low (100-500) Lower May not saturate inference Fast Default (1000) Moderate Good for most cases Moderate High (2000-5000) Higher Better for deep pipelines Slower <p>When to increase: High-capacity inference server, single-model workflows, memory not constrained</p> <p>When to decrease: Memory-constrained environments, development/debugging, complex multi-model pipelines</p>"},{"location":"concepts/architecture-and-performance/#max_parallel_requests-inferenceparams","title":"<code>max_parallel_requests</code> (InferenceParams)","text":"<p>Controls concurrent LLM API calls per model alias.</p> <pre><code>import data_designer.config as dd\n\nmodel = dd.ModelConfig(\n    alias=\"my-model\",\n    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n    inference_parameters=dd.ChatCompletionInferenceParams(\n        max_parallel_requests=8,\n    ),\n)\n</code></pre> <p>Default: 4</p> <p>When to increase: Your inference backend has high throughput capacity, you're using a cloud API with generous rate limits, or you're running vLLM/TensorRT-LLM with multiple GPUs</p> <p>When to decrease: You're hitting rate limits or 429 errors, the inference server is overloaded, or you want more predictable/debuggable execution</p> <p>Finding the optimal value</p> <p>The right value depends on your inference stack and model. Self-hosted vLLM servers can often handle values as high as 256, 512, or even 1024 depending on your hardware.</p> <p>Benchmark approach: Run a small dataset (e.g., 100 records) with increasing <code>max_parallel_requests</code> values (4 \u2192 8 \u2192 16 \u2192 32 \u2192 ...) and measure generation time. Stop increasing when the runtime stops decreasing\u2014that's when your inference server is saturated.</p>"},{"location":"concepts/architecture-and-performance/#non_inference_max_parallel_workers-runconfig","title":"<code>non_inference_max_parallel_workers</code> (RunConfig)","text":"<p>Controls thread pool size for non-LLM operations (samplers, expressions, validators).</p> <pre><code>run_config = dd.RunConfig(non_inference_max_parallel_workers=8)\ndesigner.set_run_config(run_config)\n</code></pre> <p>Default: 4</p> <p>When to increase: Many CPU-bound columns (complex expressions, heavy sampling)</p>"},{"location":"concepts/architecture-and-performance/#error-handling-runconfig","title":"Error Handling (RunConfig)","text":"<p>Control retry behavior and early shutdown for failed generations.</p> <pre><code>run_config = dd.RunConfig(\n    max_conversation_restarts=5,           # Full conversation restarts (default: 5)\n    max_conversation_correction_steps=0,   # In-conversation corrections (default: 0)\n    disable_early_shutdown=False,          # Enable early shutdown (default)\n    shutdown_error_rate=0.5,               # Shut down if &gt;50% errors\n    shutdown_error_window=10,              # Min tasks before error monitoring\n)\ndesigner.set_run_config(run_config)\n</code></pre> <p>When to adjust:</p> <ul> <li>Strict schemas: Increase <code>max_conversation_restarts</code> to 7, add <code>max_conversation_correction_steps=2</code></li> <li>Debugging: Set <code>disable_early_shutdown=True</code> to see all errors</li> <li>Simple text: Reduce <code>max_conversation_restarts</code> to 3</li> </ul>"},{"location":"concepts/architecture-and-performance/#common-problems","title":"Common Problems","text":"Problem Symptom Solution Low throughput Low GPU utilization Increase <code>max_parallel_requests</code> and/or <code>buffer_size</code> Long tail of slow generations Most records fast, few very slow Reduce <code>max_conversation_restarts</code>, simplify schemas, improve prompts Multi-model idle periods One model busy, others idle Reduce <code>buffer_size</code> for faster cycling, or consolidate models Memory errors OOM crashes Reduce <code>buffer_size</code> and <code>max_parallel_requests</code> Too many errors Generation fails frequently Check prompts/schemas; adjust <code>shutdown_error_rate</code> or disable early shutdown for debugging"},{"location":"concepts/architecture-and-performance/#tuning-workflow","title":"Tuning Workflow","text":"<ol> <li>Start with defaults for initial development</li> <li>Profile your workload: How many LLM columns? How many records? What models?</li> <li>Identify bottleneck: Low GPU util \u2192 increase <code>max_parallel_requests</code>. Memory issues \u2192 decrease <code>buffer_size</code>. Long tails \u2192 tune retry settings.</li> <li>Iterate: Make one change at a time, measure impact before next change</li> </ol>"},{"location":"concepts/architecture-and-performance/#related-documentation","title":"Related Documentation","text":"<ul> <li>Deployment Options: Choosing between library and microservice</li> <li>Model Configuration: Complete model settings reference</li> <li>Inference Parameters: Detailed parameter reference</li> </ul>"},{"location":"concepts/columns/","title":"Columns","text":"<p>Columns are the fundamental building blocks in Data Designer. Each column represents a field in your dataset and defines how to generate it\u2014whether that's sampling from a distribution, calling an LLM, or applying a transformation.</p> <p>The Declarative Approach</p> <p>Columns are declarative specifications. You describe what you want, and the framework handles how to generate it\u2014managing execution order, batching, parallelization, and resources automatically.</p>"},{"location":"concepts/columns/#column-types","title":"Column Types","text":"<p>Data Designer provides ten built-in column types, each optimized for different generation scenarios.</p>"},{"location":"concepts/columns/#sampler-columns","title":"\ud83c\udfb2 Sampler Columns","text":"<p>Sampler columns generate data using numerical sampling\u2014fast, deterministic, and ideal for numerical and categorical dataset fields. They're significantly faster than LLMs and can produce data following specific distributions (Poisson for event counts, Gaussian for measurements, etc.).</p> <p>Available sampler types:</p> <ul> <li>UUID: Unique identifiers</li> <li>Category: Categorical values with optional probability weights</li> <li>Subcategory: Hierarchical categorical data (states within countries, models within brands)</li> <li>Uniform: Evenly distributed numbers (integers or floats)</li> <li>Gaussian: Normally distributed values with configurable mean and standard deviation</li> <li>Bernoulli: Binary outcomes with specified success probability</li> <li>Bernoulli Mixture: Binary outcomes from multiple probability components</li> <li>Binomial: Count of successes in repeated trials</li> <li>Poisson: Count data and event frequencies</li> <li>Scipy: Access to the full scipy.stats distribution library</li> <li>Person: Realistic synthetic individuals with names, demographics, and attributes</li> <li>Datetime: Timestamps within specified ranges</li> <li>Timedelta: Time duration values</li> </ul> <p>Conditional Sampling</p> <p>Samplers support conditional parameters that change behavior based on other columns. Want age distributions that vary by country? Income ranges that depend on occupation? Just define conditions on existing column values.</p>"},{"location":"concepts/columns/#llm-text-columns","title":"\ud83d\udcdd LLM-Text Columns","text":"<p>LLM-Text columns generate natural language text: product descriptions, customer reviews, narrative summaries, email threads, or anything requiring semantic understanding and creativity.</p> <p>Use Jinja2 templating in prompts to reference other columns. Data Designer automatically manages dependencies and injects the referenced column values into the prompt.</p> <p>Generation Traces</p> <p>LLM columns can optionally capture message traces in a separate <code>{column_name}__trace</code> column. Set <code>with_trace</code> on the column config to control what's captured: <code>TraceType.NONE</code> (default, no trace), <code>TraceType.LAST_MESSAGE</code> (final assistant message only), or <code>TraceType.ALL_MESSAGES</code> (full conversation history). The trace includes the ordered message history for the final generation attempt (system/user/assistant/tool calls/tool results), and may include model reasoning fields when the provider exposes them.</p> <p>Extracting Reasoning Content</p> <p>Some models expose chain-of-thought reasoning separately from the main response via a <code>reasoning_content</code> field. To capture only this reasoning (without the full trace), set <code>extract_reasoning_content=True</code>:</p> <pre><code>dd.LLMTextColumnConfig(\n    name=\"answer\",\n    model_alias=\"reasoning-model\",\n    prompt=\"Solve this problem: {{ problem }}\",\n    extract_reasoning_content=True,  # Creates answer__reasoning_content column\n)\n</code></pre> <p>This creates a <code>{column_name}__reasoning_content</code> column containing the stripped reasoning content from the final assistant response, or <code>None</code> if the model didn't provide reasoning. This is independent of <code>with_trace</code>\u2014you can use either or both.</p> <p>Tool Use in LLM Columns</p> <p>LLM columns can invoke external tools during generation via MCP (Model Context Protocol). Enable tools by setting <code>tool_alias</code> to reference a configured <code>ToolConfig</code>:</p> <pre><code>dd.LLMTextColumnConfig(\n    name=\"answer\",\n    model_alias=\"nvidia-text\",\n    prompt=\"Search for information and answer: {{ question }}\",\n    tool_alias=\"search-tools\",  # References a ToolConfig\n    with_trace=dd.TraceType.ALL_MESSAGES,  # Capture tool call history\n)\n</code></pre> <p>When <code>tool_alias</code> is set, the model can request tool calls during generation. Data Designer executes the tools via configured MCP providers and feeds results back until the model produces a final answer. See Tool Use &amp; MCP for full configuration details.</p> <p>Performance</p> <p>LLM columns are parallelized within each batch using <code>max_parallel_requests</code> from your model's inference parameters. See the Architecture &amp; Performance guide for optimization strategies.</p>"},{"location":"concepts/columns/#llm-code-columns","title":"\ud83d\udcbb LLM-Code Columns","text":"<p>LLM-Code columns generate code in specific programming languages. They handle the prompting and parsing necessary to extract clean code from the LLM's response\u2014automatically detecting and extracting code from markdown blocks. You provide the prompt and choose the model; the column handles the extraction.</p> <p>Supported languages: Bash, C, C++, C#, COBOL, Go, Java, JavaScript, Kotlin, Python, Ruby, Rust, Scala, Swift, TypeScript, plus SQL dialects (SQLite, PostgreSQL, MySQL, T-SQL, BigQuery, ANSI SQL).</p>"},{"location":"concepts/columns/#llm-structured-columns","title":"\ud83d\uddc2\ufe0f LLM-Structured Columns","text":"<p>LLM-Structured columns generate JSON with a guaranteed schema. Define your structure using a Pydantic model or JSON schema, and Data Designer ensures the LLM output conforms\u2014no parsing errors, no schema drift.</p> <p>Use for complex nested structures: API responses, configuration files, database records with multiple related fields, or any structured data where type safety matters. Schemas can be arbitrarily complex with nested objects, arrays, enums, and validation constraints, but success depends on the model's capabilities.</p> <p>Schema Complexity and Model Choice</p> <p>Flat schemas with simple fields are easier and more robustly produced across models. Deeply nested schemas with complex validation constraints are more sensitive to model choice\u2014stronger models handle complexity better. If you're experiencing schema conformance issues, try simplifying the schema or switching to a more capable model.</p>"},{"location":"concepts/columns/#llm-judge-columns","title":"\u2696\ufe0f LLM-Judge Columns","text":"<p>LLM-Judge columns score generated content across multiple quality dimensions using LLMs as evaluators.</p> <p>Define scoring rubrics (relevance, accuracy, fluency, helpfulness) and the judge model evaluates each record. Score rubrics specify criteria and scoring options (1-5 scales, categorical grades, etc.), producing quantified quality metrics for every data point.</p> <p>Use judge columns for data quality filtering (e.g., keep only 4+ rated responses), A/B testing generation strategies, and quality monitoring over time.</p>"},{"location":"concepts/columns/#embedding-columns","title":"\ud83e\uddec Embedding Columns","text":"<p>Embedding columns generate vector embeddings (numerical representations) for text content using embedding models. These embeddings capture semantic meaning, enabling similarity search, clustering, and semantic analysis.</p> <p>Specify a <code>target_column</code> containing text, and Data Designer generates embeddings for that content. The target column can contain either a single text string or a list of text strings in stringified JSON format. In the latter case, embeddings are generated for each text string in the list.</p> <p>Common use cases:</p> <ul> <li>Semantic search: Generate embeddings for documents, then find similar content by vector similarity</li> <li>Clustering: Group similar texts based on embedding proximity</li> <li>Recommendation systems: Match content by semantic similarity</li> <li>Anomaly detection: Identify outliers in embedding space</li> </ul> <p>Embedding Models</p> <p>Embedding columns require an embedding model configured with <code>EmbeddingInferenceParams</code>. These models differ from chat completion models\u2014they output vectors rather than text. The generation type is automatically determined by the inference parameters type.</p>"},{"location":"concepts/columns/#expression-columns","title":"\ud83e\udde9 Expression Columns","text":"<p>Expression columns handle simple transformations using Jinja2 templates\u2014concatenate first and last names, calculate numerical totals, format date strings. No LLM overhead needed.</p> <p>Template capabilities:</p> <ul> <li>Variable substitution: Pull values from any existing column</li> <li>String filters: Uppercase, lowercase, strip whitespace, replace patterns</li> <li>Conditional logic: if/elif/else support</li> <li>Arithmetic: Add, subtract, multiply, divide</li> </ul>"},{"location":"concepts/columns/#validation-columns","title":"\ud83d\udd0d Validation Columns","text":"<p>Validation columns check generated content against rules and return structured pass/fail results.</p> <p>Built-in validation types:</p> <p>Code validation runs Python or SQL code through a linter to validate the code.</p> <p>Local callable validation accepts a Python function directly when using Data Designer as a library.</p> <p>Remote validation sends data to HTTP endpoints for validation-as-a-service. Useful for linters, security scanners, or proprietary systems.</p>"},{"location":"concepts/columns/#seed-dataset-columns","title":"\ud83c\udf31 Seed Dataset Columns","text":"<p>Seed dataset columns bootstrap generation from existing data. Provide a real dataset, and those columns become available as context for generating new synthetic data.</p> <p>Typical pattern: use seed data for one part of your schema (real product names and categories), then generate synthetic fields around it (customer reviews, purchase histories, ratings). The seed data provides realism and constraints; generated columns add volume and variation.</p>"},{"location":"concepts/columns/#custom-columns","title":"\ud83d\udd27 Custom Columns","text":"<p>Custom columns let you implement your own generation logic using Python functions. Use the <code>@custom_column_generator</code> decorator to declare dependencies, and the framework handles DAG ordering and parallelization.</p> <p>Two generation strategies:</p> <ul> <li><code>cell_by_cell</code> (default): Function receives one row, framework parallelizes</li> <li><code>full_column</code>: Function receives entire DataFrame for vectorized operations</li> </ul> <p>For LLM access, declare <code>model_aliases</code> in the decorator and receive a <code>models</code> dict as the third argument. See Custom Columns for details.</p>"},{"location":"concepts/columns/#shared-column-properties","title":"Shared Column Properties","text":"<p>Every column configuration inherits from <code>SingleColumnConfig</code> with these standard properties:</p>"},{"location":"concepts/columns/#name","title":"<code>name</code>","text":"<p>The column's identifier\u2014unique within your configuration, used in Jinja2 references, and becomes the column name in the output DataFrame. Choose descriptive names: <code>user_review</code> &gt; <code>col_17</code>.</p>"},{"location":"concepts/columns/#drop","title":"<code>drop</code>","text":"<p>Boolean flag (default: <code>False</code>) controlling whether the column appears in final output. Setting <code>drop=True</code> generates the column (available as a dependency) but excludes it from final output.</p> <p>When to drop columns:</p> <ul> <li>Intermediate calculations that feed expressions but aren't meaningful standalone</li> <li>Context columns used only for LLM prompt templates</li> <li>Validation results during development unwanted in production</li> </ul> <p>Dropped columns participate fully in generation and the dependency graph\u2014just filtered out at the end.</p>"},{"location":"concepts/columns/#column_type","title":"<code>column_type</code>","text":"<p>Literal string identifying the column type: <code>\"sampler\"</code>, <code>\"llm-text\"</code>, <code>\"expression\"</code>, etc. Set automatically by each configuration class and serves as Pydantic's discriminator for deserialization.</p> <p>You rarely set this manually\u2014instantiating <code>LLMTextColumnConfig</code> automatically sets <code>column_type=\"llm-text\"</code>. Serialization is reversible: save to YAML, load later, and Pydantic reconstructs the exact objects.</p>"},{"location":"concepts/columns/#required_columns","title":"<code>required_columns</code>","text":"<p>Computed property listing columns that must be generated before this one. The framework derives this automatically:</p> <ul> <li>For LLM/Expression columns: extracted from Jinja2 template <code>{{ variables }}</code></li> <li>For Validation columns: explicitly listed target columns</li> <li>For Sampler columns with conditional parameters: columns referenced in conditions</li> </ul> <p>You read this property for introspection but never set it\u2014always computed from configuration details.</p>"},{"location":"concepts/columns/#side_effect_columns","title":"<code>side_effect_columns</code>","text":"<p>Computed property listing columns created implicitly alongside the primary column. Currently, only LLM columns produce side effects:</p> <ul> <li><code>{name}__trace</code>: Created when <code>with_trace</code> is not <code>TraceType.NONE</code> on the column.</li> <li><code>{name}__reasoning_content</code>: Created when <code>extract_reasoning_content=True</code> on the column.</li> </ul> <p>For detailed information on each column type, refer to the column configuration code reference.</p>"},{"location":"concepts/custom_columns/","title":"Custom Columns","text":"<p>Custom columns let you implement your own generation logic using Python functions. Use them for multi-step LLM workflows, external API integration, or any scenario requiring full programmatic control. For reusable, distributable components, see Plugins instead.</p>"},{"location":"concepts/custom_columns/#quick-start","title":"Quick Start","text":"<pre><code>import data_designer.config as dd\n\n@dd.custom_column_generator(required_columns=[\"name\"])\ndef create_greeting(row: dict) -&gt; dict:\n    row[\"greeting\"] = f\"Hello, {row['name']}!\"\n    return row\n\nconfig_builder.add_column(\n    dd.CustomColumnConfig(\n        name=\"greeting\",\n        generator_function=create_greeting,\n    )\n)\n</code></pre>"},{"location":"concepts/custom_columns/#function-signatures","title":"Function Signatures","text":"<p>Three signatures are supported. Parameter names are validated:</p> Args Signature Use Case 1 <code>fn(row) -&gt; dict</code> Simple transforms 2 <code>fn(row, generator_params) -&gt; dict</code> With typed params 3 <code>fn(row, generator_params, models) -&gt; dict</code> LLM access via models dict <p>For <code>full_column</code> strategy, use <code>df</code> instead of <code>row</code>.</p> <p>For LLM access without params, use <code>generator_params: None</code>:</p> <pre><code>@dd.custom_column_generator(required_columns=[\"name\"], model_aliases=[\"my-model\"])\ndef generate_message(row: dict, generator_params: None, models: dict) -&gt; dict:\n    response, _ = models[\"my-model\"].generate(prompt=f\"Greet {row['name']}\")\n    row[\"greeting\"] = response\n    return row\n</code></pre> <p>Model aliases are validated before generation starts. If an alias doesn't exist in your config, an error is raised during the health check.</p>"},{"location":"concepts/custom_columns/#generation-strategies","title":"Generation Strategies","text":"Strategy Input Use Case <code>cell_by_cell</code> (default) <code>row: dict</code> LLM calls, row-by-row logic <code>full_column</code> <code>df: DataFrame</code> Vectorized DataFrame operations <p>Recommendation: Use <code>cell_by_cell</code> for LLM calls. The framework handles parallelization automatically. Use <code>full_column</code> only for vectorized operations that don't involve LLM calls.</p> <p>For <code>full_column</code>, set <code>generation_strategy=dd.GenerationStrategy.FULL_COLUMN</code>.</p>"},{"location":"concepts/custom_columns/#the-decorator","title":"The Decorator","text":"<pre><code>@dd.custom_column_generator(\n    required_columns=[\"col1\"],        # DAG ordering\n    side_effect_columns=[\"extra\"],    # Additional columns created\n    model_aliases=[\"model1\"],         # Required for LLM access\n)\n</code></pre>"},{"location":"concepts/custom_columns/#models-dict","title":"Models Dict","text":"<p>The third argument is a dict of <code>ModelFacade</code> instances, keyed by alias. You must declare all models required in your custom column generator in <code>model_aliases</code> - this populates the <code>models</code> dict and enables health checks before generation starts.</p> <pre><code>@dd.custom_column_generator(model_aliases=[\"my-model\"])\ndef my_generator(row: dict, generator_params: None, models: dict) -&gt; dict:\n    model = models[\"my-model\"]\n    response, trace = model.generate(\n        prompt=\"...\",\n        parser=my_custom_parser,  # optional, defaults to identity\n        system_prompt=\"...\",\n        max_correction_steps=3,\n    )\n    row[\"result\"] = response\n    return row\n</code></pre> <p>This gives you direct access to all <code>ModelFacade</code> capabilities: custom parsers, correction loops, structured output, tool use, etc.</p>"},{"location":"concepts/custom_columns/#configuration","title":"Configuration","text":"Parameter Type Required Description <code>name</code> str Yes Column name <code>generator_function</code> Callable Yes Decorated function <code>generation_strategy</code> GenerationStrategy No <code>CELL_BY_CELL</code> or <code>FULL_COLUMN</code> <code>generator_params</code> BaseModel No Typed params passed to function"},{"location":"concepts/custom_columns/#multi-turn-example","title":"Multi-Turn Example","text":"<pre><code>@dd.custom_column_generator(\n    required_columns=[\"topic\"],\n    side_effect_columns=[\"draft\", \"critique\"],\n    model_aliases=[\"writer\", \"editor\"],\n)\ndef writer_editor(row: dict, generator_params: None, models: dict) -&gt; dict:\n    draft, _ = models[\"writer\"].generate(prompt=f\"Write about '{row['topic']}'\")\n    critique, _ = models[\"editor\"].generate(prompt=f\"Critique: {draft}\")\n    revised, _ = models[\"writer\"].generate(prompt=f\"Revise based on: {critique}\\n\\nOriginal: {draft}\")\n\n    row[\"final_text\"] = revised\n    row[\"draft\"] = draft\n    row[\"critique\"] = critique\n    return row\n</code></pre>"},{"location":"concepts/custom_columns/#development-testing","title":"Development Testing","text":"<p>Test generators with real LLM calls without running the full pipeline:</p> <pre><code>data_designer = DataDesigner()\nmodels = data_designer.get_models([\"my-model\"])\nresult = my_generator({\"name\": \"Alice\"}, None, models)\n</code></pre>"},{"location":"concepts/custom_columns/#see-also","title":"See Also","text":"<ul> <li>Column Configs Reference</li> <li>Plugins Overview</li> </ul>"},{"location":"concepts/deployment-options/","title":"Deployment Options: Library vs. Microservice","text":"<p>Data Designer is available as both an open-source library and a NeMo Microservice. This guide helps you choose the right deployment option for your use case.</p>"},{"location":"concepts/deployment-options/#deployment-architectures-at-a-glance","title":"Deployment Architectures at a Glance","text":"<p>Data Designer supports three main deployment patterns:</p> <ul> <li> <p>Library + Your LLM Provider</p> <p>Each user runs the library locally and connects to their choice of LLM provider.</p> <p></p> </li> <li> <p>Library + Enterprise Gateway</p> <p>Users run the library locally but share a centralized enterprise LLM gateway with RBAC and governance.</p> <p></p> </li> <li> <p>SDG as a Service (Microservice)</p> <p>A centralized SDG service that multiple users access via REST API.</p> <p></p> </li> </ul>"},{"location":"concepts/deployment-options/#quick-comparison","title":"Quick Comparison","text":"Aspect Open-Source Library NeMo Microservice What it is Python package you import and run REST API service exposing <code>preview</code> and <code>create</code> methods Best for Developers with LLM access who want flexibility and customization Teams using NeMo Microservices platform LLM Access You provide (any OpenAI-compatible API) Integrated with NeMo Microservices Platform Installation <code>pip install data-designer</code> Deploy via NeMo Microservices platform Scaling You manage inference capacity Managed alongside other NeMo services <p>Same Configuration API</p> <p>Both the library and microservice use the same <code>DataDesignerConfigBuilder</code> API. Start with the library, and your configurations migrate seamlessly if you later adopt the NeMo platform.</p>"},{"location":"concepts/deployment-options/#when-to-use-the-open-source-library","title":"\ud83d\udce6 When to Use the Open-Source Library","text":"<p>The library is the right choice for most users. Choose it if you:</p>"},{"location":"concepts/deployment-options/#you-have-access-to-llms","title":"You Have Access to LLMs","text":"<p>You have API keys or endpoints for LLM inference:</p> <ul> <li>Cloud APIs: NVIDIA API Catalog (build.nvidia.com), OpenAI, Azure OpenAI, Anthropic</li> <li>Self-hosted: vLLM, TGI, TensorRT-LLM, or any OpenAI-compatible server</li> <li>Enterprise gateways: Centralized LLM gateway with RBAC, rate limiting, or other enterprise features</li> </ul> <pre><code>from data_designer.interface import DataDesigner\nfrom data_designer.config import ModelConfig\n\n# Use any OpenAI-compatible endpoint\nmodel = ModelConfig(\n    alias=\"my-model\",\n    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n    provider=\"nvidia\",  # or \"openai\", or a custom ModelProvider\n)\n\ndd = DataDesigner()\n# Your code controls the full workflow\n</code></pre>"},{"location":"concepts/deployment-options/#you-need-maximum-flexibility","title":"You Need Maximum Flexibility","text":"<ul> <li>Custom plugins: Extend Data Designer with custom column generators, validators, or processors</li> <li>Local development: Rapid iteration with immediate feedback</li> <li>Integration: Embed Data Designer into existing Python pipelines or notebooks</li> <li>Experimentation: Research workflows with custom models or configurations</li> </ul>"},{"location":"concepts/deployment-options/#you-already-have-enterprise-llm-infrastructure","title":"You Already Have Enterprise LLM Infrastructure","text":"<p>Library + Enterprise LLM Gateway</p> <p>Many enterprises already have centralized LLM access through API gateways with:</p> <ul> <li>Role-based access control (RBAC)</li> <li>Rate limiting and quotas</li> <li>Audit logging</li> <li>Cost allocation</li> </ul> <p>In this case, use the library and point it at your enterprise gateway. You get enterprise-grade LLM access while retaining full control over your Data Designer workflows.</p> <pre><code>from data_designer.config import ModelConfig, ModelProvider\n\n# Define your enterprise gateway as a provider\nenterprise_provider = ModelProvider(\n    name=\"enterprise-gateway\",\n    endpoint=\"https://llm-gateway.yourcompany.com/v1\",\n    api_key=\"ENTERPRISE_LLM_KEY\",  # Environment variable name (uppercase) or actual key\n)\n\n# Use the provider in your model config\nmodel = ModelConfig(\n    alias=\"enterprise-llm\",\n    model=\"gpt-4\",\n    provider=\"enterprise-gateway\",  # References the provider above\n)\n</code></pre>"},{"location":"concepts/deployment-options/#when-to-use-the-microservice","title":"\u2601\ufe0f When to Use the Microservice","text":"<p>The NeMo Microservice exposes Data Designer's <code>preview</code> and <code>create</code> methods as REST API endpoints. Choose it if you:</p>"},{"location":"concepts/deployment-options/#youre-using-the-nemo-microservices-platform","title":"You're Using the NeMo Microservices Platform","text":"<p>The primary value of the microservice is integration with other NeMo Microservices:</p> <ul> <li>NeMo Inference Microservices (NIMs): Seamless integration with NVIDIA's optimized inference endpoints</li> <li>NeMo Customizer: Generate synthetic data for model fine-tuning workflows</li> <li>NeMo Evaluator: Create evaluation datasets alongside model assessment</li> <li>Unified deployment: Single platform for your entire AI pipeline</li> </ul>"},{"location":"concepts/deployment-options/#you-want-to-expose-sdg-as-a-team-service","title":"You Want to Expose SDG as a Team Service","text":"<p>If you need to provide synthetic data generation as a shared service:</p> <ul> <li>Multi-tenant access: Multiple teams submit generation jobs via API</li> <li>Job management: Queue, monitor, and manage generation jobs centrally</li> <li>Resource sharing: Shared infrastructure for SDG workloads</li> </ul>"},{"location":"concepts/deployment-options/#decision-flowchart","title":"\ud83e\udded Decision Flowchart","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Are you using the NeMo  \u2502\n                    \u2502 Microservices platform? \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u25bc                       \u25bc\n                   YES                      NO\n                    \u2502                       \u2502\n                    \u25bc                       \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Use Microservice  \u2502   \u2502 Do you need to expose SDG \u2502\n        \u2502                   \u2502   \u2502 as a shared REST service? \u2502\n        \u2502 Integrates with   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502 NIMs, Customizer, \u2502                 \u2502\n        \u2502 Evaluator         \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u25bc                       \u25bc\n                                 YES                      NO\n                                  \u2502                       \u2502\n                                  \u25bc                       \u25bc\n                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                      \u2502 Consider if the     \u2502   \u2502 Use the Library \u2502\n                      \u2502 overhead is worth   \u2502   \u2502                 \u2502\n                      \u2502 it vs. library +    \u2502   \u2502 Most flexible   \u2502\n                      \u2502 enterprise gateway  \u2502   \u2502 option for      \u2502\n                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502 direct use      \u2502\n                                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/deployment-options/#learn-more","title":"Learn More","text":"<ul> <li>Library: Continue with this documentation</li> <li>Microservice: See the NeMo Data Designer Microservice documentation</li> </ul>"},{"location":"concepts/person_sampling/","title":"Person Sampling in Data Designer","text":"<p>Person sampling in Data Designer allows you to generate synthetic person data for your datasets. There are two distinct approaches, each with different capabilities and use cases.</p>"},{"location":"concepts/person_sampling/#overview","title":"Overview","text":"<p>Data Designer provides two ways to generate synthetic people:</p> <ol> <li>Faker-based sampling - Quick, basic PII generation for testing or when realistic demographic distributions are not relevant for your use case</li> <li>Nemotron-Personas datasets - Demographically accurate, rich persona data</li> </ol>"},{"location":"concepts/person_sampling/#approach-1-faker-based-sampling","title":"Approach 1: Faker-Based Sampling","text":""},{"location":"concepts/person_sampling/#what-it-does","title":"What It Does","text":"<p>Uses the Faker library to generate random personal information. The data is basic and not demographically accurate, but is useful for quick testing, prototyping, or when realistic demographic distributions are not relevant for your use case.</p>"},{"location":"concepts/person_sampling/#features","title":"Features","text":"<ul> <li>Gives you access to person attributes that Faker exposes</li> <li>Quick to set up with no additional downloads</li> <li>Generates random names, emails, addresses, phone numbers, etc.</li> <li>Supports all Faker-supported locales</li> <li>Not demographically grounded - data patterns don't reflect real-world demographics</li> </ul>"},{"location":"concepts/person_sampling/#usage-example","title":"Usage Example","text":"<pre><code>import data_designer.config as dd\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=dd.SamplerType.PERSON_FROM_FAKER,\n        params=dd.PersonFromFakerSamplerParams(\n            locale=\"en_US\",\n            age_range=[25, 65],\n            sex=\"Female\",\n        ),\n    )\n)\n</code></pre> <p>For mor details, see the documentation for <code>SamplerColumnConfig</code> and <code>PersonFromFakerSamplerParams</code>.</p>"},{"location":"concepts/person_sampling/#approach-2-nemotron-personas-datasets","title":"Approach 2: Nemotron-Personas Datasets","text":""},{"location":"concepts/person_sampling/#what-it-does_1","title":"What It Does","text":"<p>Uses curated Nemotron-Personas datasets from NVIDIA GPU Cloud (NGC) to generate demographically accurate person data with rich personality profiles and behavioral characteristics.</p> <p>The NGC datasets are extended versions of the open-source Nemotron-Personas datasets on HuggingFace, with additional fields and enhanced data quality.</p> <p>Supported locales:</p> <ul> <li><code>en_US</code>: United States</li> <li><code>en_IN</code>: India (English)</li> <li><code>en_SG</code>: Singapore (English)</li> <li><code>hi_Deva_IN</code>: India (Devanagari script)</li> <li><code>hi_Latn_IN</code>: India (Latin script)</li> <li><code>ja_JP</code>: Japan</li> <li><code>pt_BR</code>: Brazil (Portuguese)</li> </ul>"},{"location":"concepts/person_sampling/#features_1","title":"Features","text":"<ul> <li>Demographically accurate personal details: Names, ages, sex, marital status, education, occupation based on census data</li> <li>Rich persona details: Comprehensive behavioral profiles including:</li> <li>Big Five personality traits with scores</li> <li>Cultural backgrounds and narratives</li> <li>Skills and hobbies</li> <li>Career goals and aspirations</li> <li>Context-specific personas (professional, financial, healthcare, sports, arts, travel, culinary, etc.)</li> <li>Consistent, referenceable attributes across your dataset</li> <li>Grounded in real-world demographic distributions</li> </ul>"},{"location":"concepts/person_sampling/#prerequisites","title":"Prerequisites","text":"<p>To use the extended Nemotron-Personas datasets with Data Designer, you need to download them from NGC and move them to the Data Designer managed assets directory.</p> <p>See below for step-by-step instructions.</p>"},{"location":"concepts/person_sampling/#nemotron-personas-datasets-setup-instructions","title":"Nemotron-Personas Datasets Setup Instructions","text":""},{"location":"concepts/person_sampling/#step-0-obtain-an-ngc-api-key-and-install-the-ngc-cli","title":"Step 0: Obtain an NGC API Key and install the NGC CLI","text":"<p>To download the Nemotron-Personas datasets from NGC, you will need to obtain an NGC API key and install the NGC CLI.</p> <ol> <li>NGC API Key: Obtain from NVIDIA GPU Cloud</li> <li>NGC CLI: NGC CLI</li> </ol>"},{"location":"concepts/person_sampling/#step-1-set-your-ngc-api-key","title":"Step 1: Set Your NGC API Key","text":"<pre><code>export NGC_API_KEY=\"your-ngc-api-key-here\"\n</code></pre>"},{"location":"concepts/person_sampling/#step-2-option-1-download-nemotron-personas-datasets-via-the-data-designer-cli","title":"Step 2 (option 1): Download Nemotron-Personas Datasets via the Data Designer CLI","text":"<p>Once you have the NGC CLI and your NGC API key set up, you can download the datasets via the Data Designer CLI.</p> <p>You can pass the locales you want to download as arguments to the CLI command: <pre><code>data-designer download personas --locale en_US --locale ja_JP\n</code></pre></p> <p>Or you can use the interactive mode to select the locales you want to download: <pre><code>data-designer download personas\n</code></pre></p>"},{"location":"concepts/person_sampling/#step-2-option-2-download-nemotron-personas-datasets-directly","title":"Step 2 (option 2): Download Nemotron-Personas Datasets Directly","text":"<p>Use the NGC CLI to download the datasets: <pre><code># For Nemotron-Personas USA\nngc registry resource download-version \"nvidia/nemotron-personas/nemotron-personas-dataset-en_us\"\n\n# For Nemotron-Personas IN\nngc registry resource download-version \"nvidia/nemotron-personas/nemotron-personas-dataset-hi_deva_in\"\nngc registry resource download-version \"nvidia/nemotron-personas/nemotron-personas-dataset-hi_latn_in\"\nngc registry resource download-version \"nvidia/nemotron-personas/nemotron-personas-dataset-en_in\"\n\n# For Nemotron-Personas JP\nngc registry resource download-version \"nvidia/nemotron-personas/nemotron-personas-dataset-ja_jp\"\n\n# For Nemotron-Personas SG\nngc registry resource download-version \"nvidia/nemotron-personas/nemotron-personas-dataset-en_sg\"\n\n# For Nemotron-Personas BR\nngc registry resource download-version \"nvidia/nemotron-personas/nemotron-personas-dataset-pt_br\"\n</code></pre></p> <p>Then move the downloaded dataset to the Data Designer managed assets directory: <pre><code>mkdir -p ~/.data-designer/managed-assets/datasets/\nmv nemotron-personas-dataset-*/*.parquet ~/.data-designer/managed-assets/datasets/\n</code></pre></p>"},{"location":"concepts/person_sampling/#step-3-use-personsampler-in-your-code","title":"Step 3: Use PersonSampler in Your Code","text":"<pre><code>import data_designer.config as dd\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=dd.SamplerType.PERSON,\n        params=dd.PersonSamplerParams(\n            locale=\"en_US\",\n            sex=\"Female\",\n            age_range=[25, 45],\n            with_synthetic_personas=True,\n        ),\n    )\n)\n</code></pre> <p>For more details, see the documentation for <code>SamplerColumnConfig</code> and <code>PersonSamplerParams</code>.</p>"},{"location":"concepts/person_sampling/#available-data-fields","title":"Available Data Fields","text":"<p>Core Fields (all locales):</p> Field Type Notes <code>uuid</code> UUID Unique identifier <code>first_name</code> string <code>middle_name</code> string <code>last_name</code> string <code>sex</code> enum \"Male\" or \"Female\" <code>birth_date</code> date Derived: year, month, day <code>street_number</code> int <code>street_name</code> string <code>unit</code> string Address line 2 <code>city</code> string <code>region</code> string Alias: state <code>district</code> string Alias: county <code>postcode</code> string Alias: zipcode <code>country</code> string <code>phone_number</code> PhoneNumber Derived: area_code, country_code, prefix, line_number <code>marital_status</code> string Values: never_married, married_present, separated, widowed, divorced <code>education_level</code> string or None <code>bachelors_field</code> string or None <code>occupation</code> string or None <code>email_address</code> string <code>national_id</code> string <p>Japan-Specific Fields (<code>ja_JP</code>):</p> <ul> <li><code>area</code></li> <li><code>prefecture</code></li> <li><code>zone</code></li> </ul> <p>Brazil-Specific Fields (<code>pt_BR</code>):</p> <ul> <li><code>race</code> - Census-reported race</li> </ul> <p>Brazil and India Shared Fields (<code>pt_BR</code>, <code>en_IN</code>, <code>hi_Deva_IN</code>, <code>hi_Latn_IN</code>):</p> <ul> <li><code>religion</code> - Census-reported religion</li> </ul> <p>India-Specific Fields (<code>en_IN</code>, <code>hi_Deva_IN</code>, <code>hi_Latn_IN</code>):</p> <ul> <li><code>district</code> - Census-reported district</li> <li><code>education_degree</code> - Census-reported education degree</li> <li><code>first_language</code> - Native language</li> <li><code>second_language</code> - Second language (if applicable)</li> <li><code>third_language</code> - Third language (if applicable)</li> <li><code>zone</code> - Urban vs rural</li> </ul> <p>With Synthetic Personas Enabled:</p> <ul> <li>Big Five personality traits (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) with t-scores and labels</li> <li>Cultural background narratives</li> <li>Skills and competencies</li> <li>Hobbies and interests</li> <li>Career goals</li> <li>Context-specific personas (professional, financial, healthcare, sports, arts &amp; entertainment, travel, culinary, etc.)</li> </ul> <p>Japan-specific persona fields:</p> <ul> <li><code>aspects</code></li> <li><code>digital_skills</code></li> </ul> <p>Brazil and India shared persona fields (<code>pt_BR</code>, <code>en_IN</code>, <code>hi_Deva_IN</code>, <code>hi_Latn_IN</code>):</p> <ul> <li><code>religious_persona</code></li> <li><code>religious_background</code></li> </ul> <p>India-specific persona fields (<code>en_IN</code>, <code>hi_Deva_IN</code>, <code>hi_Latn_IN</code>):</p> <ul> <li><code>linguistic_persona</code></li> <li><code>linguistic_background</code></li> </ul>"},{"location":"concepts/person_sampling/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Description <code>locale</code> str Language/region code - must be one of: \"en_US\", \"en_IN\", \"en_SG\", \"hi_Deva_IN\", \"hi_Latn_IN\", \"ja_JP\", \"pt_BR\" <code>sex</code> str (optional) Filter by \"Male\" or \"Female\" <code>city</code> str or list[str] (optional) Filter by specific city or cities within locale <code>age_range</code> list[int] (optional) Two-element list [min_age, max_age] (default: [18, 114]) <code>with_synthetic_personas</code> bool (optional) Include rich personality profiles (default: False) <code>select_field_values</code> dict (optional) Custom field-based filtering (e.g., {\"state\": [\"NY\", \"CA\"], \"education_level\": [\"bachelors\"]})"},{"location":"concepts/processors/","title":"Processors","text":"<p>Processors are transformations that modify your dataset before or after columns are generated. They run at different stages and can reshape, filter, or augment the data.</p> <p>When to Use Processors</p> <p>Processors handle transformations that don't fit the \"column\" model: restructuring the schema for a specific output format, dropping intermediate columns in bulk, or applying batch-wide operations.</p>"},{"location":"concepts/processors/#overview","title":"Overview","text":"<p>Each processor:</p> <ul> <li>Receives the complete batch DataFrame</li> <li>Applies its transformation</li> <li>Passes the result to the next processor (or to output)</li> </ul> <p>Currently, processors run only at the <code>POST_BATCH</code> stage, i.e., after column generation completes for each batch.</p>"},{"location":"concepts/processors/#processor-types","title":"Processor Types","text":""},{"location":"concepts/processors/#drop-columns-processor","title":"\ud83d\uddd1\ufe0f Drop Columns Processor","text":"<p>Removes specified columns from the output dataset. Dropped columns are saved separately in the <code>dropped-columns</code> directory for reference.</p> <p>Dropping Columns is More Easily Achieved via <code>drop = True</code></p> <p>The Drop Columns Processor is different from others in the sense that it does not need to be explicitly added: setting <code>drop = True</code> when configuring a column will accomplish the same.</p> <p>Configuration:</p> <pre><code>import data_designer.config as dd\n\nprocessor = dd.DropColumnsProcessorConfig(\n    name=\"remove_intermediate\",\n    column_names=[\"temp_calculation\", \"raw_input\", \"debug_info\"],\n)\n</code></pre> <p>Behavior:</p> <ul> <li>Columns specified in <code>column_names</code> are removed from the output</li> <li>Original values are preserved in a separate parquet file</li> <li>Missing columns produce a warning but don't fail the build</li> <li>Column configs are automatically marked with <code>drop=True</code> when this processor is added</li> </ul> <p>Use Cases:</p> <ul> <li>Removing intermediate columns used only for LLM context</li> <li>Cleaning up debug or validation columns before final output</li> <li>Separating sensitive data from the main dataset</li> </ul>"},{"location":"concepts/processors/#schema-transform-processor","title":"\ud83d\udd04 Schema Transform Processor","text":"<p>Creates an additional dataset with a transformed schema using Jinja2 templates. The output is written to a separate directory alongside the main dataset.</p> <p>Configuration:</p> <pre><code>import data_designer.config as dd\n\nprocessor = dd.SchemaTransformProcessorConfig(\n    name=\"chat_format\",\n    template={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"{{ question }}\"},\n            {\"role\": \"assistant\", \"content\": \"{{ answer }}\"},\n        ],\n        \"metadata\": \"{{ category | upper }}\",\n    },\n)\n</code></pre> <p>Behavior:</p> <ul> <li>Each key in <code>template</code> becomes a column in the transformed dataset</li> <li>Values are Jinja2 templates with access to all columns in the batch</li> <li>Complex structures (lists, nested dicts) are supported</li> <li>Output is saved to the <code>processors-outputs/{name}/</code> directory</li> <li>The original dataset passes through unchanged</li> </ul> <p>Template Capabilities:</p> <ul> <li>Variable substitution: <code>{{ column_name }}</code></li> <li>Filters: <code>{{ text | upper }}</code>, <code>{{ text | lower }}</code>, <code>{{ text | trim }}</code></li> <li>Nested structures: Arbitrarily deep JSON structures</li> <li>Lists: <code>[\"{{ col1 }}\", \"{{ col2 }}\"]</code></li> </ul> <p>Use Cases:</p> <ul> <li>Converting flat columns to chat message format</li> <li>Restructuring data for specific model training formats</li> <li>Creating derived views without modifying the source dataset</li> </ul>"},{"location":"concepts/processors/#using-processors","title":"Using Processors","text":"<p>Add processors to your configuration using the builder's <code>add_processor</code> method:</p> <pre><code>import data_designer.config as dd\n\nbuilder = dd.DataDesignerConfigBuilder()\n\n# ... add columns ...\n\n# Drop intermediate columns\nbuilder.add_processor(\n    dd.DropColumnsProcessorConfig(\n        name=\"cleanup\",\n        column_names=[\"scratch_work\", \"raw_context\"],\n    )\n)\n\n# Transform to chat format\nbuilder.add_processor(\n    dd.SchemaTransformProcessorConfig(\n        name=\"chat_format\",\n        template={\n            \"messages\": [\n                {\"role\": \"user\", \"content\": \"{{ question }}\"},\n                {\"role\": \"assistant\", \"content\": \"{{ answer }}\"},\n            ],\n        },\n    )\n)\n</code></pre>"},{"location":"concepts/processors/#execution-order","title":"Execution Order","text":"<p>Processors execute in the order they're added. Plan accordingly when one processor's output affects another.</p>"},{"location":"concepts/processors/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"concepts/processors/#common-parameters","title":"Common Parameters","text":"Parameter Type Description <code>name</code> str Identifier for the processor, used in output directory names <code>build_stage</code> BuildStage When to run (default: <code>POST_BATCH</code>)"},{"location":"concepts/processors/#dropcolumnsprocessorconfig","title":"DropColumnsProcessorConfig","text":"Parameter Type Description <code>column_names</code> list[str] Columns to remove from output"},{"location":"concepts/processors/#schematransformprocessorconfig","title":"SchemaTransformProcessorConfig","text":"Parameter Type Description <code>template</code> dict[str, Any] Jinja2 template defining the output schema. Must be JSON-serializable."},{"location":"concepts/seed-datasets/","title":"Seed Datasets","text":"<p>Seed datasets let you bootstrap synthetic data generation from existing data. Instead of generating everything from scratch, you provide a dataset whose columns become available as context in your prompts and expressions\u2014grounding your synthetic data in real-world examples.</p> <p>When to Use Seed Datasets</p> <p>Seed datasets shine when you have real data you want to build on:</p> <ul> <li>Product catalogs \u2192 generate customer reviews</li> <li>Medical diagnoses \u2192 generate physician notes</li> <li>Code snippets \u2192 generate documentation</li> <li>Company profiles \u2192 generate financial reports</li> </ul> <p>The seed data provides realism and domain specificity; Data Designer adds volume and variation.</p>"},{"location":"concepts/seed-datasets/#the-basic-pattern","title":"The Basic Pattern","text":"<pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\n# Define your model configuration\nmodel_configs = [\n    dd.ModelConfig(\n        alias=\"my-model\",\n        model=\"nvidia/nemotron-3-nano-30b-a3b\",\n        provider=\"nvidia\",\n    )\n]\n\nconfig_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n\n# 1. Attach a seed dataset\nseed_source = dd.LocalFileSeedSource(path=\"products.csv\")\nconfig_builder.with_seed_dataset(seed_source)\n\n# 2. Reference seed columns in your prompts\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"review\",\n        model_alias=\"my-model\",\n        prompt=\"\"\"\\\nWrite a customer review for {{ product_name }}.\nCategory: {{ category }}\nPrice: ${{ price }}\n\"\"\",\n    )\n)\n</code></pre> <p>Every column in your seed dataset becomes available as a Jinja2 variable in prompts and expressions. Data Designer automatically:</p> <ul> <li>Reads rows from the seed dataset</li> <li>Injects seed column values into templates</li> </ul>"},{"location":"concepts/seed-datasets/#seed-sources","title":"Seed Sources","text":"<p>Data Designer supports three ways to provide seed data:</p>"},{"location":"concepts/seed-datasets/#localfileseedsource","title":"\ud83d\udcc1 LocalFileSeedSource","text":"<p>Load from a local file\u2014CSV, Parquet, or JSON.</p> <pre><code># Single file\nseed_source = dd.LocalFileSeedSource(path=\"data/products.csv\")\n\n# Parquet files with wildcard\nseed_source = dd.LocalFileSeedSource(path=\"data/products/*.parquet\")\n</code></pre> <p>Supported Formats</p> <ul> <li>CSV (<code>.csv</code>)</li> <li>Parquet (<code>.parquet</code>)</li> <li>JSON (<code>.json</code>, <code>.jsonl</code>)</li> </ul>"},{"location":"concepts/seed-datasets/#huggingfaceseedsource","title":"\ud83e\udd17 HuggingFaceSeedSource","text":"<p>Load directly from HuggingFace datasets without downloading manually.</p> <pre><code>seed_source = dd.HuggingFaceSeedSource(\n    path=\"datasets/gretelai/symptom_to_diagnosis/data/train.parquet\",\n    token=\"hf_...\",  # Optional, for private datasets\n)\n</code></pre>"},{"location":"concepts/seed-datasets/#dataframeseedsource","title":"\ud83d\udc3c DataFrameSeedSource","text":"<p>Use an in-memory pandas DataFrame\u2014great for preprocessing or combining multiple sources.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"raw_data.csv\")\ndf = df[df[\"quality_score\"] &gt; 0.8]  # Filter to high-quality rows\n\nseed_source = dd.DataFrameSeedSource(df=df)\n</code></pre> <p>Serialization</p> <p><code>DataFrameSeedSource</code> can't be serialized to YAML/JSON configs. Use <code>LocalFileSeedSource</code> if you need to save and share configurations.</p>"},{"location":"concepts/seed-datasets/#sampling-strategies","title":"Sampling Strategies","text":"<p>Control how rows are read from the seed dataset.</p>"},{"location":"concepts/seed-datasets/#ordered-default","title":"Ordered (Default)","text":"<p>Rows are read sequentially in their original order. Each generated record corresponds to the next row in the seed dataset. If you generate more records than exist in the seed dataset, it will cycle in order until completion.</p> <pre><code>config_builder.with_seed_dataset(\n    seed_source,\n    sampling_strategy=dd.SamplingStrategy.ORDERED,\n)\n</code></pre>"},{"location":"concepts/seed-datasets/#shuffle","title":"Shuffle","text":"<p>Rows are randomly shuffled before sampling. Useful when your seed data has some ordering you want to break.</p> <pre><code>config_builder.with_seed_dataset(\n    seed_source,\n    sampling_strategy=dd.SamplingStrategy.SHUFFLE,\n)\n</code></pre>"},{"location":"concepts/seed-datasets/#selection-strategies","title":"Selection Strategies","text":"<p>Select a subset of your seed dataset\u2014useful for large datasets or parallel processing.</p>"},{"location":"concepts/seed-datasets/#indexrange","title":"IndexRange","text":"<p>Select a specific range of row indices.</p> <pre><code># Use only rows 100-199 (100 rows total)\nconfig_builder.with_seed_dataset(\n    seed_source,\n    selection_strategy=dd.IndexRange(start=100, end=199),\n)\n</code></pre>"},{"location":"concepts/seed-datasets/#partitionblock","title":"PartitionBlock","text":"<p>Split the dataset into N equal partitions and select one. Perfect for distributing work across multiple jobs.</p> <pre><code># Split into 5 partitions, use the 3rd one (index=2, zero-based)\nconfig_builder.with_seed_dataset(\n    seed_source,\n    selection_strategy=dd.PartitionBlock(index=2, num_partitions=5),\n)\n</code></pre> <p>Parallel Processing</p> <p>Run 5 parallel jobs, each with a different partition index, to process a large seed dataset in parallel:</p> <pre><code># Job 0: PartitionBlock(index=0, num_partitions=5)\n# Job 1: PartitionBlock(index=1, num_partitions=5)\n# Job 2: PartitionBlock(index=2, num_partitions=5)\n# ...\n</code></pre>"},{"location":"concepts/seed-datasets/#combining-strategies","title":"Combining Strategies","text":"<p>Sampling and selection strategies work together. For example, shuffle rows within a specific partition:</p> <pre><code>config_builder.with_seed_dataset(\n    seed_source,\n    sampling_strategy=dd.SamplingStrategy.SHUFFLE,\n    selection_strategy=dd.PartitionBlock(index=0, num_partitions=10),\n)\n</code></pre>"},{"location":"concepts/seed-datasets/#complete-example","title":"Complete Example","text":"<p>Here's a complete example generating physician notes from a symptom-to-diagnosis seed dataset:</p> <pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\ndata_designer = DataDesigner()\n\nmodel_configs = [\n    dd.ModelConfig(\n        alias=\"medical-notes\",\n        model=\"nvidia/nemotron-3-nano-30b-a3b\",\n        provider=\"nvidia\",\n    )\n]\n\nconfig_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n\n# Attach seed dataset (has 'diagnosis' and 'symptoms' columns)\nseed_source = dd.LocalFileSeedSource(path=\"symptom_to_diagnosis.csv\")\nconfig_builder.with_seed_dataset(seed_source)\n\n# Generate patient info\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"patient\",\n        sampler_type=dd.SamplerType.PERSON_FROM_FAKER,\n        params=dd.PersonFromFakerSamplerParams(),\n    )\n)\n\nconfig_builder.add_column(\n    dd.ExpressionColumnConfig(\n        name=\"patient_name\",\n        expr=\"{{ patient.first_name }} {{ patient.last_name }}\",\n    )\n)\n\n# Generate notes grounded in seed data\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"physician_notes\",\n        model_alias=\"medical-notes\",\n        prompt=\"\"\"\\\nYou are a physician writing notes after a patient visit.\n\nPatient: {{ patient_name }}\nDiagnosis: {{ diagnosis }}\nReported Symptoms: {{ symptoms }}\n\nWrite detailed clinical notes for this visit.\n\"\"\",\n    )\n)\n\n# Preview\npreview = designer.preview(config_builder, num_records=5)\npreview.display_sample_record()\n</code></pre>"},{"location":"concepts/seed-datasets/#best-practices","title":"Best Practices","text":""},{"location":"concepts/seed-datasets/#keep-seed-data-clean","title":"Keep Seed Data Clean","text":"<p>Garbage in, garbage out. Clean your seed data before using it:</p> <ul> <li>Remove duplicates</li> <li>Fix encoding issues</li> <li>Filter out low-quality rows</li> <li>Standardize column names</li> </ul>"},{"location":"concepts/seed-datasets/#match-generation-volume-to-seed-size","title":"Match Generation Volume to Seed Size","text":"<p>If your seed dataset has 1,000 rows and you generate 10,000 records, each seed row will be used ~10 times. Consider whether that's appropriate for your use case.</p>"},{"location":"concepts/seed-datasets/#use-seed-data-for-diversity-control","title":"Use Seed Data for Diversity Control","text":"<p>Seed datasets are excellent for controlling the distribution of your synthetic data. Want 30% electronics, 50% clothing, 20% home goods? Curate your seed dataset to match.</p>"},{"location":"concepts/tool_use_and_mcp/","title":"Tool Use &amp; MCP","text":"<p>Tool use lets LLM columns call external tools during generation (e.g., lookups, calculations, retrieval, domain services). Data Designer supports tool use via the Model Context Protocol (MCP), which standardizes how tools are discovered and invoked.</p>"},{"location":"concepts/tool_use_and_mcp/#quick-start","title":"Quick Start","text":"<ol> <li>Configure an MCP provider (Local or Remote)</li> <li>Create a ToolConfig referencing your provider</li> <li>Add <code>tool_alias</code> to your LLM column</li> </ol> <pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\n# 1. Configure provider\n\n## Local Stdio provider\nmcp_provider = dd.LocalStdioMCPProvider(\n    name=\"demo-mcp\",\n    command=\"python\",\n    args=[\"-m\", \"my_mcp_server\"],\n)\n\n## Remote provider\n# mcp_provider = dd.MCPProvider(\n#     name=\"remote-mcp\",\n#     endpoint=\"https://mcp.example.invalid/sse\",\n#     api_key=\"REMOTE_MCP_API_KEY\",\n# )\n\ndata_designer = DataDesigner(mcp_providers=[mcp_provider])\n\n# 2. Create tool config\ntool_config = dd.ToolConfig(\n    tool_alias=\"my-tools\",\n    providers=[\"demo-mcp\"],\n)\n\nbuilder = dd.DataDesignerConfigBuilder(tool_configs=[tool_config])\n\n# 3. Use tools in column\nbuilder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"answer\",\n        prompt=\"Use tools to answer: {{ question }}\",\n        model_alias=\"nvidia-text\",\n        tool_alias=\"my-tools\",\n    )\n)\n</code></pre>"},{"location":"concepts/tool_use_and_mcp/#guides","title":"Guides","text":"Guide Description MCP Providers Configure local subprocess or remote SSE providers Tool Configs Define tool permissions and limits Enabling Tools on Columns Use tools in LLM generation Configure via CLI Interactive CLI configuration Traces Capture full conversation history Safety &amp; Limits Allowlists, budgets, timeouts"},{"location":"concepts/tool_use_and_mcp/#example","title":"Example","text":"<p>See the PDF Q&amp;A Recipe for a complete working example.</p>"},{"location":"concepts/tool_use_and_mcp/#code-reference","title":"Code Reference","text":"<p>For internal architecture and API documentation, see MCP Code Reference.</p>"},{"location":"concepts/traces/","title":"Message Traces","text":"<p>Traces capture the conversation history during LLM generation, including system prompts, user prompts, model reasoning, tool calls, tool results, and the final response. This visibility is essential for understanding model behavior, debugging generation issues, and iterating on prompts.</p> <p>Traces are also useful in certain scenarios as the target output of the workflow, e.g. producing an SFT dataset for fine-tuning tool-use capability, for instance.</p>"},{"location":"concepts/traces/#overview","title":"Overview","text":"<p>When generating content with LLM columns, you often need to understand what happened during generation:</p> <ul> <li>What system prompt was used?</li> <li>What did the rendered user prompt look like?</li> <li>Did the model provide any reasoning content?</li> <li>Which tools were called (if tool use is enabled)?</li> <li>What arguments were passed to tools?</li> <li>What did tools return?</li> <li>Did the model retry after failures?</li> <li>How did the model arrive at the final answer?</li> </ul> <p>Traces provide this visibility by capturing the ordered message history for each generation, including any multi-turn conversations that occur during tool use or retry scenarios.</p>"},{"location":"concepts/traces/#trace-types","title":"Trace Types","text":"<p>Data Designer supports three trace modes via the <code>TraceType</code> enum:</p> TraceType Description <code>TraceType.NONE</code> No trace captured (default) <code>TraceType.LAST_MESSAGE</code> Only the final assistant message is captured <code>TraceType.ALL_MESSAGES</code> Full conversation history (system/user/assistant/tool)"},{"location":"concepts/traces/#enabling-traces","title":"Enabling Traces","text":""},{"location":"concepts/traces/#per-column-recommended","title":"Per-Column (Recommended)","text":"<p>Set <code>with_trace</code> on specific LLM columns:</p> <pre><code>import data_designer.config as dd\n\n# Capture full conversation history\nbuilder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"answer\",\n        prompt=\"Answer: {{ question }}\",\n        model_alias=\"nvidia-text\",\n        with_trace=dd.TraceType.ALL_MESSAGES,  # Full trace\n    )\n)\n\n# Capture only the final assistant response\nbuilder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"summary\",\n        prompt=\"Summarize: {{ text }}\",\n        model_alias=\"nvidia-text\",\n        with_trace=dd.TraceType.LAST_MESSAGE,  # Just the final response\n    )\n)\n</code></pre>"},{"location":"concepts/traces/#trace-column-naming","title":"Trace Column Naming","text":"<p>When enabled, LLM columns produce an additional side-effect column:</p> <ul> <li><code>{column_name}__trace</code></li> </ul> <p>For example, if your column is named <code>\"answer\"</code>, the trace column will be <code>\"answer__trace\"</code>.</p>"},{"location":"concepts/traces/#trace-data-structure","title":"Trace Data Structure","text":"<p>Each trace is a <code>list[dict]</code> where each dict represents a message in the conversation.</p>"},{"location":"concepts/traces/#message-fields-by-role","title":"Message Fields by Role","text":"Role Fields Description <code>system</code> <code>role</code>, <code>content</code> System prompt setting model behavior. <code>content</code> is a list of blocks in ChatML format. <code>user</code> <code>role</code>, <code>content</code> User prompt (rendered from template). <code>content</code> is a list of blocks (text + multimodal). <code>assistant</code> <code>role</code>, <code>content</code>, <code>tool_calls</code>, <code>reasoning_content</code> Model response; <code>content</code> may be empty if only requesting tools. <code>tool</code> <code>role</code>, <code>content</code>, <code>tool_call_id</code> Tool execution result; <code>tool_call_id</code> links to the request."},{"location":"concepts/traces/#example-trace-simple-generation","title":"Example Trace (Simple Generation)","text":"<p>A basic trace without tool use:</p> <pre><code>[\n    # System message (if configured)\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant that provides clear, concise answers.\"}]\n    },\n    # User message (the rendered prompt)\n    {\n        \"role\": \"user\",\n        \"content\": [{\"type\": \"text\", \"text\": \"What is the capital of France?\"}]\n    },\n    # Final assistant response\n    {\n        \"role\": \"assistant\",\n        \"content\": [{\"type\": \"text\", \"text\": \"The capital of France is Paris.\"}],\n        \"reasoning_content\": None  # May contain reasoning if model supports it\n    }\n]\n</code></pre>"},{"location":"concepts/traces/#example-trace-with-tool-use","title":"Example Trace (With Tool Use)","text":"<p>When tool use is enabled, traces capture the full conversation including tool calls:</p> <pre><code>[\n    # System message\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You must call tools before answering. Only use tool results.\"}]\n    },\n    # User message (the rendered prompt)\n    {\n        \"role\": \"user\",\n        \"content\": [{\"type\": \"text\", \"text\": \"What documents are in the knowledge base about machine learning?\"}]\n    },\n    # Assistant requests tool calls\n    {\n        \"role\": \"assistant\",\n        \"content\": [{\"type\": \"text\", \"text\": \"\"}],\n        \"tool_calls\": [\n            {\n                \"id\": \"call_abc123\",\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": \"list_docs\",\n                    \"arguments\": \"{\\\"query\\\": \\\"machine learning\\\"}\"\n                }\n            }\n        ]\n    },\n    # Tool response (linked by tool_call_id)\n    {\n        \"role\": \"tool\",\n        \"content\": [{\"type\": \"text\", \"text\": \"Found 3 documents: intro_ml.pdf, neural_networks.pdf, transformers.pdf\"}],\n        \"tool_call_id\": \"call_abc123\"\n    },\n    # Final assistant response\n    {\n        \"role\": \"assistant\",\n        \"content\": [{\"type\": \"text\", \"text\": \"The knowledge base contains three documents about machine learning: ...\"}]\n    }\n]\n</code></pre>"},{"location":"concepts/traces/#the-tool_calls-structure","title":"The tool_calls Structure","text":"<p>When an assistant message includes tool calls:</p> <pre><code>{\n    \"id\": \"call_abc123\",           # Unique ID linking to tool response\n    \"type\": \"function\",            # Always \"function\" for MCP tools\n    \"function\": {\n        \"name\": \"search_docs\",     # Tool name\n        \"arguments\": \"{...}\"       # JSON string of tool arguments\n    }\n}\n</code></pre>"},{"location":"concepts/traces/#extracting-reasoning-content","title":"Extracting Reasoning Content","text":"<p>Some models (particularly those with extended thinking or chain-of-thought capabilities) expose their reasoning process separately via the <code>reasoning_content</code> field in assistant messages. While this is included in full traces, you may want to capture it separately without the overhead of storing the entire conversation history.</p>"},{"location":"concepts/traces/#dedicated-reasoning-column","title":"Dedicated Reasoning Column","text":"<p>Set <code>extract_reasoning_content=True</code> on any LLM column to create a <code>{column_name}__reasoning_content</code> side-effect column:</p> <pre><code>import data_designer.config as dd\n\nbuilder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"solution\",\n        prompt=\"Solve this math problem step by step: {{ problem }}\",\n        model_alias=\"reasoning-model\",\n        extract_reasoning_content=True,  # Creates solution__reasoning_content\n    )\n)\n</code></pre> <p>The extracted reasoning content:</p> <ul> <li>Contains only the <code>reasoning_content</code> from the final assistant message in the trace</li> <li>Is stripped of leading/trailing whitespace</li> <li>Is <code>None</code> if the model didn't provide reasoning content or if it was whitespace-only</li> </ul>"},{"location":"concepts/traces/#when-to-use-each-approach","title":"When to Use Each Approach","text":"Need Approach Full conversation history for debugging <code>with_trace=True</code> Just the model's reasoning/thinking <code>extract_reasoning_content=True</code> Both conversation history and separate reasoning Use both options Fine-tuning data with reasoning <code>extract_reasoning_content=True</code> for clean extraction"},{"location":"concepts/traces/#availability","title":"Availability","text":"<p>The <code>extract_reasoning_content</code> option is available on all LLM column types:</p> <ul> <li><code>LLMTextColumnConfig</code></li> <li><code>LLMCodeColumnConfig</code></li> <li><code>LLMStructuredColumnConfig</code></li> <li><code>LLMJudgeColumnConfig</code></li> </ul>"},{"location":"concepts/traces/#see-also","title":"See Also","text":"<ul> <li>Safety and Limits: Understand turn limits and timeout behavior</li> </ul>"},{"location":"concepts/validators/","title":"Validators","text":"<p>Validators are quality assurance mechanisms in Data Designer that check generated content against rules and return structured pass/fail results. They enable automated verification of data for correctness, code quality, and adherence to specifications.</p> <p>Quality Gates for Generated Data</p> <p>Validators act as quality gates in your generation pipeline. Use them to filter invalid records, score code quality, verify format compliance, or integrate with external validation services.</p>"},{"location":"concepts/validators/#overview","title":"Overview","text":"<p>Validation columns execute validation logic against target columns and produce structured results indicating:</p> <ul> <li><code>is_valid</code>: Boolean pass/fail status</li> <li>Additional metadata: Error messages, scores, severity levels, and custom fields</li> </ul> <p>Validators currently support three execution strategies:</p> <ol> <li>Code validation: Lint and check Python or SQL code using industry-standard tools</li> <li>Local callable validation: Execute custom Python functions for flexible validation logic</li> <li>Remote validation: Send data to HTTP endpoints for external validation services</li> </ol>"},{"location":"concepts/validators/#validator-types","title":"Validator Types","text":""},{"location":"concepts/validators/#python-code-validator","title":"\ud83d\udc0d Python Code Validator","text":"<p>The Python code validator runs generated Python code through Ruff, a fast Python linter that checks for syntax errors, undefined variables, and code quality issues.</p> <p>Configuration:</p> <pre><code>import data_designer.config as dd\n\nvalidator_params = dd.CodeValidatorParams(code_lang=dd.CodeLang.PYTHON)\n</code></pre> <p>Validation Output:</p> <p>Each validated record returns:</p> <ul> <li><code>is_valid</code>: <code>True</code> if no fatal or error-level issues found</li> <li><code>python_linter_score</code>: Quality score from 0-10 (based on pylint formula)</li> <li><code>python_linter_severity</code>: Highest severity level found (<code>\"none\"</code>, <code>\"convention\"</code>, <code>\"refactor\"</code>, <code>\"warning\"</code>, <code>\"error\"</code>, <code>\"fatal\"</code>)</li> <li><code>python_linter_messages</code>: List of linter messages with line numbers, columns, and descriptions</li> </ul> <p>Severity Levels:</p> <ul> <li>Fatal: Syntax errors preventing code execution</li> <li>Error: Undefined names, invalid syntax</li> <li>Warning: Code smells and potential issues</li> <li>Refactor: Simplification opportunities</li> <li>Convention: Style guide violations</li> </ul> <p>A record is marked valid if it has no messages or only messages at warning/convention/refactor levels.</p> <p>Example Validation Result:</p> <pre><code>{\n    \"is_valid\": False,\n    \"python_linter_score\": 0,\n    \"python_linter_severity\": \"error\",\n    \"python_linter_messages\": [\n        {\n            \"type\": \"error\",\n            \"symbol\": \"F821\",\n            \"line\": 1,\n            \"column\": 7,\n            \"message\": \"Undefined name `it`\"\n        }\n    ]\n}\n</code></pre>"},{"location":"concepts/validators/#sql-code-validator","title":"\ud83d\uddc4\ufe0f SQL Code Validator","text":"<p>The SQL code validator uses SQLFluff, a dialect-aware SQL linter that checks query syntax and structure.</p> <p>Configuration:</p> <pre><code>import data_designer.config as dd\n\nvalidator_params = dd.CodeValidatorParams(code_lang=dd.CodeLang.SQL_POSTGRES)\n</code></pre> <p>Multiple Dialects</p> <p>The SQL code validator supports multiple dialects: <code>SQL_POSTGRES</code>, <code>SQL_ANSI</code>, <code>SQL_MYSQL</code>, <code>SQL_SQLITE</code>, <code>SQL_TSQL</code> and <code>SQL_BIGQUERY</code>.</p> <p>Validation Output:</p> <p>Each validated record returns:</p> <ul> <li><code>is_valid</code>: <code>True</code> if no parsing errors found</li> <li><code>error_messages</code>: Concatenated error descriptions (empty string if valid)</li> </ul> <p>The validator focuses on parsing errors (PRS codes) that indicate malformed SQL. It also checks for common pitfalls like <code>DECIMAL</code> definitions without scale parameters.</p> <p>Example Validation Result:</p> <pre><code># Valid SQL\n{\n    \"is_valid\": True,\n    \"error_messages\": \"\"\n}\n\n# Invalid SQL\n{\n    \"is_valid\": False,\n    \"error_messages\": \"PRS: Line 1, Position 1: Found unparsable section: 'NOT SQL'\"\n}\n</code></pre>"},{"location":"concepts/validators/#local-callable-validator","title":"\ud83d\udd27 Local Callable Validator","text":"<p>The local callable validator executes custom Python functions for flexible validation logic.</p> <p>Configuration:</p> <pre><code>import pandas as pd\n\nimport data_designer.config as dd\n\ndef my_validation_function(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Validate that values are positive.\n\n    Args:\n        df: DataFrame with target columns\n\n    Returns:\n        DataFrame with is_valid column and optional metadata\n    \"\"\"\n    result = pd.DataFrame()\n    result[\"is_valid\"] = df[\"price\"] &gt; 0\n    result[\"error_message\"] = result[\"is_valid\"].apply(\n        lambda valid: \"\" if valid else \"Price must be positive\"\n    )\n    return result\n\nvalidator_params = dd.LocalCallableValidatorParams(\n    validation_function=my_validation_function,\n    output_schema={  # Optional: enforce output schema\n        \"type\": \"object\",\n        \"properties\": {\n            \"data\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"is_valid\": {\"type\": [\"boolean\", \"null\"]},\n                        \"error_message\": {\"type\": \"string\"}\n                    },\n                    \"required\": [\"is_valid\"]\n                }\n            }\n        }\n    }\n)\n</code></pre> <p>Function Requirements:</p> <ul> <li>Input: DataFrame with target columns</li> <li>Output: DataFrame with <code>is_valid</code> column (boolean or null)</li> <li>Extra fields: Any additional columns become validation metadata</li> </ul> <p>The <code>output_schema</code> parameter is optional but recommended\u2014it validates the function's output against a JSON schema, catching unexpected return formats.</p>"},{"location":"concepts/validators/#remote-validator","title":"\ud83c\udf10 Remote Validator","text":"<p>The remote validator sends data to HTTP endpoints for validation-as-a-service. This is useful for when you have validation software that needs to run on external compute and you can expose it through a service. Some examples are:</p> <ul> <li>External linting services</li> <li>Security scanners</li> <li>Domain-specific validators</li> <li>Proprietary validation systems</li> </ul> <p>Authentication</p> <p>Currently, the remote validator is only able to perform unauthenticated API calls. When implementing your own service, you can rely on network isolation for security. If you need to reach a service that requires authentication, you should implement a local proxy.</p> <p>Configuration:</p> <pre><code>import data_designer.config as dd\n\nvalidator_params = dd.RemoteValidatorParams(\n    endpoint_url=\"https://api.example.com/validate\",\n    timeout=30.0,  # Request timeout in seconds\n    max_retries=3,  # Retry attempts on failure\n    retry_backoff=2.0,  # Exponential backoff factor\n    max_parallel_requests=4,  # Concurrent request limit\n    output_schema={  # Optional: enforce response schema\n        \"type\": \"object\",\n        \"properties\": {\n            \"data\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"is_valid\": {\"type\": [\"boolean\", \"null\"]},\n                        \"confidence\": {\"type\": \"string\"}\n                    }\n                }\n            }\n        }\n    }\n)\n</code></pre> <p>Request Format:</p> <p>The validator sends POST requests with this structure:</p> <pre><code>{\n    \"data\": [\n        {\"column1\": \"value1\", \"column2\": \"value2\"},\n        {\"column1\": \"value3\", \"column2\": \"value4\"}\n    ]\n}\n</code></pre> <p>Expected Response Format:</p> <p>The endpoint must return:</p> <pre><code>{\n    \"data\": [\n        {\n            \"is_valid\": true,\n            \"custom_field\": \"any additional metadata\"\n        },\n        {\n            \"is_valid\": false,\n            \"custom_field\": \"more metadata\"\n        }\n    ]\n}\n</code></pre> <p>Retry Behavior:</p> <p>The validator automatically retries on:</p> <ul> <li>Network errors</li> <li>HTTP status codes: 429 (rate limit), 500, 502, 503, 504</li> </ul> <p>Failed requests use exponential backoff: <code>delay = retry_backoff^attempt</code>.</p> <p>Parallelization:</p> <p>Set <code>max_parallel_requests</code> to control concurrency. Higher values improve throughput but increase server load. The validator batches requests according to the <code>batch_size</code> parameter in the validation column configuration.</p>"},{"location":"concepts/validators/#using-validators-in-columns","title":"Using Validators in Columns","text":"<p>Add validation columns to your configuration using the builder's <code>add_column</code> method:</p> <pre><code>import data_designer.config as dd\n\nbuilder = dd.DataDesignerConfigBuilder()\n\n# Generate Python code\nbuilder.add_column(\n    dd.LLMCodeColumnConfig(\n        name=\"sorting_algorithm\",\n        prompt=\"Write a Python function to sort a list using bubble sort.\",\n        code_lang=\"python\",\n        model_alias=\"my-model\"\n    )\n)\n\n# Validate the generated code\nbuilder.add_column(\n    dd.ValidationColumnConfig(\n        name=\"code_validation\",\n        target_columns=[\"sorting_algorithm\"],\n        validator_type=\"code\",\n        validator_params=dd.CodeValidatorParams(code_lang=dd.CodeLang.PYTHON),\n        batch_size=10,\n        drop=False,\n    )\n)\n</code></pre> <p>The <code>target_columns</code> parameter specifies which columns to validate. All target columns are passed to the validator together (except for code validators, which process each column separately).</p>"},{"location":"concepts/validators/#configuration-parameters","title":"Configuration Parameters","text":"<p>See more about parameters used to instantiate <code>ValidationColumnConfig</code> in the code reference.</p>"},{"location":"concepts/validators/#batch-size-considerations","title":"Batch Size Considerations","text":"<p>Larger batch sizes improve efficiency but consume more memory:</p> <ul> <li>Code validators: 5-20 records (file I/O overhead)</li> <li>Local callable: 10-50 records (depends on function complexity)</li> <li>Remote validators: 1-10 records (network latency, server capacity)</li> </ul> <p>Adjust based on:</p> <ul> <li>Validator computational cost</li> <li>Available memory</li> <li>Network bandwidth (for remote validators)</li> <li>Server rate limits</li> </ul> <p>If the validation logic uses information from other samples, only samples in the batch will be considered.</p>"},{"location":"concepts/validators/#multiple-column-validation","title":"Multiple Column Validation","text":"<p>Validate multiple columns simultaneously:</p> <pre><code>import data_designer.config as dd\n\nbuilder.add_column(\n    dd.ValidationColumnConfig(\n        name=\"multi_column_validation\",\n        target_columns=[\"column_a\", \"column_b\", \"column_c\"],\n        validator_type=\"remote\",\n        validator_params=dd.RemoteValidatorParams(\n            endpoint_url=\"https://api.example.com/validate\"\n        )\n    )\n)\n</code></pre> <p>Note: Code validators always process each target column separately, even when multiple columns are specified. Local callable and remote validators receive all target columns together.</p>"},{"location":"concepts/validators/#see-also","title":"See Also","text":"<ul> <li>Validator Parameters Reference: Configuration object schemas</li> </ul>"},{"location":"concepts/mcp/configure-mcp-cli/","title":"Configuring MCP Using the CLI","text":"<p>The Data Designer CLI provides an interactive interface for creating and managing MCP providers and tool configurations stored in your Data Designer home directory (default: <code>~/.data-designer/</code>).</p>"},{"location":"concepts/mcp/configure-mcp-cli/#configuration-files","title":"Configuration Files","text":"<p>The CLI manages two YAML configuration files for MCP:</p> <ul> <li><code>mcp_providers.yaml</code>: MCP provider configurations</li> <li><code>tool_configs.yaml</code>: Tool configurations</li> </ul> <p>Custom Directory</p> <p>You can customize the configuration directory location with the <code>DATA_DESIGNER_HOME</code> environment variable: <pre><code>export DATA_DESIGNER_HOME=\"/path/to/your/custom/directory\"\n</code></pre></p>"},{"location":"concepts/mcp/configure-mcp-cli/#cli-commands","title":"CLI Commands","text":"<p>The Data Designer CLI provides commands for MCP configuration:</p> <pre><code># Configure MCP providers\ndata-designer config mcp\n\n# Configure tool configs\ndata-designer config tools\n\n# List all configurations (including MCP)\ndata-designer config list\n</code></pre> <p>Getting help</p> <p>See available commands: <pre><code>data-designer config --help\n</code></pre></p>"},{"location":"concepts/mcp/configure-mcp-cli/#configuring-mcp-providers","title":"Configuring MCP Providers","text":"<p>Run the interactive MCP provider configuration command:</p> <pre><code>data-designer config mcp\n</code></pre>"},{"location":"concepts/mcp/configure-mcp-cli/#provider-type-selection","title":"Provider Type Selection","text":"<p>The wizard first asks you to choose a provider type:</p> <ol> <li>Remote SSE: Connect to a pre-existing MCP server via HTTP Server-Sent Events</li> <li>Local stdio subprocess: Launch an MCP server as a subprocess</li> </ol>"},{"location":"concepts/mcp/configure-mcp-cli/#remote-sse-configuration","title":"Remote SSE Configuration","text":"<p>When configuring a Remote SSE provider, you'll be prompted for:</p> <ul> <li>Name: Unique identifier (e.g., <code>\"doc-search\"</code>)</li> <li>Endpoint: SSE endpoint URL (e.g., <code>\"http://localhost:8080/sse\"</code>)</li> <li>API Key: Optional API key or environment variable name</li> </ul>"},{"location":"concepts/mcp/configure-mcp-cli/#local-stdio-configuration","title":"Local Stdio Configuration","text":"<p>When configuring a Local stdio provider, you'll be prompted for:</p> <ul> <li>Name: Unique identifier (e.g., <code>\"local-tools\"</code>)</li> <li>Command: Executable to run (e.g., <code>\"python\"</code>)</li> <li>Arguments: Command-line arguments (e.g., <code>\"-m my_mcp_server\"</code>)</li> <li>Environment Variables: Optional environment variables for the subprocess</li> </ul>"},{"location":"concepts/mcp/configure-mcp-cli/#available-operations","title":"Available Operations","text":"<ul> <li>Add a new provider: Define a new MCP provider</li> <li>Update an existing provider: Modify provider settings</li> <li>Delete a provider: Remove a provider</li> <li>Delete all providers: Remove all MCP providers</li> </ul>"},{"location":"concepts/mcp/configure-mcp-cli/#configuring-tool-configs","title":"Configuring Tool Configs","text":"<p>Run the interactive tool configuration command:</p> <pre><code>data-designer config tools\n</code></pre> <p>Provider Required</p> <p>You need at least one MCP provider configured before adding tool configs. Run <code>data-designer config mcp</code> first if none exist.</p>"},{"location":"concepts/mcp/configure-mcp-cli/#configuration-options","title":"Configuration Options","text":"<p>When creating a tool config, you'll be prompted for:</p> <ul> <li>Tool Alias: Unique name for referencing in columns (e.g., <code>\"my-tools\"</code>)</li> <li>Providers: Select one or more MCP providers (checkbox selection)</li> <li>Allowed Tools: Optionally restrict to specific tools (leave empty for all)</li> <li>Max Tool Call Turns: Maximum tool-calling iterations (default: 5)</li> <li>Timeout: Per-call timeout in seconds (default: 60.0)</li> </ul>"},{"location":"concepts/mcp/configure-mcp-cli/#available-operations_1","title":"Available Operations","text":"<ul> <li>Add a new tool config: Define a new tool configuration</li> <li>Update an existing tool config: Modify settings</li> <li>Delete a tool config: Remove a tool configuration</li> <li>Delete all tool configs: Remove all tool configurations</li> </ul>"},{"location":"concepts/mcp/configure-mcp-cli/#listing-configurations","title":"Listing Configurations","text":"<p>View all current configurations:</p> <pre><code>data-designer config list\n</code></pre> <p>This command displays:</p> <ul> <li>Model Providers: All configured model providers</li> <li>Model Configurations: All configured models</li> <li>MCP Providers: All configured MCP providers with their endpoints</li> <li>Tool Configurations: All configured tool configs with their settings</li> </ul>"},{"location":"concepts/mcp/configure-mcp-cli/#manual-editing","title":"Manual Editing","text":"<p>You can also edit the YAML files directly for advanced configurations. The files are located at:</p> <ul> <li><code>~/.data-designer/mcp_providers.yaml</code></li> <li><code>~/.data-designer/tool_configs.yaml</code></li> </ul> <p>After manual edits, the changes take effect the next time you initialize <code>DataDesigner</code>.</p>"},{"location":"concepts/mcp/configure-mcp-cli/#see-also","title":"See Also","text":"<ul> <li>MCP Providers: Learn about provider configuration options</li> <li>Tool Configurations: Learn about tool config options</li> <li>Configure Model Settings with the CLI: CLI guide for model configuration</li> </ul>"},{"location":"concepts/mcp/enabling-tools/","title":"Enabling Tools on Columns","text":"<p>This guide explains how to enable tool use on LLM columns by connecting them to tool configurations via the <code>tool_alias</code> parameter.</p>"},{"location":"concepts/mcp/enabling-tools/#overview","title":"Overview","text":"<p>To enable tool use on an LLM column, you reference a <code>ToolConfig</code> by its alias. During generation, the model can then request tool calls, and Data Designer executes them and feeds the results back to the model.</p>"},{"location":"concepts/mcp/enabling-tools/#using-tool_alias","title":"Using tool_alias","text":"<p>Add the <code>tool_alias</code> parameter to any supported LLM column configuration:</p> <pre><code>import data_designer.config as dd\n\nbuilder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"answer\",\n        prompt=\"Use tools as needed to answer: {{ question }}\",\n        model_alias=\"nvidia-text\",\n        tool_alias=\"my-tools\",  # References a ToolConfig\n    )\n)\n</code></pre>"},{"location":"concepts/mcp/enabling-tools/#supported-column-types","title":"Supported Column Types","text":"<p>Tool use is supported on these column configuration types:</p> Column Type Description <code>LLMTextColumnConfig</code> Text generation with tool access <code>LLMCodeColumnConfig</code> Code generation with tool access <code>LLMStructuredColumnConfig</code> Structured JSON generation with tool access <code>LLMJudgeColumnConfig</code> Judge/scoring with tool access"},{"location":"concepts/mcp/enabling-tools/#how-it-works","title":"How It Works","text":"<p>When <code>tool_alias</code> is specified:</p> <ol> <li>Tool schemas are fetched from the referenced MCP providers</li> <li>Model receives tool schemas with the prompt</li> <li>Model can request tool calls in its response</li> <li>Data Designer executes calls and returns results to the model</li> <li>Iteration continues until the model produces a final answer (or limits are reached)</li> </ol>"},{"location":"concepts/mcp/enabling-tools/#complete-example","title":"Complete Example","text":"<p>Here's a complete workflow showing provider \u2192 ToolConfig \u2192 column:</p> <pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\n# 1. Configure MCP provider\nmcp_provider = dd.LocalStdioMCPProvider(\n    name=\"demo-mcp\",\n    command=\"python\",\n    args=[\"-m\", \"my_mcp_server\"],\n)\n\n# 2. Create DataDesigner instance with provider\ndata_designer = DataDesigner(mcp_providers=[mcp_provider])\n\n# 3. Define tool configuration\ntool_config = dd.ToolConfig(\n    tool_alias=\"my-tools\",\n    providers=[\"demo-mcp\"],\n    allow_tools=[\"search_docs\", \"get_fact\"],\n    max_tool_call_turns=5,\n)\n\n# 4. Create config builder with tool config\nbuilder = dd.DataDesignerConfigBuilder(tool_configs=[tool_config])\n\n# 5. Add columns that use tools\nbuilder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"question\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\"What is machine learning?\", \"Explain neural networks\"]\n        ),\n    )\n)\n\nbuilder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"answer\",\n        prompt=\"Use the available tools to research and answer: {{ question }}\",\n        model_alias=\"nvidia-text\",\n        tool_alias=\"my-tools\",  # Enable tools\n        with_trace=dd.TraceType.ALL_MESSAGES,  # Capture tool call history\n    )\n)\n\n# 6. Generate data\nresults = data_designer.preview(builder, num_records=5)\n</code></pre>"},{"location":"concepts/mcp/enabling-tools/#see-also","title":"See Also","text":"<ul> <li>Tool Configurations: Configure tool access and limits</li> <li>Traces: Capture and inspect tool call history</li> <li>MCP Providers: Configure MCP server connections</li> </ul>"},{"location":"concepts/mcp/mcp-providers/","title":"MCP Providers","text":"<p>MCP providers are external services that host and serve tools via the Model Context Protocol (MCP). Data Designer uses provider configurations to establish connections to these services.</p>"},{"location":"concepts/mcp/mcp-providers/#overview","title":"Overview","text":"<p>An MCP provider defines how Data Designer connects to a tool server. Data Designer supports two provider types:</p> Provider Class Connection Method Use Case <code>MCPProvider</code> HTTP Server-Sent Events Connect to a pre-existing MCP server <code>LocalStdioMCPProvider</code> Subprocess via stdin/stdout Launch an MCP server as a subprocess <p>When you create a <code>ToolConfig</code>, you reference providers by name, and Data Designer uses those provider settings to communicate with the appropriate MCP servers.</p>"},{"location":"concepts/mcp/mcp-providers/#mcpprovider-remote-sse","title":"MCPProvider (Remote SSE)","text":"<p>Use <code>MCPProvider</code> to connect to a pre-existing MCP server via Server-Sent Events:</p> <pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\nmcp_provider = dd.MCPProvider(\n    name=\"remote-mcp\",\n    endpoint=\"http://localhost:8080/sse\",\n    api_key=\"MCP_API_KEY\",  # Environment variable name\n)\n\ndata_designer = DataDesigner(mcp_providers=[mcp_provider])\n</code></pre>"},{"location":"concepts/mcp/mcp-providers/#mcpprovider-fields","title":"MCPProvider Fields","text":"Field Type Required Description <code>name</code> <code>str</code> Yes Unique identifier for the provider <code>endpoint</code> <code>str</code> Yes SSE endpoint URL (e.g., <code>\"http://localhost:8080/sse\"</code>) <code>api_key</code> <code>str</code> No API key or environment variable name <code>provider_type</code> <code>str</code> No Always <code>\"sse\"</code> (set automatically)"},{"location":"concepts/mcp/mcp-providers/#localstdiomcpprovider-subprocess","title":"LocalStdioMCPProvider (Subprocess)","text":"<p>Use <code>LocalStdioMCPProvider</code> to launch an MCP server as a subprocess:</p> <pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\nmcp_provider = dd.LocalStdioMCPProvider(\n    name=\"demo-mcp\",\n    command=\"python\",\n    args=[\"-m\", \"my_mcp_server_module\"],\n    env={\"MY_SERVICE_TOKEN\": \"...\"},\n)\n\ndata_designer = DataDesigner(mcp_providers=[mcp_provider])\n</code></pre>"},{"location":"concepts/mcp/mcp-providers/#localstdiomcpprovider-fields","title":"LocalStdioMCPProvider Fields","text":"Field Type Required Description <code>name</code> <code>str</code> Yes Unique identifier for the provider <code>command</code> <code>str</code> Yes Executable to run (e.g., <code>\"python\"</code>, <code>\"node\"</code>) <code>args</code> <code>list[str]</code> No Command-line arguments <code>env</code> <code>dict[str, str]</code> No Environment variables for the subprocess <code>provider_type</code> <code>str</code> No Always <code>\"stdio\"</code> (set automatically)"},{"location":"concepts/mcp/mcp-providers/#api-key-configuration","title":"API Key Configuration","text":"<p>The <code>api_key</code> field can be specified in two ways:</p> <ol> <li> <p>Environment variable name (recommended): Set <code>api_key</code> to the name of an environment variable (e.g., <code>\"MCP_API_KEY\"</code>). Data Designer will resolve it at runtime.</p> </li> <li> <p>Plain-text value: Set <code>api_key</code> to the actual API key string. This is less secure and not recommended for production.</p> </li> </ol> <pre><code># Method 1: Environment variable (recommended)\nprovider = dd.MCPProvider(\n    name=\"secure-mcp\",\n    endpoint=\"https://mcp.example.com/sse\",\n    api_key=\"MCP_API_KEY\",  # Will be resolved from environment\n)\n\n# Method 2: Direct value (not recommended)\nprovider = dd.MCPProvider(\n    name=\"secure-mcp\",\n    endpoint=\"https://mcp.example.com/sse\",\n    api_key=\"actual-api-key-value\",\n)\n</code></pre>"},{"location":"concepts/mcp/mcp-providers/#yaml-configuration","title":"YAML Configuration","text":"<p>Both provider types use a <code>provider_type</code> discriminator field in YAML configurations. When writing YAML configs manually (e.g., in <code>~/.data-designer/mcp_providers.yaml</code>), include the discriminator:</p> <pre><code>providers:\n  # Remote SSE provider\n  - name: doc-search\n    provider_type: sse\n    endpoint: http://localhost:8080/sse\n    api_key: ${MCP_API_KEY}\n\n  # Local stdio provider\n  - name: local-tools\n    provider_type: stdio\n    command: python\n    args:\n      - -m\n      - my_mcp_server\n    env:\n      DEBUG: \"true\"\n</code></pre>"},{"location":"concepts/mcp/mcp-providers/#using-multiple-providers","title":"Using Multiple Providers","text":"<p>You can configure multiple MCP providers and use them together in a single <code>ToolConfig</code>:</p> <pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\nproviders = [\n    dd.MCPProvider(\n        name=\"doc-search-mcp\",\n        endpoint=\"http://localhost:8080/sse\",\n    ),\n    dd.LocalStdioMCPProvider(\n        name=\"calculator-mcp\",\n        command=\"python\",\n        args=[\"-m\", \"calculator_mcp\"],\n    ),\n]\n\ndata_designer = DataDesigner(mcp_providers=providers)\n</code></pre>"},{"location":"concepts/mcp/mcp-providers/#see-also","title":"See Also","text":"<ul> <li>Tool Configurations: Configure tool access with ToolConfig</li> <li>Configure MCP with the CLI: Use the CLI to manage MCP providers</li> <li>Enabling Tools on Columns: Use tools in LLM columns</li> </ul>"},{"location":"concepts/mcp/safety-and-limits/","title":"Safety and Limits","text":"<p>This guide covers the safety controls available for tool use, including allowlists, turn budgets, and timeouts. These controls help prevent runaway loops and ensure predictable generation behavior.</p>"},{"location":"concepts/mcp/safety-and-limits/#overview","title":"Overview","text":"<p>When LLM columns use tools, the model can make multiple tool calls in a loop until it produces a final answer. Without limits, this could lead to:</p> <ul> <li>Excessive API calls and costs</li> <li>Long generation times</li> <li>Infinite loops if the model keeps requesting tools</li> </ul> <p>Data Designer provides three types of controls:</p> Control Purpose Tool allowlists Restrict which tools can be called Turn budgets Limit iterations of tool-calling Timeouts Cap individual tool call latency"},{"location":"concepts/mcp/safety-and-limits/#tool-allowlists","title":"Tool Allowlists","text":"<p>Restrict which tools are available using <code>allow_tools</code>:</p> <pre><code>import data_designer.config as dd\n\ntool_config = dd.ToolConfig(\n    tool_alias=\"restricted-tools\",\n    providers=[\"demo-mcp\"],\n    allow_tools=[\"search_docs\", \"list_docs\"],  # Only these tools\n)\n</code></pre>"},{"location":"concepts/mcp/safety-and-limits/#behavior","title":"Behavior","text":"Setting Behavior <code>allow_tools=None</code> (default) All tools from the providers are available <code>allow_tools=[\"tool1\", \"tool2\"]</code> Only the specified tools are available <p>Tools not in the allowlist won't be included in the schemas sent to the model, so the model won't know they exist.</p> <p>Use allowlists for untrusted tools</p> <p>If your MCP providers expose tools that could be dangerous or expensive, use allowlists to restrict access to only the tools you need.</p>"},{"location":"concepts/mcp/safety-and-limits/#turn-budgets","title":"Turn Budgets","text":"<p>Limit the number of tool-calling iterations using <code>max_tool_call_turns</code>:</p> <pre><code>import data_designer.config as dd\n\ntool_config = dd.ToolConfig(\n    tool_alias=\"limited-tools\",\n    providers=[\"demo-mcp\"],\n    max_tool_call_turns=5,  # Maximum 5 iterations (default)\n)\n</code></pre>"},{"location":"concepts/mcp/safety-and-limits/#understanding-turns","title":"Understanding Turns","text":"<p>A turn is one iteration where the LLM requests tool calls. With parallel tool calling, a single turn may execute multiple tools simultaneously.</p> Scenario Turn Count Model requests 1 tool 1 turn Model requests 3 tools in parallel 1 turn Model requests 1 tool, then 2 more, then 1 more 3 turns <p>This approach gives models flexibility to use parallel calling efficiently while still bounding total iterations.</p>"},{"location":"concepts/mcp/safety-and-limits/#graceful-budget-exhaustion","title":"Graceful Budget Exhaustion","text":"<p>When the turn limit is reached, Data Designer doesn't abruptly stop generation. Instead:</p> <ol> <li>The model's tool call request is recorded in the conversation</li> <li>Tool \"results\" are returned with a refusal message explaining the limit was reached</li> <li>The model receives this feedback and can produce a final response</li> </ol> <p>This ensures the model can still provide a useful answer based on the tools it already called, rather than failing silently.</p>"},{"location":"concepts/mcp/safety-and-limits/#timeouts","title":"Timeouts","text":"<p>Limit how long each tool call can take using <code>timeout_sec</code>:</p> <pre><code>import data_designer.config as dd\n\ntool_config = dd.ToolConfig(\n    tool_alias=\"fast-tools\",\n    providers=[\"demo-mcp\"],\n    timeout_sec=30.0,  # 30 seconds per tool call\n)\n</code></pre>"},{"location":"concepts/mcp/safety-and-limits/#timeout-behavior","title":"Timeout Behavior","text":"<p>When a timeout occurs:</p> <ol> <li>The tool call is terminated</li> <li>An error message is returned to the model</li> <li>The model can attempt recovery (retry, skip, or answer without the result)</li> </ol> <pre><code># Example error in trace when timeout occurs\n{\n    \"role\": \"tool\",\n    \"content\": \"Error: Tool 'search_docs' failed: Connection timeout after 30s\",\n    \"tool_call_id\": \"call_abc123\"\n}\n</code></pre>"},{"location":"concepts/mcp/safety-and-limits/#default-timeout","title":"Default Timeout","text":"<p>The default timeout is 60 seconds. Adjust based on your tools:</p> Tool Type Recommended Timeout Fast lookups 5-10 seconds Database queries 15-30 seconds External API calls 30-60 seconds Complex computations 60+ seconds"},{"location":"concepts/mcp/safety-and-limits/#combining-controls","title":"Combining Controls","text":"<p>You can use all controls together for defense in depth:</p> <pre><code>import data_designer.config as dd\n\ntool_config = dd.ToolConfig(\n    tool_alias=\"secure-tools\",\n    providers=[\"demo-mcp\"],\n    allow_tools=[\"search_docs\", \"get_fact\"],  # Restricted tools\n    max_tool_call_turns=3,                     # Limited iterations\n    timeout_sec=15.0,                          # Fast timeout\n)\n</code></pre>"},{"location":"concepts/mcp/safety-and-limits/#see-also","title":"See Also","text":"<ul> <li>Tool Configurations: Complete ToolConfig reference</li> <li>Traces: Monitor tool usage patterns</li> </ul>"},{"location":"concepts/mcp/tool-configs/","title":"Tool Configurations","text":"<p>Tool configurations define how LLM columns access external tools during generation. Each <code>ToolConfig</code> specifies which MCP providers to use, which tools are allowed, and operational limits.</p>"},{"location":"concepts/mcp/tool-configs/#overview","title":"Overview","text":"<p>A <code>ToolConfig</code> connects LLM columns to MCP providers. When you create column configurations (like <code>LLMTextColumnConfig</code> or <code>LLMCodeColumnConfig</code>), you reference a tool configuration by its alias. Data Designer uses the tool configuration to determine which tools are available and how to manage tool calls.</p>"},{"location":"concepts/mcp/tool-configs/#toolconfig-structure","title":"ToolConfig Structure","text":"<p>The <code>ToolConfig</code> class has the following fields:</p> Field Type Required Description <code>tool_alias</code> <code>str</code> Yes Unique identifier for this tool configuration (referenced by columns) <code>providers</code> <code>list[str]</code> Yes List of MCP provider names to use (can reference multiple providers) <code>allow_tools</code> <code>list[str]</code> No Restrict to specific tools (<code>None</code> = allow all tools from providers) <code>max_tool_call_turns</code> <code>int</code> No Maximum tool-calling iterations (default: 5) <code>timeout_sec</code> <code>float</code> No Per-call timeout in seconds (default: 60.0)"},{"location":"concepts/mcp/tool-configs/#examples","title":"Examples","text":""},{"location":"concepts/mcp/tool-configs/#basic-tool-configuration","title":"Basic Tool Configuration","text":"<pre><code>import data_designer.config as dd\n\ntool_config = dd.ToolConfig(\n    tool_alias=\"my-tools\",\n    providers=[\"demo-mcp\"],\n)\n</code></pre>"},{"location":"concepts/mcp/tool-configs/#restricting-allowed-tools","title":"Restricting Allowed Tools","text":"<pre><code>import data_designer.config as dd\n\ntool_config = dd.ToolConfig(\n    tool_alias=\"search-only\",\n    providers=[\"demo-mcp\"],\n    allow_tools=[\"search_docs\", \"list_docs\"],  # Only these tools allowed\n)\n</code></pre>"},{"location":"concepts/mcp/tool-configs/#using-multiple-providers","title":"Using Multiple Providers","text":"<p>A single <code>ToolConfig</code> can reference multiple MCP providers, allowing tools to be drawn from different sources:</p> <pre><code>import data_designer.config as dd\n\ntool_config = dd.ToolConfig(\n    tool_alias=\"multi-search\",\n    providers=[\"doc-search-mcp\", \"web-search-mcp\"],\n    allow_tools=[\"search_docs\", \"search_web\", \"list_docs\"],\n    max_tool_call_turns=10,\n)\n</code></pre> <p>When the model requests a tool call, Data Designer automatically finds which provider hosts that tool and routes the call appropriately.</p>"},{"location":"concepts/mcp/tool-configs/#setting-operational-limits","title":"Setting Operational Limits","text":"<pre><code>import data_designer.config as dd\n\ntool_config = dd.ToolConfig(\n    tool_alias=\"limited-tools\",\n    providers=[\"demo-mcp\"],\n    max_tool_call_turns=3,   # Maximum 3 tool-calling iterations\n    timeout_sec=30.0,        # 30 seconds per tool call\n)\n</code></pre>"},{"location":"concepts/mcp/tool-configs/#adding-to-config-builder","title":"Adding to Config Builder","text":"<p>Tool configurations can be added to the config builder in two ways:</p> <pre><code>import data_designer.config as dd\n\ntool_config = dd.ToolConfig(\n    tool_alias=\"my-tools\",\n    providers=[\"demo-mcp\"],\n)\n\n# Method 1: Pass at initialization\nbuilder = dd.DataDesignerConfigBuilder(tool_configs=[tool_config])\n\n# Method 2: Add later\nbuilder = dd.DataDesignerConfigBuilder()\nbuilder.add_tool_config(tool_config)\n</code></pre>"},{"location":"concepts/mcp/tool-configs/#understanding-turn-based-limiting","title":"Understanding Turn-Based Limiting","text":"<p>The <code>max_tool_call_turns</code> parameter limits how many tool-calling iterations (turns) are permitted, not the total number of individual tool calls.</p> <p>Turn-based vs call-based counting</p> <p>A turn is one iteration where the LLM requests tool calls. With parallel tool calling, a single turn may execute multiple tools simultaneously.</p> <p>For example, if the model requests 3 tools in parallel, that counts as 1 turn, not 3. This gives models flexibility to use parallel calling efficiently while still bounding total iterations.</p> <p>When the turn limit is reached, Data Designer gracefully refuses additional tool calls rather than failing abruptly. The model receives feedback explaining the limit was reached and can produce a final response based on the tools it already called.</p>"},{"location":"concepts/mcp/tool-configs/#see-also","title":"See Also","text":"<ul> <li>MCP Providers: Configure connections to MCP servers</li> <li>Enabling Tools on Columns: Reference tool configs from LLM columns</li> <li>Safety and Limits: Detailed guide on tool safety controls</li> <li>Configure MCP with the CLI: Use the CLI to manage tool configurations</li> </ul>"},{"location":"concepts/models/configure-model-settings-with-the-cli/","title":"Configuring Model Settings Using The CLI","text":"<p>The Data Designer CLI provides an interactive interface for creating and managing default model providers and model configurations stored in your Data Designer home directory (default: <code>~/.data-designer/</code>).</p>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#configuration-files","title":"Configuration Files","text":"<p>The CLI manages two YAML configuration files:</p> <ul> <li><code>model_providers.yaml</code>: Model provider configurations</li> <li><code>model_configs.yaml</code>: Model configurations</li> </ul> <p>Automatic Configuration</p> <p>If these configuration files don't already exist, the Data Designer library automatically creates them with default settings at runtime when first initialized.</p> <p>Custom Directory</p> <p>You can customize the configuration directory location with the <code>DATA_DESIGNER_HOME</code> environment variable: <pre><code>export DATA_DESIGNER_HOME=\"/path/to/your/custom/directory\"\n</code></pre></p>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#cli-commands","title":"CLI Commands","text":"<p>The Data Designer CLI provides four main configuration commands:</p> <pre><code># Configure model providers\ndata-designer config providers\n\n# Configure models\ndata-designer config models\n\n# List current configurations\ndata-designer config list\n\n# Reset all configurations\ndata-designer config reset\n</code></pre> <p>Getting help</p> <p>See available commands <pre><code>data-designer --help\n</code></pre></p> <p>See available sub-commands <pre><code>data-designer config --help\n</code></pre></p>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#managing-model-providers","title":"Managing Model Providers","text":"<p>Run the interactive provider configuration command:</p> <pre><code>data-designer config providers\n</code></pre>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#available-operations","title":"Available Operations","text":"<p>Add a new provider: Define a new provider by entering its name, endpoint URL, provider type, and optionally an API key (as plain text or as an environment variable name).</p> <p>Update an existing provider: Modify an existing provider's settings. All fields are pre-filled with current values.</p> <p>Delete a provider: Remove a provider and its associated models.</p> <p>Delete all providers: Remove all providers and their associated models.</p> <p>Change default provider: Set which provider is used by default. This option is only available when multiple providers are configured.</p>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#managing-model-configurations","title":"Managing Model Configurations","text":"<p>Run the interactive model configuration command:</p> <pre><code>data-designer config models\n</code></pre> <p>Provider Required</p> <p>You need at least one provider configured before adding models. Run <code>data-designer config providers</code> first if none exist.</p>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#available-operations_1","title":"Available Operations","text":"<p>Add a new model configuration</p> <p>Create a new model configuration with the following fields:</p> <ul> <li>Alias: A unique name for referencing this model in a column configuration.</li> <li>Model ID: The model identifier (e.g., <code>nvidia/nemotron-3-nano-30b-a3b</code>)</li> <li>Provider: Select from available providers (if multiple exist)</li> <li>Temperature: Sampling temperature (0.0 to 2.0)</li> <li>Top P: Nucleus sampling parameter (0.0 to 1.0)</li> <li>Max Tokens: Maximum output length (1 to 100000)</li> </ul> <p>Additional Settings</p> <p>To configure additional inference parameter settings or use distribution-based inference parameters, edit the <code>model_configs.yaml</code> file directly.</p> <p>Update an existing model configuration: Modify an existing model's configuration. All fields are pre-filled with current values.</p> <p>Delete a model configuration: Remove a single model configuration.</p> <p>Delete all model configurations: Remove all model configurations. The CLI will ask for confirmation before proceeding.</p>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#listing-configurations","title":"Listing Configurations","text":"<p>View all current configurations:</p> <pre><code>data-designer config list\n</code></pre> <p>This command displays:</p> <ul> <li>Model Providers: All configured providers with their endpoints (API keys are masked)</li> <li>Default Provider: The currently selected default provider</li> <li>Model Configurations: All configured models with their settings</li> </ul>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#resetting-configurations","title":"Resetting Configurations","text":"<p>Delete all configuration files:</p> <pre><code>data-designer config reset\n</code></pre> <p>The CLI will show which configuration files exist and ask for confirmation before deleting them.</p> <p>Destructive Operation</p> <p>This command permanently deletes all configuration files and resets to the default model providers and configurations. You'll need to reconfigure your custom configurations from scratch.</p>"},{"location":"concepts/models/configure-model-settings-with-the-cli/#see-also","title":"See Also","text":"<ul> <li>Default Model Settings: Pre-configured providers and model settings included with Data Designer</li> <li>Custom Model Settings: Learn how to create custom providers and model configurations</li> <li>Model Providers: Learn about the <code>ModelProvider</code> class and provider configuration</li> <li>Model Configurations: Learn about <code>ModelConfig</code></li> <li>Getting Started: Installation and basic usage example</li> </ul>"},{"location":"concepts/models/custom-model-settings/","title":"Custom Model Settings","text":"<p>While Data Designer ships with pre-configured model providers and configurations, you can create custom configurations to use different models, adjust inference parameters, or connect to custom API endpoints.</p>"},{"location":"concepts/models/custom-model-settings/#when-to-use-custom-settings","title":"When to Use Custom Settings","text":"<p>Use custom model settings when you need to:</p> <ul> <li>Use models not included in the defaults</li> <li>Adjust inference parameters (temperature, top_p, max_tokens) for specific use cases</li> <li>Add distribution-based inference parameters for variability</li> <li>Connect to self-hosted or custom model endpoints</li> <li>Create multiple variants of the same model with different settings</li> </ul>"},{"location":"concepts/models/custom-model-settings/#creating-and-using-custom-settings","title":"Creating and Using Custom Settings","text":""},{"location":"concepts/models/custom-model-settings/#custom-models-with-default-providers","title":"Custom Models with Default Providers","text":"<p>Create custom model configurations that use the default providers (no need to define providers yourself):</p> <pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\n# Create custom models using default providers\ncustom_models = [\n    # High-temperature for more variability\n    dd.ModelConfig(\n        alias=\"creative-writer\",\n        model=\"nvidia/nemotron-3-nano-30b-a3b\",\n        provider=\"nvidia\",  # Uses default NVIDIA provider\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=1.2,\n            top_p=0.98,\n            max_tokens=4096,\n        ),\n    ),\n    # Low-temperature for less variability\n    dd.ModelConfig(\n        alias=\"fact-checker\",\n        model=\"nvidia/nemotron-3-nano-30b-a3b\",\n        provider=\"nvidia\",  # Uses default NVIDIA provider\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=0.1,\n            top_p=0.9,\n            max_tokens=2048,\n        ),\n    ),\n]\n\n# Create DataDesigner (uses default providers)\ndata_designer = DataDesigner()\n\n# Pass custom models to config builder\nconfig_builder = dd.DataDesignerConfigBuilder(model_configs=custom_models)\n\n# Add a topic column using a categorical sampler\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"topic\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\"Artificial Intelligence\", \"Space Exploration\", \"Ancient History\", \"Climate Science\"],\n        ),\n    )\n)\n\n# Use your custom models\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"creative_story\",\n        model_alias=\"creative-writer\",\n        prompt=\"Write a creative short story about {{topic}}.\",\n    )\n)\n\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"facts\",\n        model_alias=\"fact-checker\",\n        prompt=\"List 3 facts about {{topic}}.\",\n    )\n)\n\n# Preview your dataset\npreview_result = data_designer.preview(config_builder=config_builder)\npreview_result.display_sample_record()\n</code></pre> <p>Default Providers Always Available</p> <p>When you only specify <code>model_configs</code>, the default model providers (NVIDIA, OpenAI, and OpenRouter) are still available. You only need to create custom providers if you want to connect to different endpoints or modify provider settings.</p> <p>Mixing Custom and Default Models</p> <p>When you provide custom <code>model_configs</code> to <code>DataDesignerConfigBuilder</code>, they replace the defaults entirely. To use custom model configs in addition to the default configs, use the add_model_config method:</p> <pre><code>import data_designer.config as dd\n\n# Load defaults first\nconfig_builder = dd.DataDesignerConfigBuilder()\n\n# Add custom model to defaults\nconfig_builder.add_model_config(\n    dd.ModelConfig(\n        alias=\"my-custom-model\",\n        model=\"nvidia/llama-3.3-nemotron-super-49b-v1.5\",\n        provider=\"nvidia\",  # Uses default provider\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=0.6,\n            max_tokens=8192,\n        ),\n    )\n)\n\n# Now you can use both default and custom models\n# Default: nvidia-text, nvidia-reasoning, nvidia-vision, etc.\n# Custom: my-custom-model\n</code></pre>"},{"location":"concepts/models/custom-model-settings/#custom-providers-with-custom-models","title":"Custom Providers with Custom Models","text":"<p>Define both custom providers and custom model configurations when you need to connect to services not included in the defaults:</p> <p>Network Accessibility</p> <p>The custom provider endpoints must be reachable from where Data Designer runs. Ensure network connectivity, firewall rules, and any VPN requirements are properly configured.</p> <pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\n# Step 1: Define custom providers\ncustom_providers = [\n    dd.ModelProvider(\n        name=\"my-custom-provider\",\n        endpoint=\"https://api.my-llm-service.com/v1\",\n        provider_type=\"openai\",  # OpenAI-compatible API\n        api_key=\"MY_SERVICE_API_KEY\",  # Environment variable name\n    ),\n    dd.ModelProvider(\n        name=\"my-self-hosted-provider\",\n        endpoint=\"https://my-org.internal.com/llm/v1\",\n        provider_type=\"openai\",\n        api_key=\"SELF_HOSTED_API_KEY\",\n    ),\n]\n\n# Step 2: Define custom models\ncustom_models = [\n    dd.ModelConfig(\n        alias=\"my-text-model\",\n        model=\"openai/some-model-id\",\n        provider=\"my-custom-provider\",  # References provider by name\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=0.85,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n    dd.ModelConfig(\n        alias=\"my-self-hosted-text-model\",\n        model=\"openai/some-hosted-model-id\",\n        provider=\"my-self-hosted-provider\",\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=0.7,\n            top_p=0.9,\n            max_tokens=1024,\n        ),\n    ),\n]\n\n# Step 3: Create DataDesigner with custom providers\ndata_designer = DataDesigner(model_providers=custom_providers)\n\n# Step 4: Create config builder with custom models\nconfig_builder = dd.DataDesignerConfigBuilder(model_configs=custom_models)\n\n# Step 5: Add a topic column using a categorical sampler\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"topic\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\"Technology\", \"Healthcare\", \"Finance\", \"Education\"],\n        ),\n    )\n)\n\n# Step 6: Use your custom model by referencing its alias\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"short_news_article\",\n        model_alias=\"my-text-model\",  # Reference custom alias\n        prompt=\"Write a short news article about the '{{topic}}' topic in 10 sentences.\",\n    )\n)\n\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"long_news_article\",\n        model_alias=\"my-self-hosted-text-model\",  # Reference custom alias\n        prompt=\"Write a detailed news article about the '{{topic}}' topic.\",\n    )\n)\n\n# Step 7: Preview your dataset\npreview_result = data_designer.preview(config_builder=config_builder)\npreview_result.display_sample_record()\n</code></pre>"},{"location":"concepts/models/custom-model-settings/#see-also","title":"See Also","text":"<ul> <li>Default Model Settings: Pre-configured providers and model settings</li> <li>Configure Model Settings With the CLI: CLI-based configuration</li> <li>Getting Started: Installation and basic usage example</li> </ul>"},{"location":"concepts/models/default-model-settings/","title":"Default Model Settings","text":"<p>Data Designer ships with pre-configured model providers and model configurations that make it easy to start generating synthetic data without manual setup.</p>"},{"location":"concepts/models/default-model-settings/#model-providers","title":"Model Providers","text":"<p>Data Designer includes a few default model providers that are configured automatically:</p>"},{"location":"concepts/models/default-model-settings/#nvidia-provider-nvidia","title":"NVIDIA Provider (<code>nvidia</code>)","text":"<ul> <li>Endpoint: <code>https://integrate.api.nvidia.com/v1</code></li> <li>API Key: Set via <code>NVIDIA_API_KEY</code> environment variable</li> <li>Models: Access to NVIDIA's hosted models from build.nvidia.com</li> <li>Getting Started: Sign up and get your API key at build.nvidia.com</li> </ul> <p>The NVIDIA provider gives you access to state-of-the-art models including Nemotron and other NVIDIA-optimized models.</p>"},{"location":"concepts/models/default-model-settings/#openai-provider-openai","title":"OpenAI Provider (<code>openai</code>)","text":"<ul> <li>Endpoint: <code>https://api.openai.com/v1</code></li> <li>API Key: Set via <code>OPENAI_API_KEY</code> environment variable</li> <li>Models: Access to OpenAI's model catalog</li> <li>Getting Started: Get your API key from platform.openai.com/api-keys</li> </ul> <p>The OpenAI provider gives you access to GPT models and other OpenAI offerings.</p>"},{"location":"concepts/models/default-model-settings/#openrouter-provider-openrouter","title":"OpenRouter Provider (<code>openrouter</code>)","text":"<ul> <li>Endpoint: <code>https://openrouter.ai/api/v1</code></li> <li>API Key: Set via <code>OPENROUTER_API_KEY</code> environment variable</li> <li>Models: Access to a wide variety of models through OpenRouter's unified API</li> <li>Getting Started: Get your API key from openrouter.ai</li> </ul> <p>The OpenRouter provider gives you access to a unified interface for many different language models from various providers.</p>"},{"location":"concepts/models/default-model-settings/#model-configurations","title":"Model Configurations","text":"<p>Data Designer provides pre-configured model aliases for common use cases. When you create a <code>DataDesignerConfigBuilder</code> without specifying <code>model_configs</code>, these default configurations are automatically available.</p>"},{"location":"concepts/models/default-model-settings/#nvidia-models","title":"NVIDIA Models","text":"<p>The following model configurations are automatically available when <code>NVIDIA_API_KEY</code> is set:</p> Alias Model Use Case Inference Parameters <code>nvidia-text</code> <code>nvidia/nemotron-3-nano-30b-a3b</code> General text generation <code>temperature=1.0, top_p=1.0</code> <code>nvidia-reasoning</code> <code>openai/gpt-oss-20b</code> Reasoning and analysis tasks <code>temperature=0.35, top_p=0.95</code> <code>nvidia-vision</code> <code>nvidia/nemotron-nano-12b-v2-vl</code> Vision and image understanding <code>temperature=0.85, top_p=0.95</code> <code>nvidia-embedding</code> <code>nvidia/llama-3.2-nv-embedqa-1b-v2</code> Text embeddings <code>encoding_format=\"float\", extra_body={\"input_type\": \"query\"}</code>"},{"location":"concepts/models/default-model-settings/#openai-models","title":"OpenAI Models","text":"<p>The following model configurations are automatically available when <code>OPENAI_API_KEY</code> is set:</p> Alias Model Use Case Inference Parameters <code>openai-text</code> <code>gpt-4.1</code> General text generation <code>temperature=0.85, top_p=0.95</code> <code>openai-reasoning</code> <code>gpt-5</code> Reasoning and analysis tasks <code>temperature=0.35, top_p=0.95</code> <code>openai-vision</code> <code>gpt-5</code> Vision and image understanding <code>temperature=0.85, top_p=0.95</code> <code>openai-embedding</code> <code>text-embedding-3-large</code> Text embeddings <code>encoding_format=\"float\"</code>"},{"location":"concepts/models/default-model-settings/#openrouter-models","title":"OpenRouter Models","text":"<p>The following model configurations are automatically available when <code>OPENROUTER_API_KEY</code> is set:</p> Alias Model Use Case Inference Parameters <code>openrouter-text</code> <code>nvidia/nemotron-3-nano-30b-a3b</code> General text generation <code>temperature=1.0, top_p=1.0</code> <code>openrouter-reasoning</code> <code>openai/gpt-oss-20b</code> Reasoning and analysis tasks <code>temperature=0.35, top_p=0.95</code> <code>openrouter-vision</code> <code>nvidia/nemotron-nano-12b-v2-vl</code> Vision and image understanding <code>temperature=0.85, top_p=0.95</code> <code>openrouter-embedding</code> <code>openai/text-embedding-3-large</code> Text embeddings <code>encoding_format=\"float\"</code>"},{"location":"concepts/models/default-model-settings/#using-default-settings","title":"Using Default Settings","text":"<p>Default settings work out of the box - no configuration needed! Simply create <code>DataDesigner</code> and <code>DataDesignerConfigBuilder</code> instances without any arguments, and reference the default model aliases in your column configurations.</p> <p>For a complete example showing how to use default model settings, see the Getting Started page.</p>"},{"location":"concepts/models/default-model-settings/#how-default-model-providers-and-configurations-work","title":"How Default Model Providers and Configurations Work","text":"<p>When the Data Designer library or the CLI is initialized, default model configurations and providers are stored in the Data Designer home directory for easy access and customization if they do not already exist. These configuration files serve as the single source of truth for model settings. By default they are saved to the following paths:</p> <ul> <li>Model Configs: <code>~/.data-designer/model_configs.yaml</code></li> <li>Model Providers: <code>~/.data-designer/model_providers.yaml</code></li> </ul> <p>Tip</p> <p>While these files provide a convenient way to specify settings for your model providers and configuration you use most often, they can always be set programmatically in your SDG workflow.</p> <p>You can customize the home directory location by setting the <code>DATA_DESIGNER_HOME</code> environment variable:</p> <pre><code># In your .bashrc, .zshrc, or similar\nexport DATA_DESIGNER_HOME=\"/path/to/your/custom/directory\"\n</code></pre> <p>These configuration files can be modified in two ways:</p> <ol> <li>Using the CLI: Run CLI commands to add, update, or delete model configurations and providers</li> <li>Manual editing: Directly edit the YAML files with your preferred text editor</li> </ol> <p>Both methods operate on the same files, ensuring consistency across your entire Data Designer setup.</p>"},{"location":"concepts/models/default-model-settings/#important-notes","title":"Important Notes","text":"<p>API Key Requirements</p> <p>While default model configurations are always available, you need to set the appropriate API key environment variable (<code>NVIDIA_API_KEY</code>, <code>OPENAI_API_KEY</code>, or <code>OPENROUTER_API_KEY</code>) to actually use the corresponding models for data generation. Without a valid API key, any attempt to generate data using that provider's models will fail.</p> <p>Environment Variables</p> <p>Store your API keys in environment variables rather than hardcoding them in your scripts:</p> <pre><code># In your .bashrc, .zshrc, or similar\nexport NVIDIA_API_KEY=\"your-api-key-here\"\nexport OPENAI_API_KEY=\"your-openai-api-key-here\"\nexport OPENROUTER_API_KEY=\"your-openrouter-api-key-here\"\n</code></pre>"},{"location":"concepts/models/default-model-settings/#see-also","title":"See Also","text":"<ul> <li>Custom Model Settings: Learn how to create custom providers and model configurations</li> <li>Configure Model Settings With the CLI: Learn how to use the CLI to manage model settings</li> <li>Model Configurations: Learn about model configurations</li> </ul>"},{"location":"concepts/models/inference-parameters/","title":"Inference Parameters","text":"<p>Inference parameters control how models generate responses during synthetic data generation. Data Designer provides two types of inference parameters: <code>ChatCompletionInferenceParams</code> for text/code/structured generation and <code>EmbeddingInferenceParams</code> for embedding generation.</p>"},{"location":"concepts/models/inference-parameters/#overview","title":"Overview","text":"<p>When you create a <code>ModelConfig</code>, you can specify inference parameters to adjust model behavior. These parameters control aspects like randomness (temperature), diversity (top_p), context size (max_tokens), and more. Data Designer supports both static values and dynamic distribution-based sampling for certain parameters.</p>"},{"location":"concepts/models/inference-parameters/#chat-completion-inference-parameters","title":"Chat Completion Inference Parameters","text":"<p>The <code>ChatCompletionInferenceParams</code> class controls how models generate text completions (for text, code, and structured data generation). It provides fine-grained control over generation behavior and supports both static values and dynamic distribution-based sampling.</p>"},{"location":"concepts/models/inference-parameters/#fields","title":"Fields","text":"Field Type Required Description <code>temperature</code> <code>float</code> or <code>Distribution</code> No Controls randomness in generation (0.0 to 2.0). Higher values = more creative/random <code>top_p</code> <code>float</code> or <code>Distribution</code> No Nucleus sampling parameter (0.0 to 1.0). Controls diversity by filtering low-probability tokens <code>max_tokens</code> <code>int</code> No Maximum number of tokens to generate in the response (\u2265 1) <code>max_parallel_requests</code> <code>int</code> No Maximum concurrent API requests to this model (default: 4, \u2265 1). See Concurrency Control below. <code>timeout</code> <code>int</code> No API request timeout in seconds (\u2265 1) <code>extra_body</code> <code>dict[str, Any]</code> No Additional parameters to include in the API request body <p>Default Values</p> <p>If <code>temperature</code>, <code>top_p</code>, or <code>max_tokens</code> are not provided, the model provider's default values will be used. Different providers and models may have different defaults.</p> <p>Controlling Reasoning Effort for GPT-OSS Models</p> <p>For gpt-oss models like <code>gpt-oss-20b</code> and <code>gpt-oss-120b</code>, you can control the reasoning effort using the <code>extra_body</code> parameter:</p> <pre><code>import data_designer.config as dd\n\n# High reasoning effort (more thorough, slower)\ninference_parameters = dd.ChatCompletionInferenceParams(\n    extra_body={\"reasoning_effort\": \"high\"}\n)\n\n# Medium reasoning effort (balanced)\ninference_parameters = dd.ChatCompletionInferenceParams(\n    extra_body={\"reasoning_effort\": \"medium\"}\n)\n\n# Low reasoning effort (faster, less thorough)\ninference_parameters = dd.ChatCompletionInferenceParams(\n    extra_body={\"reasoning_effort\": \"low\"}\n)\n</code></pre>"},{"location":"concepts/models/inference-parameters/#temperature-and-top-p-guidelines","title":"Temperature and Top P Guidelines","text":"<ul> <li> <p>Temperature:</p> <ul> <li><code>0.0-0.3</code>: Highly deterministic, focused outputs (ideal for structured/reasoning tasks)</li> <li><code>0.4-0.7</code>: Balanced creativity and coherence (general purpose)</li> <li><code>0.8-1.0</code>: Creative, diverse outputs (ideal for creative writing)</li> <li><code>1.0+</code>: Highly random and experimental</li> </ul> </li> <li> <p>Top P:</p> <ul> <li><code>0.1-0.5</code>: Very focused, only most likely tokens</li> <li><code>0.6-0.9</code>: Balanced diversity</li> <li><code>0.95-1.0</code>: Maximum diversity, including less likely tokens</li> </ul> </li> </ul> <p>Adjusting Temperature and Top P Together</p> <p>When tuning both parameters simultaneously, consider these combinations:</p> <ul> <li>For deterministic/structured outputs: Low temperature (<code>0.0-0.3</code>) + moderate-to-high top_p (<code>0.8-0.95</code>)<ul> <li>The low temperature ensures focus, while top_p allows some token diversity</li> </ul> </li> <li>For balanced generation: Moderate temperature (<code>0.5-0.7</code>) + high top_p (<code>0.9-0.95</code>)<ul> <li>This is a good starting point for most use cases</li> </ul> </li> <li>For creative outputs: Higher temperature (<code>0.8-1.0</code>) + high top_p (<code>0.95-1.0</code>)<ul> <li>Both parameters work together to maximize diversity</li> </ul> </li> </ul> <p>Avoid: Setting both very low (overly restrictive) or adjusting both dramatically at once. When experimenting, adjust one parameter at a time to understand its individual effect.</p>"},{"location":"concepts/models/inference-parameters/#distribution-based-inference-parameters","title":"Distribution-Based Inference Parameters","text":"<p>For <code>temperature</code> and <code>top_p</code> in <code>ChatCompletionInferenceParams</code>, you can specify distributions instead of fixed values. This allows Data Designer to sample different values for each generation request, introducing controlled variability into your synthetic data.</p>"},{"location":"concepts/models/inference-parameters/#uniform-distribution","title":"Uniform Distribution","text":"<p>Samples values uniformly between a low and high bound:</p> <pre><code>import data_designer.config as dd\n\ninference_params = dd.ChatCompletionInferenceParams(\n    temperature=dd.UniformDistribution(\n        params=dd.UniformDistributionParams(low=0.7, high=1.0)\n    ),\n)\n</code></pre>"},{"location":"concepts/models/inference-parameters/#manual-distribution","title":"Manual Distribution","text":"<p>Samples from a discrete set of values with optional weights:</p> <pre><code>import data_designer.config as dd\n\n# Equal probability for each value\ninference_params = dd.ChatCompletionInferenceParams(\n    temperature=dd.ManualDistribution(\n        params=dd.ManualDistributionParams(values=[0.5, 0.7, 0.9])\n    ),\n)\n\n# Weighted probabilities (normalized automatically)\ninference_params = dd.ChatCompletionInferenceParams(\n    top_p=dd.ManualDistribution(\n        params=dd.ManualDistributionParams(\n            values=[0.8, 0.9, 0.95],\n            weights=[0.2, 0.5, 0.3]  # 20%, 50%, 30% probability\n        )\n    ),\n)\n</code></pre>"},{"location":"concepts/models/inference-parameters/#concurrency-control","title":"Concurrency Control","text":"<p>The <code>max_parallel_requests</code> parameter controls how many concurrent API calls Data Designer makes to a specific model. This directly impacts throughput and should be tuned to match your inference server's capacity.</p> <p>Performance Tuning</p> <p>For recommended values by deployment type (NVIDIA API Catalog, vLLM, OpenAI, NIMs) and detailed optimization strategies, see the Architecture &amp; Performance guide.</p>"},{"location":"concepts/models/inference-parameters/#embedding-inference-parameters","title":"Embedding Inference Parameters","text":"<p>The <code>EmbeddingInferenceParams</code> class controls how models generate embeddings. This is used when working with embedding models for tasks like semantic search or similarity analysis.</p>"},{"location":"concepts/models/inference-parameters/#fields_1","title":"Fields","text":"Field Type Required Description <code>encoding_format</code> <code>Literal[\"float\", \"base64\"]</code> No Format of the embedding encoding (default: \"float\") <code>dimensions</code> <code>int</code> No Number of dimensions for the embedding <code>max_parallel_requests</code> <code>int</code> No Maximum concurrent API requests (default: 4, \u2265 1) <code>timeout</code> <code>int</code> No API request timeout in seconds (\u2265 1) <code>extra_body</code> <code>dict[str, Any]</code> No Additional parameters to include in the API request body"},{"location":"concepts/models/inference-parameters/#see-also","title":"See Also","text":"<ul> <li>Default Model Settings: Pre-configured model settings included with Data Designer</li> <li>Custom Model Settings: Learn how to create custom providers and model configurations</li> <li>Model Configurations: Learn about configuring model settings</li> <li>Model Providers: Learn about configuring model providers</li> <li>Architecture &amp; Performance: Understanding separation of concerns and optimizing concurrency</li> </ul>"},{"location":"concepts/models/model-configs/","title":"Model Configurations","text":"<p>Model configurations define the specific models you use for synthetic data generation and their associated inference parameters. Each <code>ModelConfig</code> represents a named model that can be referenced throughout your data generation workflows.</p>"},{"location":"concepts/models/model-configs/#overview","title":"Overview","text":"<p>A <code>ModelConfig</code> specifies which LLM model to use and how it should behave during generation. When you create column configurations (like <code>LLMText</code>, <code>LLMCode</code>, or <code>LLMStructured</code>), you reference a model by its alias. Data Designer uses the model configuration to determine which model to call and with what parameters.</p>"},{"location":"concepts/models/model-configs/#modelconfig-structure","title":"ModelConfig Structure","text":"<p>The <code>ModelConfig</code> class has the following fields:</p> Field Type Required Description <code>alias</code> <code>str</code> Yes Unique identifier for this model configuration (e.g., <code>\"my-text-model\"</code>, <code>\"reasoning-model\"</code>) <code>model</code> <code>str</code> Yes Model identifier as recognized by the provider (e.g., <code>\"nvidia/nemotron-3-nano-30b-a3b\"</code>, <code>\"gpt-4\"</code>) <code>inference_parameters</code> <code>InferenceParamsT</code> No Controls model behavior during generation. Use <code>ChatCompletionInferenceParams</code> for text/code/structured generation or <code>EmbeddingInferenceParams</code> for embeddings. Defaults to <code>ChatCompletionInferenceParams()</code> if not provided. The generation type is automatically determined by the inference parameters type. See Inference Parameters for details. <code>provider</code> <code>str</code> No Reference to the name of the Provider to use (e.g., <code>\"nvidia\"</code>, <code>\"openai\"</code>, <code>\"openrouter\"</code>). If not specified, one set as the default provider, which may resolve to the first provider if there are more than one <code>skip_health_check</code> <code>bool</code> No Whether to skip the health check for this model. Defaults to <code>False</code>. Set to <code>True</code> to skip health checks when you know the model is accessible or want to defer validation."},{"location":"concepts/models/model-configs/#examples","title":"Examples","text":""},{"location":"concepts/models/model-configs/#basic-model-configuration","title":"Basic Model Configuration","text":"<pre><code>import data_designer.config as dd\n\n# Simple model configuration with fixed parameters\nmodel_config = dd.ModelConfig(\n    alias=\"my-text-model\",\n    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n    provider=\"nvidia\",\n    inference_parameters=dd.ChatCompletionInferenceParams(\n        temperature=0.85,\n        top_p=0.95,\n        max_tokens=2048,\n    ),\n)\n</code></pre>"},{"location":"concepts/models/model-configs/#multiple-model-configurations-for-different-tasks","title":"Multiple Model Configurations for Different Tasks","text":"<pre><code>import data_designer.config as dd\n\nmodel_configs = [\n    # Creative tasks\n    dd.ModelConfig(\n        alias=\"creative-model\",\n        model=\"nvidia/nemotron-3-nano-30b-a3b\",\n        provider=\"nvidia\",\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=0.9,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n    # Critic tasks\n    dd.ModelConfig(\n        alias=\"critic-model\",\n        model=\"nvidia/nemotron-3-nano-30b-a3b\",\n        provider=\"nvidia\",\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=0.25,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n    # Reasoning and structured tasks\n    dd.ModelConfig(\n        alias=\"reasoning-model\",\n        model=\"openai/gpt-oss-20b\",\n        provider=\"nvidia\",\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=0.3,\n            top_p=0.9,\n            max_tokens=4096,\n        ),\n    ),\n    # Vision tasks\n    dd.ModelConfig(\n        alias=\"vision-model\",\n        model=\"nvidia/nemotron-nano-12b-v2-vl\",\n        provider=\"nvidia\",\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=0.7,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n    # Embedding tasks\n    dd.ModelConfig(\n        alias=\"embedding_model\",\n        model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n        provider=\"nvidia\",\n        inference_parameters=dd.EmbeddingInferenceParams(\n            encoding_format=\"float\",\n            extra_body={\n                \"input_type\": \"query\"\n            }\n        )\n    )\n]\n</code></pre> <p>Experiment with max_tokens for Task-Specific Model Configurations</p> <p>The number of tokens required to generate a single data entry can vary significantly with use case. For example, reasoning models often need more tokens to \"think through\" problems before generating a response. Note that <code>max_tokens</code> specifies the maximum number of output tokens to generate in the response, so set this value based on the expected length of the generated content.</p>"},{"location":"concepts/models/model-configs/#skipping-health-checks","title":"Skipping Health Checks","text":"<p>By default, Data Designer runs a health check for each model before starting data generation to ensure the model is accessible and configured correctly. You can skip this health check for specific models by setting <code>skip_health_check=True</code>:</p> <pre><code>import data_designer.config as dd\n\nmodel_config = dd.ModelConfig(\n    alias=\"my-model\",\n    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n    provider=\"nvidia\",\n    inference_parameters=dd.ChatCompletionInferenceParams(\n        temperature=0.85,\n        top_p=0.95,\n        max_tokens=2048,\n    ),\n    skip_health_check=True,  # Skip health check for this model\n)\n</code></pre> <p>When to Skip Health Checks</p> <p>Skipping health checks can be useful when:</p> <ul> <li>You've already verified the model is accessible and want to speed up initialization</li> <li>You're using a model that doesn't support the standard health check format</li> <li>You want to defer model validation until the model is actually used</li> </ul> <p>Note that skipping health checks means errors will only be discovered during actual data generation.</p>"},{"location":"concepts/models/model-configs/#see-also","title":"See Also","text":"<ul> <li>Inference Parameters: Detailed guide to inference parameters and how to configure them</li> <li>Model Providers: Learn about configuring model providers</li> <li>Default Model Settings: Pre-configured model settings included with Data Designer</li> <li>Custom Model Settings: Learn how to create custom providers and model configurations</li> <li>Configure Model Settings With the CLI: Use the CLI to manage model settings</li> <li>Column Configurations: Learn how to use models in column configurations</li> <li>Architecture &amp; Performance: Understanding separation of concerns and optimizing concurrency</li> </ul>"},{"location":"concepts/models/model-providers/","title":"Model Providers","text":"<p>Model providers are external services that host and serve models. Data Designer uses the <code>ModelProvider</code> class to configure connections to these services.</p>"},{"location":"concepts/models/model-providers/#overview","title":"Overview","text":"<p>A <code>ModelProvider</code> defines how Data Designer connects to a provider's API endpoint. When you create a <code>ModelConfig</code>, you reference a provider by name, and Data Designer uses that provider's settings to make API calls to the appropriate endpoint.</p>"},{"location":"concepts/models/model-providers/#modelprovider-configuration","title":"ModelProvider Configuration","text":"<p>The <code>ModelProvider</code> class has the following fields:</p> Field Type Required Description <code>name</code> <code>str</code> Yes Unique identifier for the provider (e.g., <code>\"nvidia\"</code>, <code>\"openai\"</code>, <code>\"openrouter\"</code>) <code>endpoint</code> <code>str</code> Yes API endpoint URL (e.g., <code>\"https://integrate.api.nvidia.com/v1\"</code>) <code>provider_type</code> <code>str</code> No Provider type (default: <code>\"openai\"</code>). Uses OpenAI-compatible API format <code>api_key</code> <code>str</code> No API key or environment variable name (e.g., <code>\"NVIDIA_API_KEY\"</code>) <code>extra_body</code> <code>dict[str, Any]</code> No Additional parameters to include in the request body of all API requests to the provider. <code>extra_headers</code> <code>dict[str, str]</code> No Additional headers to include in all API requests to the provider."},{"location":"concepts/models/model-providers/#api-key-configuration","title":"API Key Configuration","text":"<p>The <code>api_key</code> field can be specified in two ways:</p> <ol> <li> <p>Environment variable name (recommended): Set <code>api_key</code> to the name of an environment variable (e.g., <code>\"NVIDIA_API_KEY\"</code>). Data Designer will automatically resolve it at runtime.</p> </li> <li> <p>Plain-text value: Set <code>api_key</code> to the actual API key string. This is less secure and not recommended for production use.</p> </li> </ol> <pre><code># Method 1: Environment variable (recommended)\nprovider = ModelProvider(\n    name=\"nvidia\",\n    endpoint=\"https://integrate.api.nvidia.com/v1\",\n    api_key=\"NVIDIA_API_KEY\",  # Will be resolved from environment\n)\n\n# Method 2: Direct value (not recommended)\nprovider = ModelProvider(\n    name=\"nvidia\",\n    endpoint=\"https://integrate.api.nvidia.com/v1\",\n    api_key=\"nvapi-abc123...\",  # Direct API key\n)\n</code></pre>"},{"location":"concepts/models/model-providers/#see-also","title":"See Also","text":"<ul> <li>Model Configurations: Learn about configuring models</li> <li>Inference Parameters: Detailed guide to inference parameters and how to configure them</li> <li>Default Model Settings: Pre-configured providers and model settings included with Data Designer</li> <li>Custom Model Settings: Learn how to create custom providers and model configurations</li> <li>Model Configurations: Learn about configuring models</li> <li>Inference Parameters: Detailed guide to inference parameters and how to configure them</li> <li>Configure Model Settings With the CLI: Use the CLI to manage providers and model settings</li> <li>Getting Started: Installation and basic usage example</li> </ul>"},{"location":"devnotes/","title":"Dev Notes","text":"<p>Welcome to NeMo Data Designer Dev Notes! Here you'll find in-depth guides, tutorials, and insights about synthetic data generation.</p>"},{"location":"devnotes/designing-data-designer-why-sdg-is-a-systems-problem/","title":"Designing Data Designer: Why SDG Is a Systems Problem","text":"<p>Synthetic data generation is more than a single prompt to a large language model. In this post, we walk through the design principles behind NeMo Data Designer and explain why we built it as a composable orchestration framework - treating SDG as a system of specialized stages rather than a monolithic generation task.</p> <p></p> <p>When people first encounter synthetic data generation, the instinct is natural: write a detailed prompt, call a powerful LLM, and collect the output. For quick experiments and small datasets, this works fine. But as you push toward production-quality data at scale - millions of records, multiple interrelated fields, strict quality requirements - the cracks start to show. We built Data Designer around the conviction that SDG is fundamentally a systems problem, and the framework's architecture reflects that belief at every level.</p> <p>This post isn't about a specific dataset or benchmark result. Instead, we want to pull back the curtain on the design principles that shaped Data Designer itself, and share the reasoning behind the decisions we made.</p>"},{"location":"devnotes/designing-data-designer-why-sdg-is-a-systems-problem/#a-prompt-is-not-all-you-need","title":"A Prompt Is Not All You Need","text":"<p>The most common approach to synthetic data generation looks something like this: pack everything you need into one prompt - the schema, the constraints, the style guidelines, the quality criteria - and send it to the biggest model you have access to. Crank up <code>max_tokens</code>, parse the output, and repeat.</p> <p>This approach has a ceiling, and you hit it faster than you'd expect.</p> <p>Quality degrades when you overload a single call. Asking one model to simultaneously generate content, maintain diversity across a dataset, follow a complex schema, and self-assess quality is asking it to solve several distinct problems at once. The model has to allocate its \"attention budget\" across all of these competing objectives, and something always gives. Usually it's the subtler requirements - diversity narrows, edge cases get dropped, and the data starts looking suspiciously uniform.</p> <p>Prompts become unmaintainable. As requirements grow, the prompt balloons. What started as a clean paragraph becomes a multi-page document with nested instructions, conditional logic described in natural language, and examples that compete for context window space. At this point, iterating on one aspect of generation (say, adjusting the complexity distribution or tweaking the output schema) means editing a fragile mega-prompt and hoping nothing else breaks.</p> <p>There are no quality gates. In a single-call setup, validation happens after you've already spent the compute. If 30% of your records are malformed or low-quality, you find out at the end and either filter them out (wasting the tokens) or re-generate (wasting even more). There's no mechanism to catch problems between stages, because there are no stages.</p> <p>Scaling is limited. A single model call is a single point of failure. You can parallelize across records, but you can't parallelize across stages of generation, and you can't route different parts of the task to models that are better suited for them.</p> <p>None of these are problems with LLMs themselves - they're problems with treating SDG as a single-step task. The fix isn't a better model. It's a better architecture.</p>"},{"location":"devnotes/designing-data-designer-why-sdg-is-a-systems-problem/#sdg-as-a-system-of-specialized-stages","title":"SDG as a System of Specialized Stages","text":"<p>The shift in thinking is straightforward: instead of asking one model to do everything, decompose the generation task into a pipeline of focused stages, each responsible for one well-defined job.</p> <p>Regardless of what you're generating - QA pairs for retrieval training, reasoning traces for pretraining, multi-turn conversations for alignment, product reviews for testing, or labeled examples for classification - a well-decomposed SDG pipeline typically has four kinds of stages:</p> <ol> <li> <p>Seed curation. Control what goes in. Whether you're sampling from an existing corpus, selecting subsets of your data, or generating realistic persona profiles with demographic and personality attributes, the seed data defines the distribution your synthetic data will cover. This is where you control diversity and domain coverage - before any LLM is involved - so that downstream generation stages inherit that diversity naturally through their prompts.</p> </li> <li> <p>Staged generation. Each generation step has a focused job. One stage might extract structured metadata from a document. Another might generate content grounded in that metadata. A third might transform or enrich that content further. Because each stage has a narrow scope, its prompt is simple, its output is predictable, and it's easy to iterate on independently.</p> </li> <li> <p>Dependency management. Later stages build on earlier outputs. A content generation stage needs access to extracted metadata. A formatting stage needs the generated content. These dependencies form a directed acyclic graph (DAG), and the system needs to resolve that graph automatically - so you can focus on defining the stages, not orchestrating them.</p> </li> <li> <p>Quality control. Validation and scoring aren't afterthoughts - they're explicit stages in the pipeline. An LLM judge can evaluate the output of a generation stage and a validator can check structural constraints. Because these run as part of the generation pipeline, you can identify quality issues early and make informed decisions about which records to keep before investing in further downstream processing.</p> </li> </ol> <p>This decomposition buys you something that a single prompt never can: the ability to reason about, test, and improve each stage independently.</p>"},{"location":"devnotes/designing-data-designer-why-sdg-is-a-systems-problem/#design-principles-behind-data-designer","title":"Design Principles Behind Data Designer","text":"<p>With that framing in mind, here are the principles that guided Data Designer's architecture.</p>"},{"location":"devnotes/designing-data-designer-why-sdg-is-a-systems-problem/#declarative-over-imperative","title":"Declarative over imperative","text":"<p>When you define a Data Designer workflow, you describe the structure of the dataset you want - not the execution plan for how to generate it. You declare columns, their types, their prompts or schemas, and the models they should use. The framework handles the rest: resolving dependencies, scheduling execution, managing parallelism, batching requests, and retrying failures.</p> <p>This is a deliberate choice. We wanted the configuration to read like a description of the desired output, not a script full of API calls and error handling. It makes workflows easier to read, easier to share, and easier to modify - you can swap a model, adjust a prompt, or add a validation stage without rewriting control flow.</p>"},{"location":"devnotes/designing-data-designer-why-sdg-is-a-systems-problem/#columns-as-composable-units","title":"Columns as composable units","text":"<p>The core abstraction in Data Designer is the column. Each column represents a single field in your dataset, and each column has a well-defined generation strategy: it might be an LLM text generation call, a structured output with a Pydantic schema, an embedding computation, a sampler, a Jinja2 expression that combines other columns, or a quality evaluation from an LLM judge.</p> <p>Columns reference each other through Jinja2 templates. When one column's prompt includes <code>{{ document_artifacts }}</code>, the framework knows that column depends on the <code>document_artifacts</code> column and must run after it. These references are automatically extracted to build a dependency graph, and the framework topologically sorts the graph to determine execution order. You don't write orchestration code - you just write columns, and the DAG emerges from the references between them.</p> <p>This composability is what makes it possible to go from a simple two-column workflow to a complex multi-stage pipeline without changing the underlying execution model.</p>"},{"location":"devnotes/designing-data-designer-why-sdg-is-a-systems-problem/#multi-model-by-design","title":"Multi-model by design","text":"<p>Not every stage in a pipeline needs the same model. Extracting structured metadata from a document is a different task than generating creative long-form content, which is a different task than scoring quality, which is a different task than computing embeddings.</p> <p>Data Designer treats multi-model orchestration as a first-class concern. Each column can specify its own model alias, and the framework manages model routing, per-model parallelism limits, and usage tracking independently. In practice, this means you can use a large reasoning model for your hardest generation stage, a smaller and faster model for evaluation and scoring, and a dedicated embedding model for semantic representations - all within the same workflow, without writing any routing logic yourself.</p>"},{"location":"devnotes/designing-data-designer-why-sdg-is-a-systems-problem/#quality-as-a-first-class-stage","title":"Quality as a first-class stage","text":"<p>In Data Designer, quality control isn't a post-processing step you bolt on after generation. Validators and LLM-as-judge evaluations are column types, just like generation columns. They participate in the same dependency graph, run in the same execution engine, and their outputs are available to downstream stages.</p> <p>This means you can define a pipeline where a judge evaluates generated records immediately after they're created, and a downstream expression column flags records below a quality threshold - all within a single workflow definition. Quality scores are part of the pipeline, not something you remember to compute afterwards.</p>"},{"location":"devnotes/designing-data-designer-why-sdg-is-a-systems-problem/#extensibility-via-plugins","title":"Extensibility via plugins","text":"<p>No framework can anticipate every use case. Data Designer's plugin system lets you define custom column generators that work alongside the built-in types. A plugin is a Python class that inherits from the base column generator, packages with a configuration schema, and registers itself through a standard entry point. Once installed, it's indistinguishable from a built-in column type - it participates in dependency resolution, batching, and parallel execution like everything else.</p> <p>This is how domain-specific functionality gets added without forking the framework. If your use case requires embedding-based deduplication with FAISS indices and cosine similarity thresholds, for instance, you can build it as a plugin and drop it into any pipeline that needs it.</p>"},{"location":"devnotes/designing-data-designer-why-sdg-is-a-systems-problem/#what-this-looks-like-in-practice","title":"What This Looks Like in Practice","text":"<p>These principles apply to any SDG use case. Whether you're generating reasoning traces for pretraining (as in our RQA dev note), multi-turn conversations for alignment tuning, labeled examples for text classification, product reviews for testing a recommendation system, or code-repair pairs for training a coding assistant - the same decomposition applies. You identify the stages, define the columns, declare the dependencies, and let the framework handle execution.</p> <p>To make one example concrete, consider a pipeline for generating training data for a retrieval model. The goal is to produce high-quality question-answer pairs grounded in a corpus of documents, with quality scoring. We choose this example because it exercises several stages and model types in a single workflow, but the pattern generalizes to any domain.</p> <p>In a single-prompt approach, you'd try to pack all of this into one call: \"Given this document, generate diverse QA pairs of varying complexity and only include high-quality ones.\" The model would do its best, but you'd have limited control over any individual aspect.</p> <p>With Data Designer, the same task decomposes into a pipeline of focused stages:</p> <pre><code>      Seed Documents         Seed dataset column ingests documents\n            \u2502                 from local files or HuggingFace\n            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Artifact Extraction    \u2502  LLM extracts key concepts, entities,\n\u2502                         \u2502  relationships from each document\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  QA Generation          \u2502  LLM generates questions &amp; answers grounded\n\u2502                         \u2502  in the extracted artifacts\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Quality Evaluation     \u2502  LLM judge scores each QA pair\n\u2502                         \u2502  on relevance, accuracy, clarity\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n            \u25bc\n      Final Dataset\n</code></pre> <p>Each box is a column. Each one can use a different model. Each one has a focused prompt or algorithm. And because they're declared as columns with explicit dependencies, the framework handles the execution order, the batching, and the parallelism.</p> <p>The critical insight - and the one that applies regardless of your use case - is that every stage is independently configurable, testable, and replaceable. Want to try a different model for quality evaluation? Swap the model alias on that column. Want to tighten quality thresholds? Adjust the judge column's scoring rubric. Want to add a new stage that generates hard negatives for contrastive learning? Add a column and declare its dependencies. The rest of the pipeline doesn't change.</p>"},{"location":"devnotes/designing-data-designer-why-sdg-is-a-systems-problem/#summary","title":"Summary","text":"<p>Synthetic data generation at scale is a systems problem, not just a prompting problem. The design principles behind Data Designer reflect this:</p> <ol> <li>Declarative over imperative - describe the dataset you want, not the execution plan</li> <li>Columns as composable units - each stage is self-contained, with dependencies resolved automatically via a DAG</li> <li>Multi-model by design - match the model to the task, with per-column model routing</li> <li>Quality as a first-class stage - validators and judges are part of the pipeline, not afterthoughts</li> <li>Extensibility via plugins - add domain-specific logic without forking the framework</li> </ol> <p>The result is a general-purpose framework where complex, multi-stage generation workflows - whether you're building retrieval training data, reasoning datasets, conversational corpora, or something we haven't imagined yet - are expressed as simple column declarations. The hard problems of orchestration, dependency resolution, batching, and error handling are solved once, in the framework, rather than reimplemented in every project.</p> <p>Key Resources:</p> <ol> <li>NeMo Data Designer on GitHub</li> <li>Data Designer Documentation</li> <li>RQA Dev Note: Graduate-Level Science Reasoning Data</li> </ol> <p>Want to learn more about NeMo Data Designer? Check out our documentation and start building your own synthetic data pipelines today.</p>"},{"location":"devnotes/graduate-level-science-reasoning-data-with-nemo-data-designer/","title":"Graduate-Level Science Reasoning Data with NeMo Data Designer","text":"<p>Using NeMo Data Designer, we created the RQA (Reasoning Question-Answer) dataset: a massive collection of graduate-level, reasoning-heavy science samples designed to push the boundaries of model performance.</p> <p>Inference-time reasoning has transformed LLM capabilities, boosting performance in difficult domains like math and science. While reasoning is introduced in the post-training phase using Reinforcement Learning (RL), it builds on patterns that the model has seen throughout pretraining. In fact, research from NVIDIA has shown that front-loading examples of reasoning into the pretraining phase can have a positive, compounding impact on the quality of the final model. When training Nemotron 3 Nano, our goal was to introduce rich and diverse examples of reasoning directly into pretraining, laying the groundwork for reasoning RL in post-training.</p> <p>Using NeMo Data Designer, we created the RQA (Reasoning Question-Answer) dataset: a massive collection of graduate-level, reasoning-heavy science samples designed to push the boundaries of model performance. Each sample contains a question, a trace from a reasoning LLM attempting to answer that question, and the final resulting answer. As we\u2019ll show in the results, introducing RQA into pretraining didn\u2019t just result in stronger scientific reasoning - it improved math and coding performance as well.</p> <p>This blog post walks you through how we built it, and how you can adapt our approach for your own reasoning-intensive datasets.</p> <p></p>"},{"location":"devnotes/graduate-level-science-reasoning-data-with-nemo-data-designer/#step-1-curating-high-quality-science-seeds-from-essential-web","title":"Step 1: Curating High-Quality Science Seeds from Essential-Web","text":"<p>For our reasoning dataset, we knew that both quality and diversity were critical. We wanted to show the model examples of reasoning through difficult scientific problems, and we wanted to make sure that those problems covered as wide a range of scientific domains as possible. Using seed passages from web text was an obvious choice, because it allowed us to use the seed data to control both quality and diversity.</p> <p>We started with Essential-Web, a Common Crawl (web text) dataset where each document has been labelled with respect to both quality and subject. For instance, documents are labelled with an estimated Education Level, where Graduate Level indicates that the text \u201crequires graduate-level education or domain expertise. Assumes deep background knowledge and specialized training to comprehend\u201d. These labels let us rapidly filter down the documents to the highest-quality seeds for our scientific reasoning dataset.</p> <p>Starting from the STEM subset of Essential-Web, we filtered to documents that were:</p> <ol> <li>Undergraduate-to-graduate education level</li> <li>Advanced reasoning depth</li> <li>High technical correctness</li> <li>Advanced Bloom taxonomy levels for both cognitive processes (Analyze, Evaluate or Create) and knowledge domains (Conceptual, Procedural or Metacognitive)</li> <li>In the English language and over 1000 characters.</li> </ol> <p>The resulting subset consisted of roughly 14 million documents, mostly academic. Since many of the documents were very long, we extracted random chunks of \\&lt;4096 characters in length.</p> <p>Essential-AI also labelled the documents according to the Free Decimal Correspondence (FDC) code, a public-domain analogue of the Dewey Decimal system. Using the FDC code, we could see that the topics weren\u2019t equally balanced across scientific domains; for instance, Medicine &amp; Health was heavily over-represented. Since we planned to generate \\&lt;14 million samples in total, we aimed to capture as broad a range of topics as possible in the subset of seeds we used.</p> <p>To arrive at a smaller set of seed documents balanced by topic, we used a hierarchical round-robin approach. First, we rotated between selecting seed documents across 8 major domains (Biology, Chemistry, Computer Science, Engineering, Math, Medicine/Health, Physics, and Other). Within each high-level domain, we further rotated between seed documents based on their 3-digit FDC codes; for instance, given a Physics sample with code 535 (Light), the next Physics sample might be from code 536 (Heat), then 537 (Electricity) and so on, ensuring that no single subdomain dominates. We continued the round robin selection at the first and second decimal place of the FDC code, where they existed.</p> <p>We tested approaches using both the first 4.5 million and the first 9 million seeds according to the round-robin approach described above.</p>"},{"location":"devnotes/graduate-level-science-reasoning-data-with-nemo-data-designer/#step-2-generating-challenging-questions","title":"Step 2: Generating Challenging Questions","text":"<p>With our seed documents ready, we moved to NeMo Data Designer to design the actual dataset. While the seed documents ground our dataset in the types of advanced scientific topics we\u2019re interested in, they don\u2019t typically show the active process of thinking through a difficult scientific problem; instead, scientific papers usually show the polished end result of advanced reasoning. This is where LLMs come in.</p> <p>We first needed examples of the type of tough questions that Nemotron might be asked by a user in the real world. To do this, we used Data Designer to prompt a reasoning-enabled LLM to generate a graduate-level question inspired by each seed passage:</p> <pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\nQUESTION_PROMPT = \"\"\"\nTask: Generate a highly challenging, graduate-level reasoning question\ninspired by the following passage.\n\nFollow these instructions:\n1. The text serves only as inspiration for the question. You *must not*\n   reference the text directly in any way.\n2. The question should be appropriate for an advanced graduate-level exam\n   in a course specialized in this topic.\n3. Ensure that the question requires higher-order reasoning beyond simple\n   recall, such as mathematical reasoning, quantitative analysis, or synthesis.\n4. Tag the question with \"Question:\".\n\nText:\n{{ seed_passage }}\n\nQuestion: [question]\n\"\"\"\n\n# Configure the workflow with a reasoning-enabled model\nconfig = dd.DataDesignerConfigBuilder(model_configs=[\n    dd.ModelConfig(\n        alias=\"reasoning-model\",\n        model=\"qwen/qwen3-235b-a22b\",\n        provider=\"nvidia\",\n    ),\n])\n\nconfig.with_seed_dataset(\n    dd.LocalFileSeedSource(path=\"path/to/seed_data.parquet\"),\n    sampling_strategy=dd.SamplingStrategy.SHUFFLE,\n)\n\nconfig.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"question\",\n        prompt=QUESTION_PROMPT,\n        model_alias=\"reasoning-model\",\n    )\n)\n</code></pre> <p>Note that our prompt emphasizes that the question shouldn\u2019t reference the source passage. We want questions that stand on their own, without including the source passage itself; since these are passages from Common Crawl, we can expect that they appear in the pretraining data already, and our focus here is on generating new tokens.</p>"},{"location":"devnotes/graduate-level-science-reasoning-data-with-nemo-data-designer/#step-3-generating-high-quality-answers-with-reasoning-traces","title":"Step 3: Generating High-Quality Answers with Reasoning Traces","text":"<p>If you\u2019ve ever tried to read a teacher\u2019s answer key before, you know that sometimes the person who wrote the question isn\u2019t always the best at explaining how to answer it. In the real world, reasoning involves a lot of what-ifs, dead ends and backtracking - the types of behavior we can only get from a model when it has never seen the question before. This is why we chose to decouple answer generation from question generation, ensuring that the model doesn\u2019t have any context about how the question was generated or the source passage itself when it attempts to answer it.</p> <p>Below, we prompt the LLM directly with the questions we generated above, then capture the resulting reasoning trace and final answer for our RQA samples.</p> <pre><code>config.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"answer\",\n        prompt=\"{{ question }}\",  # Present just the question\n        model_alias=\"reasoning-model\",\n        extract_reasoning_content=True,  # Extract reasoning into separate column\n    )\n)\n\n# Combine question, reasoning trace, and answer into final sample\nconfig.add_column(\n    dd.ExpressionColumnConfig(\n        name=\"rqa_sample\",\n        expr=\"{{ question }}\\n\\n{{ answer__reasoning_content }}\\n\\n{{ answer }}\",\n    )\n)\n</code></pre> <p>In the resulting dataset, we see the following new columns concatenated to the seed data columns:</p> <ul> <li><code>question</code></li> <li><code>answer</code></li> <li><code>answer__reasoning_content</code></li> <li><code>rqa_sample</code></li> </ul> <p>The <code>question</code> and <code>answer</code> columns are the final result of the calls to our reasoning LLM, while <code>answer__reasoning_content</code> is the reasoning trace generated by the LLM when generating the answer. Typically we discard the reasoning trace, but here it\u2019s critical: we want to include the model\u2019s chain-of-thought in the final training data, distilling strong priors in Nemotron Nano 3 about how to work through a challenging problem. The final column, <code>rqa_sample</code>, uses Jinja2 syntax to combine all three fields into the final sample for training.</p> <p>We chose to use the same frontier reasoning model to answer the questions as we used to generate them - leveraging the model\u2019s advanced capabilities both for formulating a tough, well-formed question and for puzzling through the answer. But with Data Designer, this choice is up to you - you can mix-and-match models any way you like.</p>"},{"location":"devnotes/graduate-level-science-reasoning-data-with-nemo-data-designer/#results-measurable-improvements-in-stem-reasoning","title":"Results: Measurable Improvements in STEM Reasoning","text":"<p>To evaluate the impact of the RQA data, we ran continued pretraining experiments on an internal checkpoint of Nemotron-H 8B. Nemotron-H used a two-phase pretraining approach (you can read more about it in our white paper here). We intervened at the Phase 2 training stage, comparing the result of replacing either 4% or 8% of the existing data blend with RQA samples (taking weight from high-quality Common Crawl data). We ran the intervention for 18k steps, between a checkpoint at 140k steps and a checkpoint at 158k steps.</p> Data Blend Validation Loss (\u2193) MMLU-Pro (with CoT, \u2191) Math 500 (with CoT, \u2191) GSM8K (with CoT, \u2191) Humaneval+ (\u2191) MBPP+ (\u2191) Baseline data blend (140k steps) 1.309 36.99 - 79.98 38.14 48.68 Baseline data blend (158k steps) 1.258 43.39 71.00 81.96 42.71 53.31 with RQA (4.5m @4%, 158k steps) 1.256 44.31 73.40 82.79 47.20 54.84 with RQA (9m @8%, 158k steps) 1.255 45.80 73.40 84.76 45.61 53.80 <p>One of the most surprising (and exciting!) results was that RQA didn\u2019t just improve performance on tests of scientific reasoning like MMLU-Pro - it also improved performance on benchmarks associated with math reasoning (Math 500, GSM8K) and coding capabilities (Humaneval+, MBPP+). This shows how early introduction of advanced reasoning capabilities can produce robust improvements across different domains.</p> <p>You can check out the RQA dataset we generated for Nemotron 3 Nano here.</p>"},{"location":"devnotes/graduate-level-science-reasoning-data-with-nemo-data-designer/#get-started-with-data-designer","title":"Get Started with Data Designer","text":"<p>Apart from the seed data, the entire pipeline is reproducible using NeMo Data Designer. Note how Data Designer handles complex data formatting with ease, leveraging Jinja2 templates in prompt generation and built-in logic to extract reasoning traces from model responses.</p> <pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\n# Configure your model\nmodel_configs = [\n    dd.ModelConfig(\n        alias=\"reasoning-model\",\n        model=\"qwen/qwen3-235b-a22b\",\n        provider=\"nvidia\",\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            max_tokens=8192,\n            timeout=300,  # 5 minute timeout for long reasoning chains\n        ),\n    ),\n]\n\n# Build the workflow\nconfig = dd.DataDesignerConfigBuilder(model_configs=model_configs)\nconfig.with_seed_dataset(\n    dd.LocalFileSeedSource(path=\"path/to/your_seed_data.parquet\"),\n    sampling_strategy=dd.SamplingStrategy.SHUFFLE,\n)\n\n# Generate questions\nconfig.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"question\",\n        prompt=QUESTION_PROMPT,\n        model_alias=\"reasoning-model\",\n    )\n)\n\n# Generate answers with reasoning trace\nconfig.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"answer\",\n        prompt=\"{{ question }}\",\n        model_alias=\"reasoning-model\",\n        extract_reasoning_content=True,  # Extract reasoning into separate column\n    )\n)\n\n# Combine into final sample\nconfig.add_column(\n    dd.ExpressionColumnConfig(\n        name=\"rqa_sample\",\n        expr=\"{{ question }}\\n\\n{{ answer__reasoning_content }}\\n\\n{{ answer }}\",\n    )\n)\n\n# Run generation and save to disk\ndata_designer = DataDesigner()\nresult = data_designer.create(\n    config_builder=config,\n    num_records=N_RECORDS,\n    dataset_name=\"rqa_dataset\",\n)\n</code></pre>"},{"location":"devnotes/graduate-level-science-reasoning-data-with-nemo-data-designer/#summary","title":"Summary","text":"<p>The RQA dataset demonstrates that targeted synthetic data generation can meaningfully improve advanced reasoning capabilities. By:</p> <ol> <li>Curating high-quality scientific seed data</li> <li>Generating challenging, standalone questions from those seeds</li> <li>Using powerful reasoning models to reason through how to answer those questions</li> </ol> <p>\u2026 we created a dataset that pushes models toward graduate-level science reasoning - and generalizable improvements on math and code as well.</p> <p>Key Resources:</p> <ol> <li>NeMo Data Designer on GitHub</li> <li>Nemotron 3 Nano Technical Report</li> <li>Essential-Web</li> </ol> <p>The workflow is fully configurable and extensible: swap in your own seed data, adjust the prompts, or add custom validators. Data Designer makes it possible to iterate rapidly on synthetic data pipelines, turning what used to be months of manual annotation into hours of programmable generation.</p> <p>Want to learn more about NeMo Data Designer? Check out our documentation and start building your own high-fidelity synthetic datasets today.</p>"},{"location":"notebook_source/","title":"\ud83d\udcd3 Notebooks in <code>.py</code> Format","text":"<p>In this folder you can find all our tutorial notebooks in <code>.py</code> format. They can be converted to actual Jupyter notebooks by typing</p> <pre><code>make convert-execute-notebooks\n</code></pre> <p>from the root of the repository. This will not only convert but also execute all of the notebooks -- for that to work, make sure you went through our Quick Start and have API keys set. A new folder <code>docs/notebooks</code> will be created, including <code>README.md</code> and <code>pyproject.toml</code> files.</p> <p>Alternatively, you can use Jupytext directly</p> <pre><code>uv run --group notebooks --group docs jupytext --to ipynb *.py\n</code></pre>"},{"location":"notebook_source/#converting-jupyter-notebooks-to-py","title":"\ud83d\udd04 Converting Jupyter notebooks to <code>.py</code>","text":"<p>If you want to contribute with your own notebook, you can use the following command to generate <code>.py</code> files in the same format as the ones in this folder:</p> <pre><code>uv run jupytext --to py [notebook-name].ipynb -o [notebook-name].py\n</code></pre>"},{"location":"notebook_source/1-the-basics/","title":"1 the basics","text":"In\u00a0[\u00a0]: Copied! <pre>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n</pre> import data_designer.config as dd from data_designer.interface import DataDesigner In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    dd.ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     dd.ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=dd.ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[\u00a0]: Copied! <pre>config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre>config_builder.info.display(\"samplers\")\n</pre> config_builder.info.display(\"samplers\") <p>Let's start designing our product review dataset by adding product category and subcategory columns.</p> In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"product_category\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\n                \"Electronics\",\n                \"Clothing\",\n                \"Home &amp; Kitchen\",\n                \"Books\",\n                \"Home Office\",\n            ],\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"product_subcategory\",\n        sampler_type=dd.SamplerType.SUBCATEGORY,\n        params=dd.SubcategorySamplerParams(\n            category=\"product_category\",\n            values={\n                \"Electronics\": [\n                    \"Smartphones\",\n                    \"Laptops\",\n                    \"Headphones\",\n                    \"Cameras\",\n                    \"Accessories\",\n                ],\n                \"Clothing\": [\n                    \"Men's Clothing\",\n                    \"Women's Clothing\",\n                    \"Winter Coats\",\n                    \"Activewear\",\n                    \"Accessories\",\n                ],\n                \"Home &amp; Kitchen\": [\n                    \"Appliances\",\n                    \"Cookware\",\n                    \"Furniture\",\n                    \"Decor\",\n                    \"Organization\",\n                ],\n                \"Books\": [\n                    \"Fiction\",\n                    \"Non-Fiction\",\n                    \"Self-Help\",\n                    \"Textbooks\",\n                    \"Classics\",\n                ],\n                \"Home Office\": [\n                    \"Desks\",\n                    \"Chairs\",\n                    \"Storage\",\n                    \"Office Supplies\",\n                    \"Lighting\",\n                ],\n            },\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"target_age_range\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),\n    )\n)\n\n# Optionally validate that the columns are configured correctly.\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     dd.SamplerColumnConfig(         name=\"product_category\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(             values=[                 \"Electronics\",                 \"Clothing\",                 \"Home &amp; Kitchen\",                 \"Books\",                 \"Home Office\",             ],         ),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"product_subcategory\",         sampler_type=dd.SamplerType.SUBCATEGORY,         params=dd.SubcategorySamplerParams(             category=\"product_category\",             values={                 \"Electronics\": [                     \"Smartphones\",                     \"Laptops\",                     \"Headphones\",                     \"Cameras\",                     \"Accessories\",                 ],                 \"Clothing\": [                     \"Men's Clothing\",                     \"Women's Clothing\",                     \"Winter Coats\",                     \"Activewear\",                     \"Accessories\",                 ],                 \"Home &amp; Kitchen\": [                     \"Appliances\",                     \"Cookware\",                     \"Furniture\",                     \"Decor\",                     \"Organization\",                 ],                 \"Books\": [                     \"Fiction\",                     \"Non-Fiction\",                     \"Self-Help\",                     \"Textbooks\",                     \"Classics\",                 ],                 \"Home Office\": [                     \"Desks\",                     \"Chairs\",                     \"Storage\",                     \"Office Supplies\",                     \"Lighting\",                 ],             },         ),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"target_age_range\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),     ) )  # Optionally validate that the columns are configured correctly. data_designer.validate(config_builder) <p>Next, let's add samplers to generate data related to the customer and their review.</p> In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=dd.SamplerType.PERSON_FROM_FAKER,\n        params=dd.PersonFromFakerSamplerParams(age_range=[18, 70], locale=\"en_US\"),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"number_of_stars\",\n        sampler_type=dd.SamplerType.UNIFORM,\n        params=dd.UniformSamplerParams(low=1, high=5),\n        convert_to=\"int\",  # Convert the sampled float to an integer.\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"review_style\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],\n            weights=[1, 2, 2, 1],\n        ),\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     dd.SamplerColumnConfig(         name=\"customer\",         sampler_type=dd.SamplerType.PERSON_FROM_FAKER,         params=dd.PersonFromFakerSamplerParams(age_range=[18, 70], locale=\"en_US\"),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"number_of_stars\",         sampler_type=dd.SamplerType.UNIFORM,         params=dd.UniformSamplerParams(low=1, high=5),         convert_to=\"int\",  # Convert the sampled float to an integer.     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"review_style\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(             values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],             weights=[1, 2, 2, 1],         ),     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"product_name\",\n        prompt=(\n            \"You are a helpful assistant that generates product names. DO NOT add quotes around the product name.\\n\\n\"\n            \"Come up with a creative product name for a product in the '{{ product_category }}' category, focusing \"\n            \"on products related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"\n            \"{{ target_age_range }} years old. Respond with only the product name, no other text.\"\n        ),\n        model_alias=MODEL_ALIAS,\n    )\n)\n\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"customer_review\",\n        prompt=(\n            \"You are a customer named {{ customer.first_name }} from {{ customer.city }}, {{ customer.state }}. \"\n            \"You are {{ customer.age }} years old and recently purchased a product called {{ product_name }}. \"\n            \"Write a review of this product, which you gave a rating of {{ number_of_stars }} stars. \"\n            \"The style of the review should be '{{ review_style }}'. \"\n            \"Respond with only the review, no other text.\"\n        ),\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     dd.LLMTextColumnConfig(         name=\"product_name\",         prompt=(             \"You are a helpful assistant that generates product names. DO NOT add quotes around the product name.\\n\\n\"             \"Come up with a creative product name for a product in the '{{ product_category }}' category, focusing \"             \"on products related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"             \"{{ target_age_range }} years old. Respond with only the product name, no other text.\"         ),         model_alias=MODEL_ALIAS,     ) )  config_builder.add_column(     dd.LLMTextColumnConfig(         name=\"customer_review\",         prompt=(             \"You are a customer named {{ customer.first_name }} from {{ customer.city }}, {{ customer.state }}. \"             \"You are {{ customer.age }} years old and recently purchased a product called {{ product_name }}. \"             \"Write a review of this product, which you gave a rating of {{ number_of_stars }} stars. \"             \"The style of the review should be '{{ review_style }}'. \"             \"Respond with only the review, no other text.\"         ),         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-1\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-1\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"notebook_source/1-the-basics/#data-designer-tutorial-the-basics","title":"\ud83c\udfa8 Data Designer Tutorial: The Basics\u00b6","text":""},{"location":"notebook_source/1-the-basics/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>This notebook demonstrates the basics of Data Designer by generating a simple product review dataset.</p>"},{"location":"notebook_source/1-the-basics/#import-data-designer","title":"\ud83d\udce6 Import Data Designer\u00b6","text":"<ul> <li><p><code>data_designer.config</code> provides access to the configuration API.</p> </li> <li><p><code>DataDesigner</code> is the main interface for data generation.</p> </li> </ul>"},{"location":"notebook_source/1-the-basics/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebook_source/1-the-basics/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebook_source/1-the-basics/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebook_source/1-the-basics/#getting-started-with-sampler-columns","title":"\ud83c\udfb2 Getting started with sampler columns\u00b6","text":"<ul> <li><p>Sampler columns offer non-LLM based generation of synthetic data.</p> </li> <li><p>They are particularly useful for steering the diversity of the generated data, as we demonstrate below.</p> </li> </ul> <p>You can view available samplers using the config builder's <code>info</code> property:</p>"},{"location":"notebook_source/1-the-basics/#llm-generated-columns","title":"\ud83e\udd9c LLM-generated columns\u00b6","text":"<ul> <li><p>The real power of Data Designer comes from leveraging LLMs to generate text, code, and structured data.</p> </li> <li><p>When prompting the LLM, we can use Jinja templating to reference other columns in the dataset.</p> </li> <li><p>As we see below, nested json fields can be accessed using dot notation.</p> </li> </ul>"},{"location":"notebook_source/1-the-basics/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebook_source/1-the-basics/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebook_source/1-the-basics/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebook_source/1-the-basics/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Now that you've seen the basics of Data Designer, check out the following notebooks to learn more about:</p> <ul> <li><p>Structured outputs and jinja expressions</p> </li> <li><p>Seeding synthetic data generation with an external dataset</p> </li> <li><p>Providing images as context</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/","title":"2 structured outputs and jinja expressions","text":"In\u00a0[\u00a0]: Copied! <pre>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n</pre> import data_designer.config as dd from data_designer.interface import DataDesigner In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    dd.ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     dd.ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=dd.ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[\u00a0]: Copied! <pre>config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre>from decimal import Decimal\nfrom typing import Literal\n\nfrom pydantic import BaseModel, Field\n\n\n# We define a Product schema so that the name, description, and price are generated\n# in one go, with the types and constraints specified.\nclass Product(BaseModel):\n    name: str = Field(description=\"The name of the product\")\n    description: str = Field(description=\"A description of the product\")\n    price: Decimal = Field(description=\"The price of the product\", ge=10, le=1000, decimal_places=2)\n\n\nclass ProductReview(BaseModel):\n    rating: int = Field(description=\"The rating of the product\", ge=1, le=5)\n    customer_mood: Literal[\"irritated\", \"mad\", \"happy\", \"neutral\", \"excited\"] = Field(\n        description=\"The mood of the customer\"\n    )\n    review: str = Field(description=\"A review of the product\")\n</pre> from decimal import Decimal from typing import Literal  from pydantic import BaseModel, Field   # We define a Product schema so that the name, description, and price are generated # in one go, with the types and constraints specified. class Product(BaseModel):     name: str = Field(description=\"The name of the product\")     description: str = Field(description=\"A description of the product\")     price: Decimal = Field(description=\"The price of the product\", ge=10, le=1000, decimal_places=2)   class ProductReview(BaseModel):     rating: int = Field(description=\"The rating of the product\", ge=1, le=5)     customer_mood: Literal[\"irritated\", \"mad\", \"happy\", \"neutral\", \"excited\"] = Field(         description=\"The mood of the customer\"     )     review: str = Field(description=\"A review of the product\") <p>Next, let's design our product review dataset using a few more tricks compared to the previous notebook.</p> In\u00a0[\u00a0]: Copied! <pre># Since we often only want a few attributes from Person objects, we can\n# set drop=True in the column config to drop the column from the final dataset.\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=dd.SamplerType.PERSON_FROM_FAKER,\n        params=dd.PersonFromFakerSamplerParams(),\n        drop=True,\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"product_category\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\n                \"Electronics\",\n                \"Clothing\",\n                \"Home &amp; Kitchen\",\n                \"Books\",\n                \"Home Office\",\n            ],\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"product_subcategory\",\n        sampler_type=dd.SamplerType.SUBCATEGORY,\n        params=dd.SubcategorySamplerParams(\n            category=\"product_category\",\n            values={\n                \"Electronics\": [\n                    \"Smartphones\",\n                    \"Laptops\",\n                    \"Headphones\",\n                    \"Cameras\",\n                    \"Accessories\",\n                ],\n                \"Clothing\": [\n                    \"Men's Clothing\",\n                    \"Women's Clothing\",\n                    \"Winter Coats\",\n                    \"Activewear\",\n                    \"Accessories\",\n                ],\n                \"Home &amp; Kitchen\": [\n                    \"Appliances\",\n                    \"Cookware\",\n                    \"Furniture\",\n                    \"Decor\",\n                    \"Organization\",\n                ],\n                \"Books\": [\n                    \"Fiction\",\n                    \"Non-Fiction\",\n                    \"Self-Help\",\n                    \"Textbooks\",\n                    \"Classics\",\n                ],\n                \"Home Office\": [\n                    \"Desks\",\n                    \"Chairs\",\n                    \"Storage\",\n                    \"Office Supplies\",\n                    \"Lighting\",\n                ],\n            },\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"target_age_range\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),\n    )\n)\n\n# Sampler columns support conditional params, which are used if the condition is met.\n# In this example, we set the review style to rambling if the target age range is 18-25.\n# Note conditional parameters are only supported for Sampler column types.\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"review_style\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],\n            weights=[1, 2, 2, 1],\n        ),\n        conditional_params={\n            \"target_age_range == '18-25'\": dd.CategorySamplerParams(values=[\"rambling\"]),\n        },\n    )\n)\n\n# Optionally validate that the columns are configured correctly.\ndata_designer.validate(config_builder)\n</pre> # Since we often only want a few attributes from Person objects, we can # set drop=True in the column config to drop the column from the final dataset. config_builder.add_column(     dd.SamplerColumnConfig(         name=\"customer\",         sampler_type=dd.SamplerType.PERSON_FROM_FAKER,         params=dd.PersonFromFakerSamplerParams(),         drop=True,     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"product_category\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(             values=[                 \"Electronics\",                 \"Clothing\",                 \"Home &amp; Kitchen\",                 \"Books\",                 \"Home Office\",             ],         ),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"product_subcategory\",         sampler_type=dd.SamplerType.SUBCATEGORY,         params=dd.SubcategorySamplerParams(             category=\"product_category\",             values={                 \"Electronics\": [                     \"Smartphones\",                     \"Laptops\",                     \"Headphones\",                     \"Cameras\",                     \"Accessories\",                 ],                 \"Clothing\": [                     \"Men's Clothing\",                     \"Women's Clothing\",                     \"Winter Coats\",                     \"Activewear\",                     \"Accessories\",                 ],                 \"Home &amp; Kitchen\": [                     \"Appliances\",                     \"Cookware\",                     \"Furniture\",                     \"Decor\",                     \"Organization\",                 ],                 \"Books\": [                     \"Fiction\",                     \"Non-Fiction\",                     \"Self-Help\",                     \"Textbooks\",                     \"Classics\",                 ],                 \"Home Office\": [                     \"Desks\",                     \"Chairs\",                     \"Storage\",                     \"Office Supplies\",                     \"Lighting\",                 ],             },         ),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"target_age_range\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),     ) )  # Sampler columns support conditional params, which are used if the condition is met. # In this example, we set the review style to rambling if the target age range is 18-25. # Note conditional parameters are only supported for Sampler column types. config_builder.add_column(     dd.SamplerColumnConfig(         name=\"review_style\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(             values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],             weights=[1, 2, 2, 1],         ),         conditional_params={             \"target_age_range == '18-25'\": dd.CategorySamplerParams(values=[\"rambling\"]),         },     ) )  # Optionally validate that the columns are configured correctly. data_designer.validate(config_builder) <p>Next, we will use more advanced Jinja expressions to create new columns.</p> <p>Jinja expressions let you:</p> <ul> <li><p>Access nested attributes: <code>{{ customer.first_name }}</code></p> </li> <li><p>Combine values: <code>{{ customer.first_name }} {{ customer.last_name }}</code></p> </li> <li><p>Use conditional logic: <code>{% if condition %}...{% endif %}</code></p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre># We can create new columns using Jinja expressions that reference\n# existing columns, including attributes of nested objects.\nconfig_builder.add_column(\n    dd.ExpressionColumnConfig(name=\"customer_name\", expr=\"{{ customer.first_name }} {{ customer.last_name }}\")\n)\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"customer_age\", expr=\"{{ customer.age }}\"))\n\nconfig_builder.add_column(\n    dd.LLMStructuredColumnConfig(\n        name=\"product\",\n        prompt=(\n            \"Create a product in the '{{ product_category }}' category, focusing on products  \"\n            \"related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"\n            \"{{ target_age_range }} years old. The product should be priced between $10 and $1000.\"\n        ),\n        output_format=Product,\n        model_alias=MODEL_ALIAS,\n    )\n)\n\n# We can even use if/else logic in our Jinja expressions to create more complex prompt patterns.\nconfig_builder.add_column(\n    dd.LLMStructuredColumnConfig(\n        name=\"customer_review\",\n        prompt=(\n            \"Your task is to write a review for the following product:\\n\\n\"\n            \"Product Name: {{ product.name }}\\n\"\n            \"Product Description: {{ product.description }}\\n\"\n            \"Price: {{ product.price }}\\n\\n\"\n            \"Imagine your name is {{ customer_name }} and you are from {{ customer.city }}, {{ customer.state }}. \"\n            \"Write the review in a style that is '{{ review_style }}'.\"\n            \"{% if target_age_range == '18-25' %}\"\n            \"Make sure the review is more informal and conversational.\\n\"\n            \"{% else %}\"\n            \"Make sure the review is more formal and structured.\\n\"\n            \"{% endif %}\"\n            \"The review field should contain only the review, no other text.\"\n        ),\n        output_format=ProductReview,\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> # We can create new columns using Jinja expressions that reference # existing columns, including attributes of nested objects. config_builder.add_column(     dd.ExpressionColumnConfig(name=\"customer_name\", expr=\"{{ customer.first_name }} {{ customer.last_name }}\") )  config_builder.add_column(dd.ExpressionColumnConfig(name=\"customer_age\", expr=\"{{ customer.age }}\"))  config_builder.add_column(     dd.LLMStructuredColumnConfig(         name=\"product\",         prompt=(             \"Create a product in the '{{ product_category }}' category, focusing on products  \"             \"related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"             \"{{ target_age_range }} years old. The product should be priced between $10 and $1000.\"         ),         output_format=Product,         model_alias=MODEL_ALIAS,     ) )  # We can even use if/else logic in our Jinja expressions to create more complex prompt patterns. config_builder.add_column(     dd.LLMStructuredColumnConfig(         name=\"customer_review\",         prompt=(             \"Your task is to write a review for the following product:\\n\\n\"             \"Product Name: {{ product.name }}\\n\"             \"Product Description: {{ product.description }}\\n\"             \"Price: {{ product.price }}\\n\\n\"             \"Imagine your name is {{ customer_name }} and you are from {{ customer.city }}, {{ customer.state }}. \"             \"Write the review in a style that is '{{ review_style }}'.\"             \"{% if target_age_range == '18-25' %}\"             \"Make sure the review is more informal and conversational.\\n\"             \"{% else %}\"             \"Make sure the review is more formal and structured.\\n\"             \"{% endif %}\"             \"The review field should contain only the review, no other text.\"         ),         output_format=ProductReview,         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-2\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-2\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#data-designer-tutorial-structured-outputs-and-jinja-expressions","title":"\ud83c\udfa8 Data Designer Tutorial: Structured Outputs and Jinja Expressions\u00b6","text":""},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>In this notebook, we will continue our exploration of Data Designer, demonstrating more advanced data generation using structured outputs and Jinja expressions.</p> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#import-data-designer","title":"\ud83d\udce6 Import Data Designer\u00b6","text":"<ul> <li><p><code>data_designer.config</code> provides access to the configuration API.</p> </li> <li><p><code>DataDesigner</code> is the main interface for data generation.</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object that is used to interface with the library.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#designing-our-data","title":"\ud83e\uddd1\u200d\ud83c\udfa8 Designing our data\u00b6","text":"<ul> <li><p>We will again create a product review dataset, but this time we will use structured outputs and Jinja expressions.</p> </li> <li><p>Structured outputs let you specify the exact schema of the data you want to generate.</p> </li> <li><p>Data Designer supports schemas specified using either json schema or Pydantic data models (recommended).</p> </li> </ul> <p>We'll define our structured outputs using Pydantic data models</p> <p>\ud83d\udca1 Why Pydantic?</p> <ul> <li><p>Pydantic models provide better IDE support and type validation.</p> </li> <li><p>They are more Pythonic than raw JSON schemas.</p> </li> <li><p>They integrate seamlessly with Data Designer's structured output system.</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebook_source/2-structured-outputs-and-jinja-expressions/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Check out the following notebook to learn more about:</p> <ul> <li><p>Seeding synthetic data generation with an external dataset</p> </li> <li><p>Providing images as context</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/","title":"3 seeding with a dataset","text":"In\u00a0[\u00a0]: Copied! <pre>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n</pre> import data_designer.config as dd from data_designer.interface import DataDesigner In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    dd.ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     dd.ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=dd.ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[\u00a0]: Copied! <pre>config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre># Download sample dataset from Github\nimport urllib.request\n\nurl = \"https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/refs/heads/main/nemo/NeMo-Data-Designer/data/gretelai_symptom_to_diagnosis.csv\"\nlocal_filename, _ = urllib.request.urlretrieve(url, \"gretelai_symptom_to_diagnosis.csv\")\n\n# Seed datasets are passed as reference objects to the config builder.\nseed_source = dd.LocalFileSeedSource(path=local_filename)\n\nconfig_builder.with_seed_dataset(seed_source)\n</pre> # Download sample dataset from Github import urllib.request  url = \"https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/refs/heads/main/nemo/NeMo-Data-Designer/data/gretelai_symptom_to_diagnosis.csv\" local_filename, _ = urllib.request.urlretrieve(url, \"gretelai_symptom_to_diagnosis.csv\")  # Seed datasets are passed as reference objects to the config builder. seed_source = dd.LocalFileSeedSource(path=local_filename)  config_builder.with_seed_dataset(seed_source) In\u00a0[\u00a0]: Copied! <pre>config_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"patient_sampler\",\n        sampler_type=dd.SamplerType.PERSON_FROM_FAKER,\n        params=dd.PersonFromFakerSamplerParams(),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"doctor_sampler\",\n        sampler_type=dd.SamplerType.PERSON_FROM_FAKER,\n        params=dd.PersonFromFakerSamplerParams(),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"patient_id\",\n        sampler_type=dd.SamplerType.UUID,\n        params=dd.UUIDSamplerParams(\n            prefix=\"PT-\",\n            short_form=True,\n            uppercase=True,\n        ),\n    )\n)\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"first_name\", expr=\"{{ patient_sampler.first_name }}\"))\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"last_name\", expr=\"{{ patient_sampler.last_name }}\"))\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"dob\", expr=\"{{ patient_sampler.birth_date }}\"))\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"symptom_onset_date\",\n        sampler_type=dd.SamplerType.DATETIME,\n        params=dd.DatetimeSamplerParams(start=\"2024-01-01\", end=\"2024-12-31\"),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"date_of_visit\",\n        sampler_type=dd.SamplerType.TIMEDELTA,\n        params=dd.TimeDeltaSamplerParams(dt_min=1, dt_max=30, reference_column_name=\"symptom_onset_date\"),\n    )\n)\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"physician\", expr=\"Dr. {{ doctor_sampler.last_name }}\"))\n\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"physician_notes\",\n        prompt=\"\"\"\\\nYou are a primary-care physician who just had an appointment with {{ first_name }} {{ last_name }},\nwho has been struggling with symptoms from {{ diagnosis }} since {{ symptom_onset_date }}.\nThe date of today's visit is {{ date_of_visit }}.\n\n{{ patient_summary }}\n\nWrite careful notes about your visit with {{ first_name }},\nas Dr. {{ doctor_sampler.first_name }} {{ doctor_sampler.last_name }}.\n\nFormat the notes as a busy doctor might.\nRespond with only the notes, no other text.\n\"\"\",\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     dd.SamplerColumnConfig(         name=\"patient_sampler\",         sampler_type=dd.SamplerType.PERSON_FROM_FAKER,         params=dd.PersonFromFakerSamplerParams(),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"doctor_sampler\",         sampler_type=dd.SamplerType.PERSON_FROM_FAKER,         params=dd.PersonFromFakerSamplerParams(),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"patient_id\",         sampler_type=dd.SamplerType.UUID,         params=dd.UUIDSamplerParams(             prefix=\"PT-\",             short_form=True,             uppercase=True,         ),     ) )  config_builder.add_column(dd.ExpressionColumnConfig(name=\"first_name\", expr=\"{{ patient_sampler.first_name }}\"))  config_builder.add_column(dd.ExpressionColumnConfig(name=\"last_name\", expr=\"{{ patient_sampler.last_name }}\"))  config_builder.add_column(dd.ExpressionColumnConfig(name=\"dob\", expr=\"{{ patient_sampler.birth_date }}\"))  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"symptom_onset_date\",         sampler_type=dd.SamplerType.DATETIME,         params=dd.DatetimeSamplerParams(start=\"2024-01-01\", end=\"2024-12-31\"),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"date_of_visit\",         sampler_type=dd.SamplerType.TIMEDELTA,         params=dd.TimeDeltaSamplerParams(dt_min=1, dt_max=30, reference_column_name=\"symptom_onset_date\"),     ) )  config_builder.add_column(dd.ExpressionColumnConfig(name=\"physician\", expr=\"Dr. {{ doctor_sampler.last_name }}\"))  config_builder.add_column(     dd.LLMTextColumnConfig(         name=\"physician_notes\",         prompt=\"\"\"\\ You are a primary-care physician who just had an appointment with {{ first_name }} {{ last_name }}, who has been struggling with symptoms from {{ diagnosis }} since {{ symptom_onset_date }}. The date of today's visit is {{ date_of_visit }}.  {{ patient_summary }}  Write careful notes about your visit with {{ first_name }}, as Dr. {{ doctor_sampler.first_name }} {{ doctor_sampler.last_name }}.  Format the notes as a busy doctor might. Respond with only the notes, no other text. \"\"\",         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-3\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-3\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"notebook_source/3-seeding-with-a-dataset/#data-designer-tutorial-seeding-synthetic-data-generation-with-an-external-dataset","title":"\ud83c\udfa8 Data Designer Tutorial: Seeding Synthetic Data Generation with an External Dataset\u00b6","text":""},{"location":"notebook_source/3-seeding-with-a-dataset/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>In this notebook, we will demonstrate how to seed synthetic data generation in Data Designer with an external dataset.</p> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"notebook_source/3-seeding-with-a-dataset/#import-data-designer","title":"\ud83d\udce6 Import Data Designer\u00b6","text":"<ul> <li><p><code>data_designer.config</code> provides access to the configuration API.</p> </li> <li><p><code>DataDesigner</code> is the main interface for data generation.</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#prepare-a-seed-dataset","title":"\ud83c\udfe5 Prepare a seed dataset\u00b6","text":"<ul> <li><p>For this notebook, we'll create a synthetic dataset of patient notes.</p> </li> <li><p>We will seed the generation process with a symptom-to-diagnosis dataset.</p> </li> <li><p>We already have the dataset downloaded in the data directory of this repository.</p> </li> </ul> <p>\ud83c\udf31 Why use a seed dataset?</p> <ul> <li><p>Seed datasets let you steer the generation process by providing context that is specific to your use case.</p> </li> <li><p>Seed datasets are also an excellent way to inject real-world diversity into your synthetic data.</p> </li> <li><p>During generation, prompt templates can reference any of the seed dataset fields.</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#designing-our-synthetic-patient-notes-dataset","title":"\ud83c\udfa8 Designing our synthetic patient notes dataset\u00b6","text":"<ul> <li>The prompt template can reference fields from our seed dataset:<ul> <li><code>{{ diagnosis }}</code> - the medical diagnosis from the seed data</li> <li><code>{{ patient_summary }}</code> - the symptom description from the seed data</li> </ul> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebook_source/3-seeding-with-a-dataset/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebook_source/3-seeding-with-a-dataset/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Check out the following notebook to learn more about:</p> <ul> <li>Providing images as context</li> </ul>"},{"location":"notebook_source/4-providing-images-as-context/","title":"4 providing images as context","text":"In\u00a0[\u00a0]: Copied! <pre># Standard library imports\nimport base64\nimport io\nimport uuid\n\n# Third-party imports\nimport pandas as pd\nimport rich\nfrom datasets import load_dataset\nfrom IPython.display import display\nfrom rich.panel import Panel\n\n# Data Designer imports\nimport data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n</pre> # Standard library imports import base64 import io import uuid  # Third-party imports import pandas as pd import rich from datasets import load_dataset from IPython.display import display from rich.panel import Panel  # Data Designer imports import data_designer.config as dd from data_designer.interface import DataDesigner In\u00a0[\u00a0]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[\u00a0]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\nmodel_configs = [\n    dd.ModelConfig(\n        alias=\"vision\",\n        model=\"meta/llama-4-scout-17b-16e-instruct\",\n        provider=MODEL_PROVIDER,\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=0.60,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  model_configs = [     dd.ModelConfig(         alias=\"vision\",         model=\"meta/llama-4-scout-17b-16e-instruct\",         provider=MODEL_PROVIDER,         inference_parameters=dd.ChatCompletionInferenceParams(             temperature=0.60,             top_p=0.95,             max_tokens=2048,         ),     ), ] In\u00a0[\u00a0]: Copied! <pre>config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[\u00a0]: Copied! <pre># Dataset processing configuration\nIMG_COUNT = 512  # Number of images to process\nBASE64_IMAGE_HEIGHT = 512  # Standardized height for model input\n\n# Load ColPali dataset for visual documents\nimg_dataset_cfg = {\"path\": \"vidore/colpali_train_set\", \"split\": \"train\", \"streaming\": True}\n</pre> # Dataset processing configuration IMG_COUNT = 512  # Number of images to process BASE64_IMAGE_HEIGHT = 512  # Standardized height for model input  # Load ColPali dataset for visual documents img_dataset_cfg = {\"path\": \"vidore/colpali_train_set\", \"split\": \"train\", \"streaming\": True} In\u00a0[\u00a0]: Copied! <pre>def resize_image(image, height: int):\n    \"\"\"\n    Resize image while maintaining aspect ratio.\n\n    Args:\n        image: PIL Image object\n        height: Target height in pixels\n\n    Returns:\n        Resized PIL Image object\n    \"\"\"\n    original_width, original_height = image.size\n    width = int(original_width * (height / original_height))\n    return image.resize((width, height))\n\n\ndef convert_image_to_chat_format(record, height: int) -&gt; dict:\n    \"\"\"\n    Convert PIL image to base64 format for chat template usage.\n\n    Args:\n        record: Dataset record containing image and metadata\n        height: Target height for image resizing\n\n    Returns:\n        Updated record with base64_image and uuid fields\n    \"\"\"\n    # Resize image for consistent processing\n    image = resize_image(record[\"image\"], height)\n\n    # Convert to base64 string\n    img_buffer = io.BytesIO()\n    image.save(img_buffer, format=\"PNG\")\n    byte_data = img_buffer.getvalue()\n    base64_encoded_data = base64.b64encode(byte_data)\n    base64_string = base64_encoded_data.decode(\"utf-8\")\n\n    # Return updated record\n    return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())}\n</pre> def resize_image(image, height: int):     \"\"\"     Resize image while maintaining aspect ratio.      Args:         image: PIL Image object         height: Target height in pixels      Returns:         Resized PIL Image object     \"\"\"     original_width, original_height = image.size     width = int(original_width * (height / original_height))     return image.resize((width, height))   def convert_image_to_chat_format(record, height: int) -&gt; dict:     \"\"\"     Convert PIL image to base64 format for chat template usage.      Args:         record: Dataset record containing image and metadata         height: Target height for image resizing      Returns:         Updated record with base64_image and uuid fields     \"\"\"     # Resize image for consistent processing     image = resize_image(record[\"image\"], height)      # Convert to base64 string     img_buffer = io.BytesIO()     image.save(img_buffer, format=\"PNG\")     byte_data = img_buffer.getvalue()     base64_encoded_data = base64.b64encode(byte_data)     base64_string = base64_encoded_data.decode(\"utf-8\")      # Return updated record     return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())} In\u00a0[\u00a0]: Copied! <pre># Load and process the visual document dataset\nprint(\"\ud83d\udce5 Loading and processing document images...\")\n\nimg_dataset_iter = iter(\n    load_dataset(**img_dataset_cfg).map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT})\n)\nimg_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])\n\nprint(f\"\u2705 Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\")\n</pre> # Load and process the visual document dataset print(\"\ud83d\udce5 Loading and processing document images...\")  img_dataset_iter = iter(     load_dataset(**img_dataset_cfg).map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT}) ) img_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])  print(f\"\u2705 Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\") In\u00a0[\u00a0]: Copied! <pre>img_dataset.head()\n</pre> img_dataset.head() In\u00a0[\u00a0]: Copied! <pre># Add the seed dataset containing our processed images\ndf_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]]\nconfig_builder.with_seed_dataset(dd.DataFrameSeedSource(df=df_seed))\n</pre> # Add the seed dataset containing our processed images df_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]] config_builder.with_seed_dataset(dd.DataFrameSeedSource(df=df_seed)) In\u00a0[\u00a0]: Copied! <pre># Add a column to generate detailed document summaries\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"summary\",\n        model_alias=\"vision\",\n        prompt=(\n            \"Provide a detailed summary of the content in this image in Markdown format. \"\n            \"Start from the top of the image and then describe it from top to bottom. \"\n            \"Place a summary at the bottom.\"\n        ),\n        multi_modal_context=[\n            dd.ImageContext(\n                column_name=\"base64_image\",\n                data_type=dd.ModalityDataType.BASE64,\n                image_format=dd.ImageFormat.PNG,\n            )\n        ],\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> # Add a column to generate detailed document summaries config_builder.add_column(     dd.LLMTextColumnConfig(         name=\"summary\",         model_alias=\"vision\",         prompt=(             \"Provide a detailed summary of the content in this image in Markdown format. \"             \"Start from the top of the image and then describe it from top to bottom. \"             \"Place a summary at the bottom.\"         ),         multi_modal_context=[             dd.ImageContext(                 column_name=\"base64_image\",                 data_type=dd.ModalityDataType.BASE64,                 image_format=dd.ImageFormat.PNG,             )         ],     ) )  data_designer.validate(config_builder) In\u00a0[\u00a0]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) In\u00a0[\u00a0]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() In\u00a0[\u00a0]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset In\u00a0[\u00a0]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() In\u00a0[\u00a0]: Copied! <pre># Compare original document with generated summary\nindex = 0  # Change this to view different examples\n\n# Merge preview data with original images for comparison\ncomparison_dataset = preview.dataset.merge(pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\")\n\n# Extract the record for display\nrecord = comparison_dataset.iloc[index]\n\nprint(\"\ud83d\udcc4 Original Document Image:\")\ndisplay(resize_image(record.image, BASE64_IMAGE_HEIGHT))\n\nprint(\"\\n\ud83d\udcdd Generated Summary:\")\nrich.print(Panel(record.summary, title=\"Document Summary\", title_align=\"left\"))\n</pre> # Compare original document with generated summary index = 0  # Change this to view different examples  # Merge preview data with original images for comparison comparison_dataset = preview.dataset.merge(pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\")  # Extract the record for display record = comparison_dataset.iloc[index]  print(\"\ud83d\udcc4 Original Document Image:\") display(resize_image(record.image, BASE64_IMAGE_HEIGHT))  print(\"\\n\ud83d\udcdd Generated Summary:\") rich.print(Panel(record.summary, title=\"Document Summary\", title_align=\"left\")) In\u00a0[\u00a0]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-4\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-4\") In\u00a0[\u00a0]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() In\u00a0[\u00a0]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report()"},{"location":"notebook_source/4-providing-images-as-context/#data-designer-tutorial-providing-images-as-context-for-vision-based-data-generation","title":"\ud83c\udfa8 Data Designer Tutorial: Providing Images as Context for Vision-Based Data Generation\u00b6","text":""},{"location":"notebook_source/4-providing-images-as-context/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>This notebook demonstrates how to provide images as context to generate text descriptions using vision-language models.</p> <ul> <li>\u2728 Visual Document Processing: Converting images to chat-ready format for model consumption</li> <li>\ud83d\udd0d Vision-Language Generation: Using vision models to generate detailed summaries from images</li> </ul> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"notebook_source/4-providing-images-as-context/#import-data-designer","title":"\ud83d\udce6 Import Data Designer\u00b6","text":"<ul> <li><p><code>data_designer.config</code> provides access to the configuration API.</p> </li> <li><p><code>DataDesigner</code> is the main interface for data generation.</p> </li> </ul>"},{"location":"notebook_source/4-providing-images-as-context/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebook_source/4-providing-images-as-context/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebook_source/4-providing-images-as-context/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebook_source/4-providing-images-as-context/#seed-dataset-creation","title":"\ud83c\udf31 Seed Dataset Creation\u00b6","text":"<p>In this section, we'll prepare our visual documents as a seed dataset for summarization:</p> <ul> <li>Loading Visual Documents: We use the ColPali dataset containing document images</li> <li>Image Processing: Convert images to base64 format for vision model consumption</li> <li>Metadata Extraction: Preserve relevant document information (filename, page number, source, etc.)</li> </ul> <p>The seed dataset will be used to generate detailed text summaries of each document image.</p>"},{"location":"notebook_source/4-providing-images-as-context/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013 preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebook_source/4-providing-images-as-context/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebook_source/4-providing-images-as-context/#visual-inspection","title":"\ud83d\udd0e Visual Inspection\u00b6","text":"<p>Let's compare the original document image with the generated summary to validate quality:</p>"},{"location":"notebook_source/4-providing-images-as-context/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebook_source/4-providing-images-as-context/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Now that you've learned how to use visual context for image summarization in Data Designer, explore more:</p> <ul> <li>Experiment with different vision models for specific document types</li> <li>Try different prompt variations to generate specialized descriptions (e.g., technical details, key findings)</li> <li>Combine vision-based summaries with other column types for multi-modal workflows</li> <li>Apply this pattern to other vision tasks like image captioning, OCR validation, or visual question answering</li> </ul>"},{"location":"notebook_source/_README/","title":"Overview","text":"<p>Welcome to the Data Designer tutorial series! These hands-on notebooks will guide you through the core concepts and features of Data Designer, from basic synthetic data generation to advanced techniques like structured outputs and dataset seeding.</p>"},{"location":"notebook_source/_README/#setting-up-your-environment","title":"\ud83d\ude80 Setting Up Your Environment","text":""},{"location":"notebook_source/_README/#local-setup-best-practices","title":"Local Setup Best Practices","text":"<p>First, download the tutorial from the release assets. To run the tutorial notebooks locally, we recommend using a virtual environment to manage dependencies:</p> uv (Recommended)pip + venv <pre><code># Extract tutorial notebooks\nunzip data_designer_tutorial.zip\ncd data_designer_tutorial\n\n# Launch Jupyter\nuv run jupyter notebook\n</code></pre> <pre><code># Extract tutorial notebooks\nunzip data_designer_tutorial.zip\ncd data_designer_tutorial\n\n# Create Python virtual environment and install required packages\npython -m venv venv\nsource venv/bin/activate\npip install data-designer jupyter\n\n# Launch Jupyter\njupyter notebook\n</code></pre>"},{"location":"notebook_source/_README/#api-keys-and-authentication","title":"API Keys and Authentication","text":"<p>Data Designer is able to interface with various LLM providers. You'll need to set up API keys for the models you want to use:</p> <pre><code># For NVIDIA API Catalog (build.nvidia.com)\nexport NVIDIA_API_KEY=\"your-api-key-here\"\n\n# For OpenAI\nexport OPENAI_API_KEY=\"your-api-key-here\"\n\n# For OpenRouter\nexport OPENROUTER_API_KEY=\"your-api-key-here\"\n</code></pre> <p>For more information, check the Welcome, Default Model Settings and how to Configure Model Settings Using The CLI.</p>"},{"location":"notebook_source/_README/#tutorial-series","title":"\ud83d\udcda Tutorial Series","text":"<p>The tutorials are designed to be completed in sequence, building upon concepts introduced in previous notebooks:</p>"},{"location":"notebook_source/_README/#1-the-basics","title":"1. The Basics","text":"<p>Learn the fundamentals of Data Designer by generating a simple product review dataset. This notebook covers:</p> <ul> <li>Setting up the <code>DataDesigner</code> interface</li> <li>Configuring models and inference parameters</li> <li>Using built-in samplers (Category, Person, Uniform)</li> <li>Generating LLM text columns with dependencies</li> <li>Understanding the generation workflow</li> </ul> <p>Start here if you're new to Data Designer!</p>"},{"location":"notebook_source/_README/#2-structured-outputs-and-jinja-expressions","title":"2. Structured Outputs and Jinja Expressions","text":"<p>Explore more advanced data generation capabilities:</p> <ul> <li>Creating structured JSON outputs with schemas</li> <li>Using Jinja expressions for derived columns</li> <li>Combining samplers with structured data</li> <li>Building complex data dependencies</li> <li>Working with nested data structures</li> </ul>"},{"location":"notebook_source/_README/#3-seeding-with-an-external-dataset","title":"3. Seeding with an External Dataset","text":"<p>Learn how to leverage existing datasets to guide synthetic data generation:</p> <ul> <li>Loading and using seed datasets</li> <li>Sampling from real data distributions</li> <li>Combining seed data with LLM generation</li> <li>Creating realistic synthetic data based on existing patterns</li> </ul>"},{"location":"notebook_source/_README/#4-providing-images-as-context","title":"4. Providing Images as Context","text":"<p>Learn how to use vision-language models to generate text descriptions from images:</p> <ul> <li>Processing and converting images to base64 format for model consumption</li> <li>Using vision-language models (VLMs) to analyze visual documents</li> <li>Generating detailed summaries from document images</li> <li>Inspecting and validating vision-based generation results</li> </ul>"},{"location":"notebook_source/_README/#important-documentation-sections","title":"\ud83d\udcd6 Important Documentation Sections","text":"<p>Before diving into the tutorials, familiarize yourself with these key documentation sections:</p>"},{"location":"notebook_source/_README/#getting-started","title":"Getting Started","text":"<ul> <li>Welcome &amp; Installation - Overview of Data Designer capabilities and installation instructions</li> </ul>"},{"location":"notebook_source/_README/#core-concepts","title":"Core Concepts","text":"<p>Understanding these concepts will help you make the most of the tutorials:</p> <ul> <li>Columns - Learn about different column types (Sampler, LLM, Expression, Validation, etc.)</li> <li>Validators - Understand how to validate generated data with Python, SQL, and remote validators</li> <li>Person Sampling - Learn how to sample realistic person data with demographic attributes</li> </ul>"},{"location":"notebook_source/_README/#code-reference","title":"Code Reference","text":"<p>Quick reference guides for the main configuration objects:</p> <ul> <li>column_configs - All column configuration types</li> <li>config_builder - The <code>DataDesignerConfigBuilder</code> API</li> <li>data_designer_config - Main configuration schema</li> <li>validator_params - Validator configuration options</li> </ul>"},{"location":"notebooks/","title":"Overview","text":"<p>Welcome to the Data Designer tutorial series! These hands-on notebooks will guide you through the core concepts and features of Data Designer, from basic synthetic data generation to advanced techniques like structured outputs and dataset seeding.</p>"},{"location":"notebooks/#setting-up-your-environment","title":"\ud83d\ude80 Setting Up Your Environment","text":""},{"location":"notebooks/#local-setup-best-practices","title":"Local Setup Best Practices","text":"<p>First, download the tutorial from the release assets. To run the tutorial notebooks locally, we recommend using a virtual environment to manage dependencies:</p> uv (Recommended)pip + venv <pre><code># Extract tutorial notebooks\nunzip data_designer_tutorial.zip\ncd data_designer_tutorial\n\n# Launch Jupyter\nuv run jupyter notebook\n</code></pre> <pre><code># Extract tutorial notebooks\nunzip data_designer_tutorial.zip\ncd data_designer_tutorial\n\n# Create Python virtual environment and install required packages\npython -m venv venv\nsource venv/bin/activate\npip install data-designer jupyter\n\n# Launch Jupyter\njupyter notebook\n</code></pre>"},{"location":"notebooks/#api-keys-and-authentication","title":"API Keys and Authentication","text":"<p>Data Designer is able to interface with various LLM providers. You'll need to set up API keys for the models you want to use:</p> <pre><code># For NVIDIA API Catalog (build.nvidia.com)\nexport NVIDIA_API_KEY=\"your-api-key-here\"\n\n# For OpenAI\nexport OPENAI_API_KEY=\"your-api-key-here\"\n\n# For OpenRouter\nexport OPENROUTER_API_KEY=\"your-api-key-here\"\n</code></pre> <p>For more information, check the Welcome, Default Model Settings and how to Configure Model Settings Using The CLI.</p>"},{"location":"notebooks/#tutorial-series","title":"\ud83d\udcda Tutorial Series","text":"<p>The tutorials are designed to be completed in sequence, building upon concepts introduced in previous notebooks:</p>"},{"location":"notebooks/#1-the-basics","title":"1. The Basics","text":"<p>Learn the fundamentals of Data Designer by generating a simple product review dataset. This notebook covers:</p> <ul> <li>Setting up the <code>DataDesigner</code> interface</li> <li>Configuring models and inference parameters</li> <li>Using built-in samplers (Category, Person, Uniform)</li> <li>Generating LLM text columns with dependencies</li> <li>Understanding the generation workflow</li> </ul> <p>Start here if you're new to Data Designer!</p>"},{"location":"notebooks/#2-structured-outputs-and-jinja-expressions","title":"2. Structured Outputs and Jinja Expressions","text":"<p>Explore more advanced data generation capabilities:</p> <ul> <li>Creating structured JSON outputs with schemas</li> <li>Using Jinja expressions for derived columns</li> <li>Combining samplers with structured data</li> <li>Building complex data dependencies</li> <li>Working with nested data structures</li> </ul>"},{"location":"notebooks/#3-seeding-with-an-external-dataset","title":"3. Seeding with an External Dataset","text":"<p>Learn how to leverage existing datasets to guide synthetic data generation:</p> <ul> <li>Loading and using seed datasets</li> <li>Sampling from real data distributions</li> <li>Combining seed data with LLM generation</li> <li>Creating realistic synthetic data based on existing patterns</li> </ul>"},{"location":"notebooks/#4-providing-images-as-context","title":"4. Providing Images as Context","text":"<p>Learn how to use vision-language models to generate text descriptions from images:</p> <ul> <li>Processing and converting images to base64 format for model consumption</li> <li>Using vision-language models (VLMs) to analyze visual documents</li> <li>Generating detailed summaries from document images</li> <li>Inspecting and validating vision-based generation results</li> </ul>"},{"location":"notebooks/#important-documentation-sections","title":"\ud83d\udcd6 Important Documentation Sections","text":"<p>Before diving into the tutorials, familiarize yourself with these key documentation sections:</p>"},{"location":"notebooks/#getting-started","title":"Getting Started","text":"<ul> <li>Welcome &amp; Installation - Overview of Data Designer capabilities and installation instructions</li> </ul>"},{"location":"notebooks/#core-concepts","title":"Core Concepts","text":"<p>Understanding these concepts will help you make the most of the tutorials:</p> <ul> <li>Columns - Learn about different column types (Sampler, LLM, Expression, Validation, etc.)</li> <li>Validators - Understand how to validate generated data with Python, SQL, and remote validators</li> <li>Person Sampling - Learn how to sample realistic person data with demographic attributes</li> </ul>"},{"location":"notebooks/#code-reference","title":"Code Reference","text":"<p>Quick reference guides for the main configuration objects:</p> <ul> <li>column_configs - All column configuration types</li> <li>config_builder - The <code>DataDesignerConfigBuilder</code> API</li> <li>data_designer_config - Main configuration schema</li> <li>validator_params - Validator configuration options</li> </ul>"},{"location":"notebooks/1-the-basics/","title":"The Basics","text":"In\u00a0[1]: Copied! <pre>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n</pre> import data_designer.config as dd from data_designer.interface import DataDesigner In\u00a0[2]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[3]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    dd.ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     dd.ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=dd.ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[4]: Copied! <pre>config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[5]: Copied! <pre>config_builder.info.display(\"samplers\")\n</pre> config_builder.info.display(\"samplers\") <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 NeMo Data Designer Samplers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Type               \u2503 Parameter                \u2503 Data Type                         \u2503 Required \u2503 Constraints      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 bernoulli          \u2502 p                        \u2502 number                            \u2502    \u2713     \u2502 &gt;= 0.0, &lt;= 1.0   \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 bernoulli_mixture  \u2502 p                        \u2502 number                            \u2502    \u2713     \u2502 &gt;= 0.0, &lt;= 1.0   \u2502\n\u2502                    \u2502 dist_name                \u2502 string                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 dist_params              \u2502 dict                              \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 binomial           \u2502 n                        \u2502 integer                           \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 p                        \u2502 number                            \u2502    \u2713     \u2502 &gt;= 0.0, &lt;= 1.0   \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 category           \u2502 values                   \u2502 string[] | integer[] | number[]   \u2502    \u2713     \u2502 len &gt; 1          \u2502\n\u2502                    \u2502 weights                  \u2502 number[] | null                   \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 datetime           \u2502 start                    \u2502 string                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 end                      \u2502 string                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 unit                     \u2502 string                            \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 gaussian           \u2502 mean                     \u2502 number                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 stddev                   \u2502 number                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 decimal_places           \u2502 integer | null                    \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 person             \u2502 locale                   \u2502 string                            \u2502          \u2502                  \u2502\n\u2502                    \u2502 sex                      \u2502 string | null                     \u2502          \u2502                  \u2502\n\u2502                    \u2502 city                     \u2502 string | string[] | null          \u2502          \u2502                  \u2502\n\u2502                    \u2502 age_range                \u2502 integer[]                         \u2502          \u2502 len &gt; 2, len &lt; 2 \u2502\n\u2502                    \u2502 select_field_values      \u2502 object | null                     \u2502          \u2502                  \u2502\n\u2502                    \u2502 with_synthetic_personas  \u2502 boolean                           \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 person_from_faker  \u2502 locale                   \u2502 string                            \u2502          \u2502                  \u2502\n\u2502                    \u2502 sex                      \u2502 string | null                     \u2502          \u2502                  \u2502\n\u2502                    \u2502 city                     \u2502 string | string[] | null          \u2502          \u2502                  \u2502\n\u2502                    \u2502 age_range                \u2502 integer[]                         \u2502          \u2502 len &gt; 2, len &lt; 2 \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 poisson            \u2502 mean                     \u2502 number                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 scipy              \u2502 dist_name                \u2502 string                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 dist_params              \u2502 dict                              \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 decimal_places           \u2502 integer | null                    \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 subcategory        \u2502 category                 \u2502 string                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 values                   \u2502 dict                              \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 timedelta          \u2502 dt_min                   \u2502 integer                           \u2502    \u2713     \u2502 &gt;= 0             \u2502\n\u2502                    \u2502 dt_max                   \u2502 integer                           \u2502    \u2713     \u2502 &gt; 0              \u2502\n\u2502                    \u2502 reference_column_name    \u2502 string                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 unit                     \u2502 string                            \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 uniform            \u2502 low                      \u2502 number                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 high                     \u2502 number                            \u2502    \u2713     \u2502                  \u2502\n\u2502                    \u2502 decimal_places           \u2502 integer | null                    \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 uuid               \u2502 prefix                   \u2502 string | null                     \u2502          \u2502                  \u2502\n\u2502                    \u2502 short_form               \u2502 boolean                           \u2502          \u2502                  \u2502\n\u2502                    \u2502 uppercase                \u2502 boolean                           \u2502          \u2502                  \u2502\n\u2502                    \u2502 sampler_type             \u2502 string                            \u2502          \u2502                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <p>Let's start designing our product review dataset by adding product category and subcategory columns.</p> In\u00a0[6]: Copied! <pre>config_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"product_category\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\n                \"Electronics\",\n                \"Clothing\",\n                \"Home &amp; Kitchen\",\n                \"Books\",\n                \"Home Office\",\n            ],\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"product_subcategory\",\n        sampler_type=dd.SamplerType.SUBCATEGORY,\n        params=dd.SubcategorySamplerParams(\n            category=\"product_category\",\n            values={\n                \"Electronics\": [\n                    \"Smartphones\",\n                    \"Laptops\",\n                    \"Headphones\",\n                    \"Cameras\",\n                    \"Accessories\",\n                ],\n                \"Clothing\": [\n                    \"Men's Clothing\",\n                    \"Women's Clothing\",\n                    \"Winter Coats\",\n                    \"Activewear\",\n                    \"Accessories\",\n                ],\n                \"Home &amp; Kitchen\": [\n                    \"Appliances\",\n                    \"Cookware\",\n                    \"Furniture\",\n                    \"Decor\",\n                    \"Organization\",\n                ],\n                \"Books\": [\n                    \"Fiction\",\n                    \"Non-Fiction\",\n                    \"Self-Help\",\n                    \"Textbooks\",\n                    \"Classics\",\n                ],\n                \"Home Office\": [\n                    \"Desks\",\n                    \"Chairs\",\n                    \"Storage\",\n                    \"Office Supplies\",\n                    \"Lighting\",\n                ],\n            },\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"target_age_range\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),\n    )\n)\n\n# Optionally validate that the columns are configured correctly.\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     dd.SamplerColumnConfig(         name=\"product_category\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(             values=[                 \"Electronics\",                 \"Clothing\",                 \"Home &amp; Kitchen\",                 \"Books\",                 \"Home Office\",             ],         ),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"product_subcategory\",         sampler_type=dd.SamplerType.SUBCATEGORY,         params=dd.SubcategorySamplerParams(             category=\"product_category\",             values={                 \"Electronics\": [                     \"Smartphones\",                     \"Laptops\",                     \"Headphones\",                     \"Cameras\",                     \"Accessories\",                 ],                 \"Clothing\": [                     \"Men's Clothing\",                     \"Women's Clothing\",                     \"Winter Coats\",                     \"Activewear\",                     \"Accessories\",                 ],                 \"Home &amp; Kitchen\": [                     \"Appliances\",                     \"Cookware\",                     \"Furniture\",                     \"Decor\",                     \"Organization\",                 ],                 \"Books\": [                     \"Fiction\",                     \"Non-Fiction\",                     \"Self-Help\",                     \"Textbooks\",                     \"Classics\",                 ],                 \"Home Office\": [                     \"Desks\",                     \"Chairs\",                     \"Storage\",                     \"Office Supplies\",                     \"Lighting\",                 ],             },         ),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"target_age_range\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),     ) )  # Optionally validate that the columns are configured correctly. data_designer.validate(config_builder) <pre>[22:48:46] [INFO] \u2705 Validation passed\n</pre> <p>Next, let's add samplers to generate data related to the customer and their review.</p> In\u00a0[7]: Copied! <pre>config_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=dd.SamplerType.PERSON_FROM_FAKER,\n        params=dd.PersonFromFakerSamplerParams(age_range=[18, 70], locale=\"en_US\"),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"number_of_stars\",\n        sampler_type=dd.SamplerType.UNIFORM,\n        params=dd.UniformSamplerParams(low=1, high=5),\n        convert_to=\"int\",  # Convert the sampled float to an integer.\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"review_style\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],\n            weights=[1, 2, 2, 1],\n        ),\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     dd.SamplerColumnConfig(         name=\"customer\",         sampler_type=dd.SamplerType.PERSON_FROM_FAKER,         params=dd.PersonFromFakerSamplerParams(age_range=[18, 70], locale=\"en_US\"),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"number_of_stars\",         sampler_type=dd.SamplerType.UNIFORM,         params=dd.UniformSamplerParams(low=1, high=5),         convert_to=\"int\",  # Convert the sampled float to an integer.     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"review_style\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(             values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],             weights=[1, 2, 2, 1],         ),     ) )  data_designer.validate(config_builder) <pre>[22:48:47] [INFO] \u2705 Validation passed\n</pre> In\u00a0[8]: Copied! <pre>config_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"product_name\",\n        prompt=(\n            \"You are a helpful assistant that generates product names. DO NOT add quotes around the product name.\\n\\n\"\n            \"Come up with a creative product name for a product in the '{{ product_category }}' category, focusing \"\n            \"on products related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"\n            \"{{ target_age_range }} years old. Respond with only the product name, no other text.\"\n        ),\n        model_alias=MODEL_ALIAS,\n    )\n)\n\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"customer_review\",\n        prompt=(\n            \"You are a customer named {{ customer.first_name }} from {{ customer.city }}, {{ customer.state }}. \"\n            \"You are {{ customer.age }} years old and recently purchased a product called {{ product_name }}. \"\n            \"Write a review of this product, which you gave a rating of {{ number_of_stars }} stars. \"\n            \"The style of the review should be '{{ review_style }}'. \"\n            \"Respond with only the review, no other text.\"\n        ),\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     dd.LLMTextColumnConfig(         name=\"product_name\",         prompt=(             \"You are a helpful assistant that generates product names. DO NOT add quotes around the product name.\\n\\n\"             \"Come up with a creative product name for a product in the '{{ product_category }}' category, focusing \"             \"on products related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"             \"{{ target_age_range }} years old. Respond with only the product name, no other text.\"         ),         model_alias=MODEL_ALIAS,     ) )  config_builder.add_column(     dd.LLMTextColumnConfig(         name=\"customer_review\",         prompt=(             \"You are a customer named {{ customer.first_name }} from {{ customer.city }}, {{ customer.state }}. \"             \"You are {{ customer.age }} years old and recently purchased a product called {{ product_name }}. \"             \"Write a review of this product, which you gave a rating of {{ number_of_stars }} stars. \"             \"The style of the review should be '{{ review_style }}'. \"             \"Respond with only the review, no other text.\"         ),         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) <pre>[22:48:47] [INFO] \u2705 Validation passed\n</pre> In\u00a0[9]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) <pre>[22:48:47] [INFO] \ud83d\udcf8 Preview generation in progress\n</pre> <pre>[22:48:47] [INFO] \u2705 Validation passed\n</pre> <pre>[22:48:47] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:48:47] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:48:47] [INFO]   |-- \ud83d\udc40 Checking 'nvidia/nemotron-3-nano-30b-a3b' in provider named 'nvidia' for model alias 'nemotron-nano-v3'...\n</pre> <pre>[22:48:47] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:48:47] [INFO] \ud83c\udfb2 Preparing samplers to generate 2 records across 6 columns\n</pre> <pre>[22:48:49] [INFO] \ud83d\udcdd llm-text model config for column 'product_name'\n</pre> <pre>[22:48:49] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:48:49] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:48:49] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:48:49] [INFO]   |-- inference parameters:\n</pre> <pre>[22:48:49] [INFO]   |  |-- generation_type=chat-completion\n</pre> <pre>[22:48:49] [INFO]   |  |-- max_parallel_requests=4\n</pre> <pre>[22:48:49] [INFO]   |  |-- extra_body={'chat_template_kwargs': {'enable_thinking': False}}\n</pre> <pre>[22:48:49] [INFO]   |  |-- temperature=1.00\n</pre> <pre>[22:48:49] [INFO]   |  |-- top_p=1.00\n</pre> <pre>[22:48:49] [INFO]   |  |-- max_tokens=2048\n</pre> <pre>[22:48:49] [INFO] \u26a1\ufe0f Processing llm-text column 'product_name' with 4 concurrent workers\n</pre> <pre>[22:48:49] [INFO] \u23f1\ufe0f llm-text column 'product_name' will report progress after each record\n</pre> <pre>[22:48:49] [INFO]   |-- \ud83d\udc25 llm-text column 'product_name' progress: 1/2 (50%) complete, 1 ok, 0 failed, 2.39 rec/s, eta 0.4s\n</pre> <pre>[22:48:49] [INFO]   |-- \ud83d\udc14 llm-text column 'product_name' progress: 2/2 (100%) complete, 2 ok, 0 failed, 4.74 rec/s, eta 0.0s\n</pre> <pre>[22:48:49] [INFO] \ud83d\udcdd llm-text model config for column 'customer_review'\n</pre> <pre>[22:48:49] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:48:49] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:48:49] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:48:49] [INFO]   |-- inference parameters:\n</pre> <pre>[22:48:49] [INFO]   |  |-- generation_type=chat-completion\n</pre> <pre>[22:48:49] [INFO]   |  |-- max_parallel_requests=4\n</pre> <pre>[22:48:49] [INFO]   |  |-- extra_body={'chat_template_kwargs': {'enable_thinking': False}}\n</pre> <pre>[22:48:49] [INFO]   |  |-- temperature=1.00\n</pre> <pre>[22:48:49] [INFO]   |  |-- top_p=1.00\n</pre> <pre>[22:48:49] [INFO]   |  |-- max_tokens=2048\n</pre> <pre>[22:48:49] [INFO] \u26a1\ufe0f Processing llm-text column 'customer_review' with 4 concurrent workers\n</pre> <pre>[22:48:49] [INFO] \u23f1\ufe0f llm-text column 'customer_review' will report progress after each record\n</pre> <pre>[22:48:50] [INFO]   |-- \ud83c\udf17 llm-text column 'customer_review' progress: 1/2 (50%) complete, 1 ok, 0 failed, 1.21 rec/s, eta 0.8s\n</pre> <pre>[22:48:53] [INFO]   |-- \ud83c\udf15 llm-text column 'customer_review' progress: 2/2 (100%) complete, 2 ok, 0 failed, 0.59 rec/s, eta 0.0s\n</pre> <pre>[22:48:53] [INFO] \ud83d\udcca Model usage summary:\n</pre> <pre>[22:48:53] [INFO]   |-- model: nvidia/nemotron-3-nano-30b-a3b\n</pre> <pre>[22:48:53] [INFO]   |-- tokens: input=365, output=853, total=1218, tps=208\n</pre> <pre>[22:48:53] [INFO]   |-- requests: success=4, failed=0, total=4, rpm=41\n</pre> <pre>[22:48:53] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:48:53] [INFO]   |-- \ud83c\udfb2 column: 'product_category'\n</pre> <pre>[22:48:53] [INFO]   |-- \ud83c\udfb2 column: 'product_subcategory'\n</pre> <pre>[22:48:53] [INFO]   |-- \ud83c\udfb2 column: 'target_age_range'\n</pre> <pre>[22:48:53] [INFO]   |-- \ud83c\udfb2 column: 'customer'\n</pre> <pre>[22:48:53] [INFO]   |-- \ud83c\udfb2 column: 'number_of_stars'\n</pre> <pre>[22:48:53] [INFO]   |-- \ud83c\udfb2 column: 'review_style'\n</pre> <pre>[22:48:53] [INFO]   |-- \ud83d\udcdd column: 'product_name'\n</pre> <pre>[22:48:53] [INFO]   |-- \ud83d\udcdd column: 'customer_review'\n</pre> <pre>[22:48:53] [INFO] \ud83d\udc4f Preview complete!\n</pre> In\u00a0[10]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() <pre>                                                                                                                   \n                                                 Generated Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name                \u2503 Value                                                                                     \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_category    \u2502 Home &amp; Kitchen                                                                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product_subcategory \u2502 Furniture                                                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 target_age_range    \u2502 35-50                                                                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer            \u2502 {                                                                                         \u2502\n\u2502                     \u2502     'uuid': 'b1939170-7e6a-4353-bbe0-97bc538e0016',                                       \u2502\n\u2502                     \u2502     'locale': 'en_US',                                                                    \u2502\n\u2502                     \u2502     'first_name': 'Monique',                                                              \u2502\n\u2502                     \u2502     'last_name': 'Fox',                                                                   \u2502\n\u2502                     \u2502     'middle_name': None,                                                                  \u2502\n\u2502                     \u2502     'sex': 'Female',                                                                      \u2502\n\u2502                     \u2502     'street_number': '49346',                                                             \u2502\n\u2502                     \u2502     'street_name': 'Andrea Way',                                                          \u2502\n\u2502                     \u2502     'city': 'Emilyfurt',                                                                  \u2502\n\u2502                     \u2502     'state': 'West Virginia',                                                             \u2502\n\u2502                     \u2502     'postcode': '76912',                                                                  \u2502\n\u2502                     \u2502     'age': 69,                                                                            \u2502\n\u2502                     \u2502     'birth_date': '1956-03-24',                                                           \u2502\n\u2502                     \u2502     'country': 'Belarus',                                                                 \u2502\n\u2502                     \u2502     'marital_status': 'widowed',                                                          \u2502\n\u2502                     \u2502     'education_level': 'graduate',                                                        \u2502\n\u2502                     \u2502     'unit': '',                                                                           \u2502\n\u2502                     \u2502     'occupation': 'Dealer',                                                               \u2502\n\u2502                     \u2502     'phone_number': '520.718.1108',                                                       \u2502\n\u2502                     \u2502     'bachelors_field': 'arts_humanities'                                                  \u2502\n\u2502                     \u2502 }                                                                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 number_of_stars     \u2502 3                                                                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 review_style        \u2502 detailed                                                                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product_name        \u2502 AtelierSeal of Oak Collection                                                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_review     \u2502 I am Monique from Emilyfurt, West Virginia, age 69. I recently bought the AtelierSeal of  \u2502\n\u2502                     \u2502 Oak Collection, rating it 3 stars. The product arrived quickly, but the seal was slightly \u2502\n\u2502                     \u2502 loose on the oak veneer and the finish feels thin and easily scratches. However, the      \u2502\n\u2502                     \u2502 grain pattern is beautiful and the style looks elegant in my study. I appreciate the      \u2502\n\u2502                     \u2502 handcrafted feel but was disappointed the edges didn't align perfectly and the scent of   \u2502\n\u2502                     \u2502 varnish is strong. Overall, it's functional and attractive, but the imperfections keep it \u2502\n\u2502                     \u2502 from a higher rating and I'd only recommend for light use.                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                    [index: 0]                                                     \n</pre> In\u00a0[11]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset Out[11]: product_category product_subcategory target_age_range customer number_of_stars review_style product_name customer_review 0 Home &amp; Kitchen Furniture 35-50 {'uuid': 'b1939170-7e6a-4353-bbe0-97bc538e0016... 3 detailed AtelierSeal of Oak Collection I am Monique from Emilyfurt, West Virginia, ag... 1 Home Office Desks 65+ {'uuid': 'd617bec8-1e9e-407d-a097-d193f3a7d885... 5 rambling Ergonomic StableOak Desk with Built\u2011In Memory Aid I\u2019m Christopher from North\u202fRobert, Kansas, and... In\u00a0[12]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2                               \u2502 8                               \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83c\udfb2 Sampler Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                    \u2503       data type \u2503            number unique values \u2503               sampler type \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_category               \u2502          string \u2502                      2 (100.0%) \u2502                   category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product_subcategory            \u2502          string \u2502                      2 (100.0%) \u2502                subcategory \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 target_age_range               \u2502          string \u2502                      2 (100.0%) \u2502                   category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer                       \u2502            dict \u2502                      2 (100.0%) \u2502          person_from_faker \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 number_of_stars                \u2502             int \u2502                      2 (100.0%) \u2502                    uniform \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 review_style                   \u2502          string \u2502                      2 (100.0%) \u2502                   category \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83d\udcdd LLM-Text Columns                                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                       \u2503               \u2503                            \u2503     prompt tokens \u2503      completion tokens \u2503\n\u2503 column name           \u2503     data type \u2503       number unique values \u2503        per record \u2503             per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_name          \u2502        string \u2502                 2 (100.0%) \u2502      74.0 +/- 1.0 \u2502            9.5 +/- 3.5 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_review       \u2502        string \u2502                 2 (100.0%) \u2502      74.0 +/- 1.0 \u2502        399.0 +/- 387.5 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> In\u00a0[13]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-1\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-1\") <pre>[22:48:53] [INFO] \ud83c\udfa8 Creating Data Designer dataset\n</pre> <pre>[22:48:53] [INFO] \u2705 Validation passed\n</pre> <pre>[22:48:53] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:48:53] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:48:53] [INFO]   |-- \ud83d\udc40 Checking 'nvidia/nemotron-3-nano-30b-a3b' in provider named 'nvidia' for model alias 'nemotron-nano-v3'...\n</pre> <pre>[22:48:54] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:48:54] [INFO] \u23f3 Processing batch 1 of 1\n</pre> <pre>[22:48:54] [INFO] \ud83c\udfb2 Preparing samplers to generate 10 records across 6 columns\n</pre> <pre>[22:48:54] [INFO] \ud83d\udcdd llm-text model config for column 'product_name'\n</pre> <pre>[22:48:54] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:48:54] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:48:54] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:48:54] [INFO]   |-- inference parameters:\n</pre> <pre>[22:48:54] [INFO]   |  |-- generation_type=chat-completion\n</pre> <pre>[22:48:54] [INFO]   |  |-- max_parallel_requests=4\n</pre> <pre>[22:48:54] [INFO]   |  |-- extra_body={'chat_template_kwargs': {'enable_thinking': False}}\n</pre> <pre>[22:48:54] [INFO]   |  |-- temperature=1.00\n</pre> <pre>[22:48:54] [INFO]   |  |-- top_p=1.00\n</pre> <pre>[22:48:54] [INFO]   |  |-- max_tokens=2048\n</pre> <pre>[22:48:54] [INFO] \u26a1\ufe0f Processing llm-text column 'product_name' with 4 concurrent workers\n</pre> <pre>[22:48:54] [INFO] \u23f1\ufe0f llm-text column 'product_name' will report progress after each record\n</pre> <pre>[22:48:54] [INFO]   |-- \ud83d\ude34 llm-text column 'product_name' progress: 1/10 (10%) complete, 1 ok, 0 failed, 3.68 rec/s, eta 2.4s\n</pre> <pre>[22:48:54] [INFO]   |-- \ud83d\ude34 llm-text column 'product_name' progress: 2/10 (20%) complete, 2 ok, 0 failed, 6.15 rec/s, eta 1.3s\n</pre> <pre>[22:48:54] [INFO]   |-- \ud83e\udd71 llm-text column 'product_name' progress: 3/10 (30%) complete, 3 ok, 0 failed, 9.04 rec/s, eta 0.8s\n</pre> <pre>[22:48:54] [INFO]   |-- \ud83e\udd71 llm-text column 'product_name' progress: 4/10 (40%) complete, 4 ok, 0 failed, 11.09 rec/s, eta 0.5s\n</pre> <pre>[22:48:54] [INFO]   |-- \ud83d\ude10 llm-text column 'product_name' progress: 5/10 (50%) complete, 5 ok, 0 failed, 9.33 rec/s, eta 0.5s\n</pre> <pre>[22:48:54] [INFO]   |-- \ud83d\ude10 llm-text column 'product_name' progress: 6/10 (60%) complete, 6 ok, 0 failed, 9.33 rec/s, eta 0.4s\n</pre> <pre>[22:48:54] [INFO]   |-- \ud83d\ude10 llm-text column 'product_name' progress: 7/10 (70%) complete, 7 ok, 0 failed, 10.08 rec/s, eta 0.3s\n</pre> <pre>[22:48:54] [INFO]   |-- \ud83d\ude0a llm-text column 'product_name' progress: 8/10 (80%) complete, 8 ok, 0 failed, 10.18 rec/s, eta 0.2s\n</pre> <pre>[22:48:55] [INFO]   |-- \ud83d\ude0a llm-text column 'product_name' progress: 9/10 (90%) complete, 9 ok, 0 failed, 9.61 rec/s, eta 0.1s\n</pre> <pre>[22:48:56] [INFO]   |-- \ud83e\udd29 llm-text column 'product_name' progress: 10/10 (100%) complete, 10 ok, 0 failed, 4.80 rec/s, eta 0.0s\n</pre> <pre>[22:48:56] [INFO] \ud83d\udcdd llm-text model config for column 'customer_review'\n</pre> <pre>[22:48:56] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:48:56] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:48:56] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:48:56] [INFO]   |-- inference parameters:\n</pre> <pre>[22:48:56] [INFO]   |  |-- generation_type=chat-completion\n</pre> <pre>[22:48:56] [INFO]   |  |-- max_parallel_requests=4\n</pre> <pre>[22:48:56] [INFO]   |  |-- extra_body={'chat_template_kwargs': {'enable_thinking': False}}\n</pre> <pre>[22:48:56] [INFO]   |  |-- temperature=1.00\n</pre> <pre>[22:48:56] [INFO]   |  |-- top_p=1.00\n</pre> <pre>[22:48:56] [INFO]   |  |-- max_tokens=2048\n</pre> <pre>[22:48:56] [INFO] \u26a1\ufe0f Processing llm-text column 'customer_review' with 4 concurrent workers\n</pre> <pre>[22:48:56] [INFO] \u23f1\ufe0f llm-text column 'customer_review' will report progress after each record\n</pre> <pre>[22:48:56] [INFO]   |-- \ud83e\udd5a llm-text column 'customer_review' progress: 1/10 (10%) complete, 1 ok, 0 failed, 2.20 rec/s, eta 4.1s\n</pre> <pre>[22:48:57] [INFO]   |-- \ud83e\udd5a llm-text column 'customer_review' progress: 2/10 (20%) complete, 2 ok, 0 failed, 2.20 rec/s, eta 3.6s\n</pre> <pre>[22:48:57] [INFO]   |-- \ud83d\udc23 llm-text column 'customer_review' progress: 3/10 (30%) complete, 3 ok, 0 failed, 1.93 rec/s, eta 3.6s\n</pre> <pre>[22:48:58] [INFO]   |-- \ud83d\udc23 llm-text column 'customer_review' progress: 4/10 (40%) complete, 4 ok, 0 failed, 1.95 rec/s, eta 3.1s\n</pre> <pre>[22:48:58] [INFO]   |-- \ud83d\udc25 llm-text column 'customer_review' progress: 5/10 (50%) complete, 5 ok, 0 failed, 2.25 rec/s, eta 2.2s\n</pre> <pre>[22:48:59] [INFO]   |-- \ud83d\udc25 llm-text column 'customer_review' progress: 6/10 (60%) complete, 6 ok, 0 failed, 2.13 rec/s, eta 1.9s\n</pre> <pre>[22:48:59] [INFO]   |-- \ud83d\udc25 llm-text column 'customer_review' progress: 7/10 (70%) complete, 7 ok, 0 failed, 2.47 rec/s, eta 1.2s\n</pre> <pre>[22:48:59] [INFO]   |-- \ud83d\udc24 llm-text column 'customer_review' progress: 8/10 (80%) complete, 8 ok, 0 failed, 2.41 rec/s, eta 0.8s\n</pre> <pre>[22:48:59] [INFO]   |-- \ud83d\udc24 llm-text column 'customer_review' progress: 9/10 (90%) complete, 9 ok, 0 failed, 2.60 rec/s, eta 0.4s\n</pre> <pre>[22:48:59] [INFO]   |-- \ud83d\udc14 llm-text column 'customer_review' progress: 10/10 (100%) complete, 10 ok, 0 failed, 2.86 rec/s, eta 0.0s\n</pre> <pre>[22:48:59] [INFO] \ud83d\udcca Model usage summary:\n</pre> <pre>[22:48:59] [INFO]   |-- model: nvidia/nemotron-3-nano-30b-a3b\n</pre> <pre>[22:48:59] [INFO]   |-- tokens: input=1987, output=2050, total=4037, tps=682\n</pre> <pre>[22:48:59] [INFO]   |-- requests: success=20, failed=0, total=20, rpm=202\n</pre> <pre>[22:48:59] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:48:59] [INFO]   |-- \ud83c\udfb2 column: 'product_category'\n</pre> <pre>[22:48:59] [INFO]   |-- \ud83c\udfb2 column: 'product_subcategory'\n</pre> <pre>[22:48:59] [INFO]   |-- \ud83c\udfb2 column: 'target_age_range'\n</pre> <pre>[22:49:00] [INFO]   |-- \ud83c\udfb2 column: 'customer'\n</pre> <pre>[22:49:00] [INFO]   |-- \ud83c\udfb2 column: 'number_of_stars'\n</pre> <pre>[22:49:00] [INFO]   |-- \ud83c\udfb2 column: 'review_style'\n</pre> <pre>[22:49:00] [INFO]   |-- \ud83d\udcdd column: 'product_name'\n</pre> <pre>[22:49:00] [INFO]   |-- \ud83d\udcdd column: 'customer_review'\n</pre> In\u00a0[14]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() Out[14]: product_category product_subcategory target_age_range customer number_of_stars review_style product_name customer_review 0 Clothing Winter Coats 65+ {'age': 29, 'bachelors_field': 'education', 'b... 2 structured with bullet points Senior Mobility Coat - **Age &amp; Location:** 29, Mullenport, New York... 1 Home &amp; Kitchen Decor 25-35 {'age': 25, 'bachelors_field': 'no_degree', 'b... 3 rambling MoodRibbon Light\u0441urve Honestly I wasn't sure what to expect with the... 2 Books Textbooks 35-50 {'age': 58, 'bachelors_field': 'no_degree', 'b... 3 structured with bullet points Smart Scholar Textbook Hub - **Rating**: \u2605\u2605\u2605\u2606\u2606 (3 stars)   - **Pros**:   ... 3 Home &amp; Kitchen Organization 35-50 {'age': 50, 'bachelors_field': 'arts_humanitie... 2 brief ClearView Drawer Organizer 2 stars \u2013 Poor fit and flimsy construction; wo... 4 Clothing Activewear 65+ {'age': 69, 'bachelors_field': 'business', 'bi... 1 detailed SilverFlex Stretch Tee The SilverFlex Stretch Tee feels like buying a... In\u00a0[15]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 10                              \u2502 8                               \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83c\udfb2 Sampler Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                    \u2503       data type \u2503            number unique values \u2503               sampler type \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_category               \u2502          string \u2502                       3 (30.0%) \u2502                   category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product_subcategory            \u2502          string \u2502                       7 (70.0%) \u2502                subcategory \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 target_age_range               \u2502          string \u2502                       5 (50.0%) \u2502                   category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer                       \u2502            dict \u2502                     10 (100.0%) \u2502          person_from_faker \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 number_of_stars                \u2502             int \u2502                       3 (30.0%) \u2502                    uniform \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 review_style                   \u2502          string \u2502                       4 (40.0%) \u2502                   category \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83d\udcdd LLM-Text Columns                                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                       \u2503               \u2503                            \u2503     prompt tokens \u2503      completion tokens \u2503\n\u2503 column name           \u2503     data type \u2503       number unique values \u2503        per record \u2503             per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_name          \u2502        string \u2502                10 (100.0%) \u2502      74.0 +/- 0.8 \u2502           4.5 +/- 64.0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_review       \u2502        string \u2502                10 (100.0%) \u2502     69.5 +/- 60.7 \u2502        131.0 +/- 136.3 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre>"},{"location":"notebooks/1-the-basics/#data-designer-tutorial-the-basics","title":"\ud83c\udfa8 Data Designer Tutorial: The Basics\u00b6","text":""},{"location":"notebooks/1-the-basics/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>This notebook demonstrates the basics of Data Designer by generating a simple product review dataset.</p>"},{"location":"notebooks/1-the-basics/#import-data-designer","title":"\ud83d\udce6 Import Data Designer\u00b6","text":"<ul> <li><p><code>data_designer.config</code> provides access to the configuration API.</p> </li> <li><p><code>DataDesigner</code> is the main interface for data generation.</p> </li> </ul>"},{"location":"notebooks/1-the-basics/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebooks/1-the-basics/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebooks/1-the-basics/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebooks/1-the-basics/#getting-started-with-sampler-columns","title":"\ud83c\udfb2 Getting started with sampler columns\u00b6","text":"<ul> <li><p>Sampler columns offer non-LLM based generation of synthetic data.</p> </li> <li><p>They are particularly useful for steering the diversity of the generated data, as we demonstrate below.</p> </li> </ul> <p>You can view available samplers using the config builder's <code>info</code> property:</p>"},{"location":"notebooks/1-the-basics/#llm-generated-columns","title":"\ud83e\udd9c LLM-generated columns\u00b6","text":"<ul> <li><p>The real power of Data Designer comes from leveraging LLMs to generate text, code, and structured data.</p> </li> <li><p>When prompting the LLM, we can use Jinja templating to reference other columns in the dataset.</p> </li> <li><p>As we see below, nested json fields can be accessed using dot notation.</p> </li> </ul>"},{"location":"notebooks/1-the-basics/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebooks/1-the-basics/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebooks/1-the-basics/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebooks/1-the-basics/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Now that you've seen the basics of Data Designer, check out the following notebooks to learn more about:</p> <ul> <li><p>Structured outputs and jinja expressions</p> </li> <li><p>Seeding synthetic data generation with an external dataset</p> </li> <li><p>Providing images as context</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/","title":"Structured Outputs and Jinja Expressions","text":"In\u00a0[1]: Copied! <pre>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n</pre> import data_designer.config as dd from data_designer.interface import DataDesigner In\u00a0[2]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[3]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    dd.ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     dd.ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=dd.ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[4]: Copied! <pre>config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[5]: Copied! <pre>from decimal import Decimal\nfrom typing import Literal\n\nfrom pydantic import BaseModel, Field\n\n\n# We define a Product schema so that the name, description, and price are generated\n# in one go, with the types and constraints specified.\nclass Product(BaseModel):\n    name: str = Field(description=\"The name of the product\")\n    description: str = Field(description=\"A description of the product\")\n    price: Decimal = Field(description=\"The price of the product\", ge=10, le=1000, decimal_places=2)\n\n\nclass ProductReview(BaseModel):\n    rating: int = Field(description=\"The rating of the product\", ge=1, le=5)\n    customer_mood: Literal[\"irritated\", \"mad\", \"happy\", \"neutral\", \"excited\"] = Field(\n        description=\"The mood of the customer\"\n    )\n    review: str = Field(description=\"A review of the product\")\n</pre> from decimal import Decimal from typing import Literal  from pydantic import BaseModel, Field   # We define a Product schema so that the name, description, and price are generated # in one go, with the types and constraints specified. class Product(BaseModel):     name: str = Field(description=\"The name of the product\")     description: str = Field(description=\"A description of the product\")     price: Decimal = Field(description=\"The price of the product\", ge=10, le=1000, decimal_places=2)   class ProductReview(BaseModel):     rating: int = Field(description=\"The rating of the product\", ge=1, le=5)     customer_mood: Literal[\"irritated\", \"mad\", \"happy\", \"neutral\", \"excited\"] = Field(         description=\"The mood of the customer\"     )     review: str = Field(description=\"A review of the product\") <p>Next, let's design our product review dataset using a few more tricks compared to the previous notebook.</p> In\u00a0[6]: Copied! <pre># Since we often only want a few attributes from Person objects, we can\n# set drop=True in the column config to drop the column from the final dataset.\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"customer\",\n        sampler_type=dd.SamplerType.PERSON_FROM_FAKER,\n        params=dd.PersonFromFakerSamplerParams(),\n        drop=True,\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"product_category\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\n                \"Electronics\",\n                \"Clothing\",\n                \"Home &amp; Kitchen\",\n                \"Books\",\n                \"Home Office\",\n            ],\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"product_subcategory\",\n        sampler_type=dd.SamplerType.SUBCATEGORY,\n        params=dd.SubcategorySamplerParams(\n            category=\"product_category\",\n            values={\n                \"Electronics\": [\n                    \"Smartphones\",\n                    \"Laptops\",\n                    \"Headphones\",\n                    \"Cameras\",\n                    \"Accessories\",\n                ],\n                \"Clothing\": [\n                    \"Men's Clothing\",\n                    \"Women's Clothing\",\n                    \"Winter Coats\",\n                    \"Activewear\",\n                    \"Accessories\",\n                ],\n                \"Home &amp; Kitchen\": [\n                    \"Appliances\",\n                    \"Cookware\",\n                    \"Furniture\",\n                    \"Decor\",\n                    \"Organization\",\n                ],\n                \"Books\": [\n                    \"Fiction\",\n                    \"Non-Fiction\",\n                    \"Self-Help\",\n                    \"Textbooks\",\n                    \"Classics\",\n                ],\n                \"Home Office\": [\n                    \"Desks\",\n                    \"Chairs\",\n                    \"Storage\",\n                    \"Office Supplies\",\n                    \"Lighting\",\n                ],\n            },\n        ),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"target_age_range\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),\n    )\n)\n\n# Sampler columns support conditional params, which are used if the condition is met.\n# In this example, we set the review style to rambling if the target age range is 18-25.\n# Note conditional parameters are only supported for Sampler column types.\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"review_style\",\n        sampler_type=dd.SamplerType.CATEGORY,\n        params=dd.CategorySamplerParams(\n            values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],\n            weights=[1, 2, 2, 1],\n        ),\n        conditional_params={\n            \"target_age_range == '18-25'\": dd.CategorySamplerParams(values=[\"rambling\"]),\n        },\n    )\n)\n\n# Optionally validate that the columns are configured correctly.\ndata_designer.validate(config_builder)\n</pre> # Since we often only want a few attributes from Person objects, we can # set drop=True in the column config to drop the column from the final dataset. config_builder.add_column(     dd.SamplerColumnConfig(         name=\"customer\",         sampler_type=dd.SamplerType.PERSON_FROM_FAKER,         params=dd.PersonFromFakerSamplerParams(),         drop=True,     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"product_category\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(             values=[                 \"Electronics\",                 \"Clothing\",                 \"Home &amp; Kitchen\",                 \"Books\",                 \"Home Office\",             ],         ),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"product_subcategory\",         sampler_type=dd.SamplerType.SUBCATEGORY,         params=dd.SubcategorySamplerParams(             category=\"product_category\",             values={                 \"Electronics\": [                     \"Smartphones\",                     \"Laptops\",                     \"Headphones\",                     \"Cameras\",                     \"Accessories\",                 ],                 \"Clothing\": [                     \"Men's Clothing\",                     \"Women's Clothing\",                     \"Winter Coats\",                     \"Activewear\",                     \"Accessories\",                 ],                 \"Home &amp; Kitchen\": [                     \"Appliances\",                     \"Cookware\",                     \"Furniture\",                     \"Decor\",                     \"Organization\",                 ],                 \"Books\": [                     \"Fiction\",                     \"Non-Fiction\",                     \"Self-Help\",                     \"Textbooks\",                     \"Classics\",                 ],                 \"Home Office\": [                     \"Desks\",                     \"Chairs\",                     \"Storage\",                     \"Office Supplies\",                     \"Lighting\",                 ],             },         ),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"target_age_range\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(values=[\"18-25\", \"25-35\", \"35-50\", \"50-65\", \"65+\"]),     ) )  # Sampler columns support conditional params, which are used if the condition is met. # In this example, we set the review style to rambling if the target age range is 18-25. # Note conditional parameters are only supported for Sampler column types. config_builder.add_column(     dd.SamplerColumnConfig(         name=\"review_style\",         sampler_type=dd.SamplerType.CATEGORY,         params=dd.CategorySamplerParams(             values=[\"rambling\", \"brief\", \"detailed\", \"structured with bullet points\"],             weights=[1, 2, 2, 1],         ),         conditional_params={             \"target_age_range == '18-25'\": dd.CategorySamplerParams(values=[\"rambling\"]),         },     ) )  # Optionally validate that the columns are configured correctly. data_designer.validate(config_builder) <pre>[22:49:07] [INFO] \u2705 Validation passed\n</pre> <p>Next, we will use more advanced Jinja expressions to create new columns.</p> <p>Jinja expressions let you:</p> <ul> <li><p>Access nested attributes: <code>{{ customer.first_name }}</code></p> </li> <li><p>Combine values: <code>{{ customer.first_name }} {{ customer.last_name }}</code></p> </li> <li><p>Use conditional logic: <code>{% if condition %}...{% endif %}</code></p> </li> </ul> In\u00a0[7]: Copied! <pre># We can create new columns using Jinja expressions that reference\n# existing columns, including attributes of nested objects.\nconfig_builder.add_column(\n    dd.ExpressionColumnConfig(name=\"customer_name\", expr=\"{{ customer.first_name }} {{ customer.last_name }}\")\n)\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"customer_age\", expr=\"{{ customer.age }}\"))\n\nconfig_builder.add_column(\n    dd.LLMStructuredColumnConfig(\n        name=\"product\",\n        prompt=(\n            \"Create a product in the '{{ product_category }}' category, focusing on products  \"\n            \"related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"\n            \"{{ target_age_range }} years old. The product should be priced between $10 and $1000.\"\n        ),\n        output_format=Product,\n        model_alias=MODEL_ALIAS,\n    )\n)\n\n# We can even use if/else logic in our Jinja expressions to create more complex prompt patterns.\nconfig_builder.add_column(\n    dd.LLMStructuredColumnConfig(\n        name=\"customer_review\",\n        prompt=(\n            \"Your task is to write a review for the following product:\\n\\n\"\n            \"Product Name: {{ product.name }}\\n\"\n            \"Product Description: {{ product.description }}\\n\"\n            \"Price: {{ product.price }}\\n\\n\"\n            \"Imagine your name is {{ customer_name }} and you are from {{ customer.city }}, {{ customer.state }}. \"\n            \"Write the review in a style that is '{{ review_style }}'.\"\n            \"{% if target_age_range == '18-25' %}\"\n            \"Make sure the review is more informal and conversational.\\n\"\n            \"{% else %}\"\n            \"Make sure the review is more formal and structured.\\n\"\n            \"{% endif %}\"\n            \"The review field should contain only the review, no other text.\"\n        ),\n        output_format=ProductReview,\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> # We can create new columns using Jinja expressions that reference # existing columns, including attributes of nested objects. config_builder.add_column(     dd.ExpressionColumnConfig(name=\"customer_name\", expr=\"{{ customer.first_name }} {{ customer.last_name }}\") )  config_builder.add_column(dd.ExpressionColumnConfig(name=\"customer_age\", expr=\"{{ customer.age }}\"))  config_builder.add_column(     dd.LLMStructuredColumnConfig(         name=\"product\",         prompt=(             \"Create a product in the '{{ product_category }}' category, focusing on products  \"             \"related to '{{ product_subcategory }}'. The target age range of the ideal customer is \"             \"{{ target_age_range }} years old. The product should be priced between $10 and $1000.\"         ),         output_format=Product,         model_alias=MODEL_ALIAS,     ) )  # We can even use if/else logic in our Jinja expressions to create more complex prompt patterns. config_builder.add_column(     dd.LLMStructuredColumnConfig(         name=\"customer_review\",         prompt=(             \"Your task is to write a review for the following product:\\n\\n\"             \"Product Name: {{ product.name }}\\n\"             \"Product Description: {{ product.description }}\\n\"             \"Price: {{ product.price }}\\n\\n\"             \"Imagine your name is {{ customer_name }} and you are from {{ customer.city }}, {{ customer.state }}. \"             \"Write the review in a style that is '{{ review_style }}'.\"             \"{% if target_age_range == '18-25' %}\"             \"Make sure the review is more informal and conversational.\\n\"             \"{% else %}\"             \"Make sure the review is more formal and structured.\\n\"             \"{% endif %}\"             \"The review field should contain only the review, no other text.\"         ),         output_format=ProductReview,         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) <pre>[22:49:07] [INFO] \u2705 Validation passed\n</pre> In\u00a0[8]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) <pre>[22:49:07] [INFO] \ud83d\uddbc\ufe0f Preview generation in progress\n</pre> <pre>[22:49:07] [INFO] \u2705 Validation passed\n</pre> <pre>[22:49:07] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:49:07] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:49:07] [INFO]   |-- \ud83d\udc40 Checking 'nvidia/nemotron-3-nano-30b-a3b' in provider named 'nvidia' for model alias 'nemotron-nano-v3'...\n</pre> <pre>[22:49:07] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:49:07] [INFO] \ud83c\udfb2 Preparing samplers to generate 2 records across 5 columns\n</pre> <pre>[22:49:07] [INFO] \ud83e\udde9 Generating column `customer_name` from expression\n</pre> <pre>[22:49:07] [INFO] \ud83e\udde9 Generating column `customer_age` from expression\n</pre> <pre>[22:49:07] [INFO] \ud83d\uddc2\ufe0f llm-structured model config for column 'product'\n</pre> <pre>[22:49:07] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:49:07] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:49:07] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:49:07] [INFO]   |-- inference parameters:\n</pre> <pre>[22:49:07] [INFO]   |  |-- generation_type=chat-completion\n</pre> <pre>[22:49:07] [INFO]   |  |-- max_parallel_requests=4\n</pre> <pre>[22:49:07] [INFO]   |  |-- extra_body={'chat_template_kwargs': {'enable_thinking': False}}\n</pre> <pre>[22:49:07] [INFO]   |  |-- temperature=1.00\n</pre> <pre>[22:49:07] [INFO]   |  |-- top_p=1.00\n</pre> <pre>[22:49:07] [INFO]   |  |-- max_tokens=2048\n</pre> <pre>[22:49:07] [INFO] \u26a1\ufe0f Processing llm-structured column 'product' with 4 concurrent workers\n</pre> <pre>[22:49:07] [INFO] \u23f1\ufe0f llm-structured column 'product' will report progress after each record\n</pre> <pre>[22:49:08] [INFO]   |-- \ud83d\ude38 llm-structured column 'product' progress: 1/2 (50%) complete, 1 ok, 0 failed, 1.40 rec/s, eta 0.7s\n</pre> <pre>[22:49:08] [INFO]   |-- \ud83e\udd81 llm-structured column 'product' progress: 2/2 (100%) complete, 2 ok, 0 failed, 1.90 rec/s, eta 0.0s\n</pre> <pre>[22:49:08] [INFO] \ud83d\uddc2\ufe0f llm-structured model config for column 'customer_review'\n</pre> <pre>[22:49:08] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:49:08] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:49:08] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:49:08] [INFO]   |-- inference parameters:\n</pre> <pre>[22:49:08] [INFO]   |  |-- generation_type=chat-completion\n</pre> <pre>[22:49:08] [INFO]   |  |-- max_parallel_requests=4\n</pre> <pre>[22:49:08] [INFO]   |  |-- extra_body={'chat_template_kwargs': {'enable_thinking': False}}\n</pre> <pre>[22:49:08] [INFO]   |  |-- temperature=1.00\n</pre> <pre>[22:49:08] [INFO]   |  |-- top_p=1.00\n</pre> <pre>[22:49:08] [INFO]   |  |-- max_tokens=2048\n</pre> <pre>[22:49:08] [INFO] \u26a1\ufe0f Processing llm-structured column 'customer_review' with 4 concurrent workers\n</pre> <pre>[22:49:08] [INFO] \u23f1\ufe0f llm-structured column 'customer_review' will report progress after each record\n</pre> <pre>[22:49:10] [INFO]   |-- \ud83c\udf17 llm-structured column 'customer_review' progress: 1/2 (50%) complete, 1 ok, 0 failed, 0.65 rec/s, eta 1.5s\n</pre> <pre>[22:49:10] [INFO]   |-- \ud83c\udf15 llm-structured column 'customer_review' progress: 2/2 (100%) complete, 2 ok, 0 failed, 1.20 rec/s, eta 0.0s\n</pre> <pre>[22:49:10] [INFO] \ud83d\udcca Model usage summary:\n</pre> <pre>[22:49:10] [INFO]   |-- model: nvidia/nemotron-3-nano-30b-a3b\n</pre> <pre>[22:49:10] [INFO]   |-- tokens: input=1334, output=735, total=2069, tps=676\n</pre> <pre>[22:49:10] [INFO]   |-- requests: success=4, failed=0, total=4, rpm=78\n</pre> <pre>[22:49:10] [INFO] \ud83d\ude48 Dropping columns: ['customer']\n</pre> <pre>[22:49:10] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:49:10] [INFO]   |-- \ud83c\udfb2 column: 'product_category'\n</pre> <pre>[22:49:10] [INFO]   |-- \ud83c\udfb2 column: 'product_subcategory'\n</pre> <pre>[22:49:10] [INFO]   |-- \ud83c\udfb2 column: 'target_age_range'\n</pre> <pre>[22:49:10] [INFO]   |-- \ud83c\udfb2 column: 'review_style'\n</pre> <pre>[22:49:10] [INFO]   |-- \ud83e\udde9 column: 'customer_name'\n</pre> <pre>[22:49:10] [INFO]   |-- \ud83e\udde9 column: 'customer_age'\n</pre> <pre>[22:49:10] [INFO]   |-- \ud83d\uddc2\ufe0f column: 'product'\n</pre> <pre>[22:49:10] [INFO]   |-- \ud83d\uddc2\ufe0f column: 'customer_review'\n</pre> <pre>[22:49:10] [INFO] \ud83c\udf7e Preview complete!\n</pre> In\u00a0[9]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() <pre>                                                                                                                   \n                                                 Generated Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name                \u2503 Value                                                                                     \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_category    \u2502 Clothing                                                                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product_subcategory \u2502 Winter Coats                                                                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 target_age_range    \u2502 35-50                                                                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 review_style        \u2502 rambling                                                                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_name       \u2502 Kristen Patterson                                                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_age        \u2502 103                                                                                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product             \u2502 {                                                                                         \u2502\n\u2502                     \u2502     'name': 'Eternal Wool Coated Parka',                                                  \u2502\n\u2502                     \u2502     'description': 'A luxurious, insulated parkas designed for the discerning             \u2502\n\u2502                     \u2502 professional aged 35-50. Crafted from sustainably sourced, high-weight recycled wool      \u2502\n\u2502                     \u2502 blend, it features a detachable hood with integrated down fill, adjustable cuffs, and     \u2502\n\u2502                     \u2502 hidden interior pockets. The weather-resistant coating ensures protection from snow and   \u2502\n\u2502                     \u2502 wind while maintaining a sleek, timeless silhouette suitable for office to weekend        \u2502\n\u2502                     \u2502 wear.',                                                                                   \u2502\n\u2502                     \u2502     'price': 275.0                                                                        \u2502\n\u2502                     \u2502 }                                                                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_review     \u2502 {                                                                                         \u2502\n\u2502                     \u2502     'rating': 5,                                                                          \u2502\n\u2502                     \u2502     'customer_mood': 'happy',                                                             \u2502\n\u2502                     \u2502     'review': 'I was truly delighted by how this Eternal Wool Coated Parka fits my        \u2502\n\u2502                     \u2502 lifestyle so effortlessly, it feels as if the garment was tailor-made for me, a           \u2502\n\u2502                     \u2502 professional in my late thirties who values understated elegance and practical            \u2502\n\u2502                     \u2502 performance, and upon first unboxing the weight of the recycled wool blend spoke of       \u2502\n\u2502                     \u2502 durability without sacrificing a whisper of softness, the detachable hood with its        \u2502\n\u2502                     \u2502 concealed down fill elevated my early winter commutes by offering an unexpected burst of  \u2502\n\u2502                     \u2502 warmth that never felt bulky, the adjustable cuffs allow me to fine\u2011tune the seal against \u2502\n\u2502                     \u2502 the wind while still presenting a clean line that pairs beautifully with my office        \u2502\n\u2502                     \u2502 attire, hidden interior pockets provide a discreet sanctuary for my tablet and travel     \u2502\n\u2502                     \u2502 documents, and the weather\u2011resistant coating has proven its mettle through several        \u2502\n\u2502                     \u2502 snow\u2011laden evenings without a hint of seepage, all of these thoughtful details converge   \u2502\n\u2502                     \u2502 into a piece that not only shields me from the elements but also affords me a sense of    \u2502\n\u2502                     \u2502 quiet confidence as I transition from boardroom meetings to weekend strolls through the   \u2502\n\u2502                     \u2502 historic streets of Waltonstad'                                                           \u2502\n\u2502                     \u2502 }                                                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                    [index: 0]                                                     \n</pre> In\u00a0[10]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset Out[10]: product_category product_subcategory target_age_range review_style customer_name customer_age product customer_review 0 Clothing Winter Coats 35-50 rambling Kristen Patterson 103 {'name': 'Eternal Wool Coated Parka', 'descrip... {'rating': 5, 'customer_mood': 'happy', 'revie... 1 Clothing Accessories 18-25 rambling Laura Hernandez 67 {'name': 'Urban Pulse Crossbody Backpack', 'de... {'rating': 5, 'customer_mood': 'happy', 'revie... In\u00a0[11]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2                               \u2502 8                               \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83c\udfb2 Sampler Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                      \u2503        data type \u2503               number unique values \u2503         sampler type \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_category                 \u2502           string \u2502                          1 (50.0%) \u2502             category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product_subcategory              \u2502           string \u2502                         2 (100.0%) \u2502          subcategory \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 target_age_range                 \u2502           string \u2502                         2 (100.0%) \u2502             category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 review_style                     \u2502           string \u2502                          1 (50.0%) \u2502             category \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                             \ud83d\uddc2\ufe0f LLM-Structured Columns                                             \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                       \u2503               \u2503                            \u2503     prompt tokens \u2503      completion tokens \u2503\n\u2503 column name           \u2503     data type \u2503       number unique values \u2503        per record \u2503             per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product               \u2502          dict \u2502                 2 (100.0%) \u2502     265.5 +/- 0.5 \u2502           94.5 +/- 5.0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_review       \u2502          dict \u2502                 2 (100.0%) \u2502     349.0 +/- 3.0 \u2502         238.5 +/- 10.6 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                               \ud83e\udde9 Expression Columns                                               \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                       \u2503                data type \u2503                             number unique values \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 customer_name                     \u2502                   string \u2502                                       2 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_age                      \u2502                   string \u2502                                       2 (100.0%) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> In\u00a0[12]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-2\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-2\") <pre>[22:49:10] [INFO] \ud83c\udfa8 Creating Data Designer dataset\n</pre> <pre>[22:49:10] [INFO] \u2705 Validation passed\n</pre> <pre>[22:49:10] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:49:10] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:49:10] [INFO]   |-- \ud83d\udc40 Checking 'nvidia/nemotron-3-nano-30b-a3b' in provider named 'nvidia' for model alias 'nemotron-nano-v3'...\n</pre> <pre>[22:49:10] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:49:10] [INFO] \u23f3 Processing batch 1 of 1\n</pre> <pre>[22:49:10] [INFO] \ud83c\udfb2 Preparing samplers to generate 10 records across 5 columns\n</pre> <pre>[22:49:11] [INFO] \ud83e\udde9 Generating column `customer_name` from expression\n</pre> <pre>[22:49:11] [INFO] \ud83e\udde9 Generating column `customer_age` from expression\n</pre> <pre>[22:49:11] [INFO] \ud83d\uddc2\ufe0f llm-structured model config for column 'product'\n</pre> <pre>[22:49:11] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:49:11] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:49:11] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:49:11] [INFO]   |-- inference parameters:\n</pre> <pre>[22:49:11] [INFO]   |  |-- generation_type=chat-completion\n</pre> <pre>[22:49:11] [INFO]   |  |-- max_parallel_requests=4\n</pre> <pre>[22:49:11] [INFO]   |  |-- extra_body={'chat_template_kwargs': {'enable_thinking': False}}\n</pre> <pre>[22:49:11] [INFO]   |  |-- temperature=1.00\n</pre> <pre>[22:49:11] [INFO]   |  |-- top_p=1.00\n</pre> <pre>[22:49:11] [INFO]   |  |-- max_tokens=2048\n</pre> <pre>[22:49:11] [INFO] \u26a1\ufe0f Processing llm-structured column 'product' with 4 concurrent workers\n</pre> <pre>[22:49:11] [INFO] \u23f1\ufe0f llm-structured column 'product' will report progress after each record\n</pre> <pre>[22:49:11] [INFO]   |-- \ud83c\udf11 llm-structured column 'product' progress: 1/10 (10%) complete, 1 ok, 0 failed, 1.70 rec/s, eta 5.3s\n</pre> <pre>[22:49:11] [INFO]   |-- \ud83c\udf11 llm-structured column 'product' progress: 2/10 (20%) complete, 2 ok, 0 failed, 2.69 rec/s, eta 3.0s\n</pre> <pre>[22:49:11] [INFO]   |-- \ud83c\udf18 llm-structured column 'product' progress: 3/10 (30%) complete, 3 ok, 0 failed, 3.60 rec/s, eta 1.9s\n</pre> <pre>[22:49:12] [INFO]   |-- \ud83c\udf18 llm-structured column 'product' progress: 4/10 (40%) complete, 4 ok, 0 failed, 3.14 rec/s, eta 1.9s\n</pre> <pre>[22:49:12] [INFO]   |-- \ud83c\udf17 llm-structured column 'product' progress: 5/10 (50%) complete, 5 ok, 0 failed, 3.69 rec/s, eta 1.4s\n</pre> <pre>[22:49:12] [INFO]   |-- \ud83c\udf17 llm-structured column 'product' progress: 6/10 (60%) complete, 6 ok, 0 failed, 4.07 rec/s, eta 1.0s\n</pre> <pre>[22:49:12] [INFO]   |-- \ud83c\udf17 llm-structured column 'product' progress: 7/10 (70%) complete, 7 ok, 0 failed, 4.59 rec/s, eta 0.7s\n</pre> <pre>[22:49:13] [INFO]   |-- \ud83c\udf16 llm-structured column 'product' progress: 8/10 (80%) complete, 8 ok, 0 failed, 3.72 rec/s, eta 0.5s\n</pre> <pre>[22:49:13] [INFO]   |-- \ud83c\udf16 llm-structured column 'product' progress: 9/10 (90%) complete, 9 ok, 0 failed, 4.11 rec/s, eta 0.2s\n</pre> <pre>[22:49:13] [INFO] Unspecified property removed from data object: tags.\n</pre> <pre>[22:49:13] [INFO] 1 unspecified properties removed from data object.\n</pre> <pre>[22:49:13] [INFO]   |-- \ud83c\udf15 llm-structured column 'product' progress: 10/10 (100%) complete, 10 ok, 0 failed, 4.54 rec/s, eta 0.0s\n</pre> <pre>[22:49:13] [INFO] \ud83d\uddc2\ufe0f llm-structured model config for column 'customer_review'\n</pre> <pre>[22:49:13] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:49:13] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:49:13] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:49:13] [INFO]   |-- inference parameters:\n</pre> <pre>[22:49:13] [INFO]   |  |-- generation_type=chat-completion\n</pre> <pre>[22:49:13] [INFO]   |  |-- max_parallel_requests=4\n</pre> <pre>[22:49:13] [INFO]   |  |-- extra_body={'chat_template_kwargs': {'enable_thinking': False}}\n</pre> <pre>[22:49:13] [INFO]   |  |-- temperature=1.00\n</pre> <pre>[22:49:13] [INFO]   |  |-- top_p=1.00\n</pre> <pre>[22:49:13] [INFO]   |  |-- max_tokens=2048\n</pre> <pre>[22:49:13] [INFO] \u26a1\ufe0f Processing llm-structured column 'customer_review' with 4 concurrent workers\n</pre> <pre>[22:49:13] [INFO] \u23f1\ufe0f llm-structured column 'customer_review' will report progress after each record\n</pre> <pre>[22:49:14] [INFO]   |-- \ud83c\udf11 llm-structured column 'customer_review' progress: 1/10 (10%) complete, 1 ok, 0 failed, 1.12 rec/s, eta 8.1s\n</pre> <pre>[22:49:14] [INFO]   |-- \ud83c\udf11 llm-structured column 'customer_review' progress: 2/10 (20%) complete, 2 ok, 0 failed, 1.86 rec/s, eta 4.3s\n</pre> <pre>[22:49:14] [INFO]   |-- \ud83c\udf18 llm-structured column 'customer_review' progress: 3/10 (30%) complete, 3 ok, 0 failed, 2.53 rec/s, eta 2.8s\n</pre> <pre>[22:49:14] [INFO]   |-- \ud83c\udf18 llm-structured column 'customer_review' progress: 4/10 (40%) complete, 4 ok, 0 failed, 2.67 rec/s, eta 2.2s\n</pre> <pre>[22:49:14] [INFO]   |-- \ud83c\udf17 llm-structured column 'customer_review' progress: 5/10 (50%) complete, 5 ok, 0 failed, 3.00 rec/s, eta 1.7s\n</pre> <pre>[22:49:15] [INFO]   |-- \ud83c\udf17 llm-structured column 'customer_review' progress: 6/10 (60%) complete, 6 ok, 0 failed, 2.43 rec/s, eta 1.6s\n</pre> <pre>[22:49:15] [INFO]   |-- \ud83c\udf17 llm-structured column 'customer_review' progress: 7/10 (70%) complete, 7 ok, 0 failed, 2.67 rec/s, eta 1.1s\n</pre> <pre>[22:49:16] [INFO]   |-- \ud83c\udf16 llm-structured column 'customer_review' progress: 8/10 (80%) complete, 8 ok, 0 failed, 2.88 rec/s, eta 0.7s\n</pre> <pre>[22:49:16] [INFO]   |-- \ud83c\udf16 llm-structured column 'customer_review' progress: 9/10 (90%) complete, 9 ok, 0 failed, 2.84 rec/s, eta 0.4s\n</pre> <pre>[22:49:16] [INFO]   |-- \ud83c\udf15 llm-structured column 'customer_review' progress: 10/10 (100%) complete, 10 ok, 0 failed, 2.82 rec/s, eta 0.0s\n</pre> <pre>[22:49:17] [INFO] \ud83d\ude48 Dropping columns: ['customer']\n</pre> <pre>[22:49:17] [INFO] \ud83d\udcca Model usage summary:\n</pre> <pre>[22:49:17] [INFO]   |-- model: nvidia/nemotron-3-nano-30b-a3b\n</pre> <pre>[22:49:17] [INFO]   |-- tokens: input=6547, output=2850, total=9397, tps=1532\n</pre> <pre>[22:49:17] [INFO]   |-- requests: success=20, failed=0, total=20, rpm=195\n</pre> <pre>[22:49:17] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:49:17] [INFO]   |-- \ud83c\udfb2 column: 'product_category'\n</pre> <pre>[22:49:17] [INFO]   |-- \ud83c\udfb2 column: 'product_subcategory'\n</pre> <pre>[22:49:17] [INFO]   |-- \ud83c\udfb2 column: 'target_age_range'\n</pre> <pre>[22:49:17] [INFO]   |-- \ud83c\udfb2 column: 'review_style'\n</pre> <pre>[22:49:17] [INFO]   |-- \ud83e\udde9 column: 'customer_name'\n</pre> <pre>[22:49:17] [INFO]   |-- \ud83e\udde9 column: 'customer_age'\n</pre> <pre>[22:49:17] [INFO]   |-- \ud83d\uddc2\ufe0f column: 'product'\n</pre> <pre>[22:49:17] [INFO]   |-- \ud83d\uddc2\ufe0f column: 'customer_review'\n</pre> In\u00a0[13]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() Out[13]: product_category product_subcategory target_age_range review_style customer_name customer_age product customer_review 0 Books Textbooks 65+ brief Melanie Mendoza 25 {'description': 'A comprehensive large-print U... {'customer_mood': 'happy', 'rating': 5, 'revie... 1 Home Office Desks 65+ detailed Denise Sellers 72 {'description': 'A sleek, height\u2011adjustable de... {'customer_mood': 'happy', 'rating': 4, 'revie... 2 Electronics Headphones 50-65 detailed Andrew Brown 78 {'description': 'The SereneSound Pro combines ... {'customer_mood': 'happy', 'rating': 5, 'revie... 3 Home Office Desks 65+ brief Alison Allen 76 {'description': 'A sturdy, height\u2011adjustable l... {'customer_mood': 'happy', 'rating': 4, 'revie... 4 Books Fiction 35-50 brief Candace Carter 100 {'description': 'An evocative historical-ficti... {'customer_mood': 'happy', 'rating': 5, 'revie... In\u00a0[14]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 10                              \u2502 8                               \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83c\udfb2 Sampler Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                      \u2503        data type \u2503               number unique values \u2503         sampler type \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product_category                 \u2502           string \u2502                          4 (40.0%) \u2502             category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 product_subcategory              \u2502           string \u2502                          9 (90.0%) \u2502          subcategory \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 target_age_range                 \u2502           string \u2502                          5 (50.0%) \u2502             category \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 review_style                     \u2502           string \u2502                          3 (30.0%) \u2502             category \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                             \ud83d\uddc2\ufe0f LLM-Structured Columns                                             \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                      \u2503               \u2503                            \u2503       prompt tokens \u2503     completion tokens \u2503\n\u2503 column name          \u2503     data type \u2503       number unique values \u2503          per record \u2503            per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 product              \u2502          dict \u2502                10 (100.0%) \u2502       264.5 +/- 0.8 \u2502         77.5 +/- 21.1 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_review      \u2502          dict \u2502                10 (100.0%) \u2502      330.5 +/- 20.4 \u2502        187.0 +/- 80.2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                               \ud83e\udde9 Expression Columns                                               \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                       \u2503                data type \u2503                             number unique values \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 customer_name                     \u2502                   string \u2502                                      10 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_age                      \u2502                   string \u2502                                        9 (90.0%) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#data-designer-tutorial-structured-outputs-and-jinja-expressions","title":"\ud83c\udfa8 Data Designer Tutorial: Structured Outputs and Jinja Expressions\u00b6","text":""},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>In this notebook, we will continue our exploration of Data Designer, demonstrating more advanced data generation using structured outputs and Jinja expressions.</p> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#import-data-designer","title":"\ud83d\udce6 Import Data Designer\u00b6","text":"<ul> <li><p><code>data_designer.config</code> provides access to the configuration API.</p> </li> <li><p><code>DataDesigner</code> is the main interface for data generation.</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object that is used to interface with the library.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#designing-our-data","title":"\ud83e\uddd1\u200d\ud83c\udfa8 Designing our data\u00b6","text":"<ul> <li><p>We will again create a product review dataset, but this time we will use structured outputs and Jinja expressions.</p> </li> <li><p>Structured outputs let you specify the exact schema of the data you want to generate.</p> </li> <li><p>Data Designer supports schemas specified using either json schema or Pydantic data models (recommended).</p> </li> </ul> <p>We'll define our structured outputs using Pydantic data models</p> <p>\ud83d\udca1 Why Pydantic?</p> <ul> <li><p>Pydantic models provide better IDE support and type validation.</p> </li> <li><p>They are more Pythonic than raw JSON schemas.</p> </li> <li><p>They integrate seamlessly with Data Designer's structured output system.</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebooks/2-structured-outputs-and-jinja-expressions/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Check out the following notebook to learn more about:</p> <ul> <li><p>Seeding synthetic data generation with an external dataset</p> </li> <li><p>Providing images as context</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/","title":"Seeding with an External Dataset","text":"In\u00a0[1]: Copied! <pre>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n</pre> import data_designer.config as dd from data_designer.interface import DataDesigner In\u00a0[2]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[3]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\n# The model ID is from build.nvidia.com.\nMODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"\n\n# We choose this alias to be descriptive for our use case.\nMODEL_ALIAS = \"nemotron-nano-v3\"\n\nmodel_configs = [\n    dd.ModelConfig(\n        alias=MODEL_ALIAS,\n        model=MODEL_ID,\n        provider=MODEL_PROVIDER,\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=1.0,\n            top_p=1.0,\n            max_tokens=2048,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        ),\n    )\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  # The model ID is from build.nvidia.com. MODEL_ID = \"nvidia/nemotron-3-nano-30b-a3b\"  # We choose this alias to be descriptive for our use case. MODEL_ALIAS = \"nemotron-nano-v3\"  model_configs = [     dd.ModelConfig(         alias=MODEL_ALIAS,         model=MODEL_ID,         provider=MODEL_PROVIDER,         inference_parameters=dd.ChatCompletionInferenceParams(             temperature=1.0,             top_p=1.0,             max_tokens=2048,             extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},         ),     ) ] In\u00a0[4]: Copied! <pre>config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[5]: Copied! <pre># Download sample dataset from Github\nimport urllib.request\n\nurl = \"https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/refs/heads/main/nemo/NeMo-Data-Designer/data/gretelai_symptom_to_diagnosis.csv\"\nlocal_filename, _ = urllib.request.urlretrieve(url, \"gretelai_symptom_to_diagnosis.csv\")\n\n# Seed datasets are passed as reference objects to the config builder.\nseed_source = dd.LocalFileSeedSource(path=local_filename)\n\nconfig_builder.with_seed_dataset(seed_source)\n</pre> # Download sample dataset from Github import urllib.request  url = \"https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/refs/heads/main/nemo/NeMo-Data-Designer/data/gretelai_symptom_to_diagnosis.csv\" local_filename, _ = urllib.request.urlretrieve(url, \"gretelai_symptom_to_diagnosis.csv\")  # Seed datasets are passed as reference objects to the config builder. seed_source = dd.LocalFileSeedSource(path=local_filename)  config_builder.with_seed_dataset(seed_source) Out[5]: <pre>DataDesignerConfigBuilder()\n</pre> In\u00a0[6]: Copied! <pre>config_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"patient_sampler\",\n        sampler_type=dd.SamplerType.PERSON_FROM_FAKER,\n        params=dd.PersonFromFakerSamplerParams(),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"doctor_sampler\",\n        sampler_type=dd.SamplerType.PERSON_FROM_FAKER,\n        params=dd.PersonFromFakerSamplerParams(),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"patient_id\",\n        sampler_type=dd.SamplerType.UUID,\n        params=dd.UUIDSamplerParams(\n            prefix=\"PT-\",\n            short_form=True,\n            uppercase=True,\n        ),\n    )\n)\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"first_name\", expr=\"{{ patient_sampler.first_name }}\"))\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"last_name\", expr=\"{{ patient_sampler.last_name }}\"))\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"dob\", expr=\"{{ patient_sampler.birth_date }}\"))\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"symptom_onset_date\",\n        sampler_type=dd.SamplerType.DATETIME,\n        params=dd.DatetimeSamplerParams(start=\"2024-01-01\", end=\"2024-12-31\"),\n    )\n)\n\nconfig_builder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"date_of_visit\",\n        sampler_type=dd.SamplerType.TIMEDELTA,\n        params=dd.TimeDeltaSamplerParams(dt_min=1, dt_max=30, reference_column_name=\"symptom_onset_date\"),\n    )\n)\n\nconfig_builder.add_column(dd.ExpressionColumnConfig(name=\"physician\", expr=\"Dr. {{ doctor_sampler.last_name }}\"))\n\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"physician_notes\",\n        prompt=\"\"\"\\\nYou are a primary-care physician who just had an appointment with {{ first_name }} {{ last_name }},\nwho has been struggling with symptoms from {{ diagnosis }} since {{ symptom_onset_date }}.\nThe date of today's visit is {{ date_of_visit }}.\n\n{{ patient_summary }}\n\nWrite careful notes about your visit with {{ first_name }},\nas Dr. {{ doctor_sampler.first_name }} {{ doctor_sampler.last_name }}.\n\nFormat the notes as a busy doctor might.\nRespond with only the notes, no other text.\n\"\"\",\n        model_alias=MODEL_ALIAS,\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> config_builder.add_column(     dd.SamplerColumnConfig(         name=\"patient_sampler\",         sampler_type=dd.SamplerType.PERSON_FROM_FAKER,         params=dd.PersonFromFakerSamplerParams(),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"doctor_sampler\",         sampler_type=dd.SamplerType.PERSON_FROM_FAKER,         params=dd.PersonFromFakerSamplerParams(),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"patient_id\",         sampler_type=dd.SamplerType.UUID,         params=dd.UUIDSamplerParams(             prefix=\"PT-\",             short_form=True,             uppercase=True,         ),     ) )  config_builder.add_column(dd.ExpressionColumnConfig(name=\"first_name\", expr=\"{{ patient_sampler.first_name }}\"))  config_builder.add_column(dd.ExpressionColumnConfig(name=\"last_name\", expr=\"{{ patient_sampler.last_name }}\"))  config_builder.add_column(dd.ExpressionColumnConfig(name=\"dob\", expr=\"{{ patient_sampler.birth_date }}\"))  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"symptom_onset_date\",         sampler_type=dd.SamplerType.DATETIME,         params=dd.DatetimeSamplerParams(start=\"2024-01-01\", end=\"2024-12-31\"),     ) )  config_builder.add_column(     dd.SamplerColumnConfig(         name=\"date_of_visit\",         sampler_type=dd.SamplerType.TIMEDELTA,         params=dd.TimeDeltaSamplerParams(dt_min=1, dt_max=30, reference_column_name=\"symptom_onset_date\"),     ) )  config_builder.add_column(dd.ExpressionColumnConfig(name=\"physician\", expr=\"Dr. {{ doctor_sampler.last_name }}\"))  config_builder.add_column(     dd.LLMTextColumnConfig(         name=\"physician_notes\",         prompt=\"\"\"\\ You are a primary-care physician who just had an appointment with {{ first_name }} {{ last_name }}, who has been struggling with symptoms from {{ diagnosis }} since {{ symptom_onset_date }}. The date of today's visit is {{ date_of_visit }}.  {{ patient_summary }}  Write careful notes about your visit with {{ first_name }}, as Dr. {{ doctor_sampler.first_name }} {{ doctor_sampler.last_name }}.  Format the notes as a busy doctor might. Respond with only the notes, no other text. \"\"\",         model_alias=MODEL_ALIAS,     ) )  data_designer.validate(config_builder) <pre>[22:49:23] [INFO] \u2705 Validation passed\n</pre> In\u00a0[7]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) <pre>[22:49:23] [INFO] \ud83e\uddd0 Preview generation in progress\n</pre> <pre>[22:49:23] [INFO] \u2705 Validation passed\n</pre> <pre>[22:49:23] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:49:23] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:49:23] [INFO]   |-- \ud83d\udc40 Checking 'nvidia/nemotron-3-nano-30b-a3b' in provider named 'nvidia' for model alias 'nemotron-nano-v3'...\n</pre> <pre>[22:49:23] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:49:23] [INFO] \ud83c\udf31 Sampling 2 records from seed dataset\n</pre> <pre>[22:49:23] [INFO]   |-- seed dataset size: 820 records\n</pre> <pre>[22:49:23] [INFO]   |-- sampling strategy: ordered\n</pre> <pre>[22:49:23] [INFO] \ud83c\udfb2 Preparing samplers to generate 2 records across 5 columns\n</pre> <pre>[22:49:23] [INFO] (\ud83d\udcbe + \ud83d\udcbe) Concatenating 2 datasets\n</pre> <pre>[22:49:23] [INFO] \ud83e\udde9 Generating column `first_name` from expression\n</pre> <pre>[22:49:23] [INFO] \ud83e\udde9 Generating column `last_name` from expression\n</pre> <pre>[22:49:23] [INFO] \ud83e\udde9 Generating column `dob` from expression\n</pre> <pre>[22:49:23] [INFO] \ud83e\udde9 Generating column `physician` from expression\n</pre> <pre>[22:49:23] [INFO] \ud83d\udcdd llm-text model config for column 'physician_notes'\n</pre> <pre>[22:49:23] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:49:23] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:49:23] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:49:23] [INFO]   |-- inference parameters:\n</pre> <pre>[22:49:23] [INFO]   |  |-- generation_type=chat-completion\n</pre> <pre>[22:49:23] [INFO]   |  |-- max_parallel_requests=4\n</pre> <pre>[22:49:23] [INFO]   |  |-- extra_body={'chat_template_kwargs': {'enable_thinking': False}}\n</pre> <pre>[22:49:23] [INFO]   |  |-- temperature=1.00\n</pre> <pre>[22:49:23] [INFO]   |  |-- top_p=1.00\n</pre> <pre>[22:49:23] [INFO]   |  |-- max_tokens=2048\n</pre> <pre>[22:49:23] [INFO] \u26a1\ufe0f Processing llm-text column 'physician_notes' with 4 concurrent workers\n</pre> <pre>[22:49:23] [INFO] \u23f1\ufe0f llm-text column 'physician_notes' will report progress after each record\n</pre> <pre>[22:49:31] [INFO]   |-- \ud83d\udc25 llm-text column 'physician_notes' progress: 1/2 (50%) complete, 1 ok, 0 failed, 0.13 rec/s, eta 7.5s\n</pre> <pre>[22:49:31] [INFO]   |-- \ud83d\udc14 llm-text column 'physician_notes' progress: 2/2 (100%) complete, 2 ok, 0 failed, 0.25 rec/s, eta 0.0s\n</pre> <pre>[22:49:32] [INFO] \ud83d\udcca Model usage summary:\n</pre> <pre>[22:49:32] [INFO]   |-- model: nvidia/nemotron-3-nano-30b-a3b\n</pre> <pre>[22:49:32] [INFO]   |-- tokens: input=297, output=3636, total=3933, tps=478\n</pre> <pre>[22:49:32] [INFO]   |-- requests: success=2, failed=0, total=2, rpm=14\n</pre> <pre>[22:49:32] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:49:32] [INFO]   |-- \ud83c\udfb2 column: 'patient_sampler'\n</pre> <pre>[22:49:32] [INFO]   |-- \ud83c\udfb2 column: 'doctor_sampler'\n</pre> <pre>[22:49:32] [INFO]   |-- \ud83c\udfb2 column: 'patient_id'\n</pre> <pre>[22:49:32] [INFO]   |-- \ud83e\udde9 column: 'first_name'\n</pre> <pre>[22:49:32] [INFO]   |-- \ud83e\udde9 column: 'last_name'\n</pre> <pre>[22:49:32] [INFO]   |-- \ud83e\udde9 column: 'dob'\n</pre> <pre>[22:49:32] [INFO]   |-- \ud83c\udfb2 column: 'symptom_onset_date'\n</pre> <pre>[22:49:32] [INFO]   |-- \ud83c\udfb2 column: 'date_of_visit'\n</pre> <pre>[22:49:32] [INFO]   |-- \ud83e\udde9 column: 'physician'\n</pre> <pre>[22:49:32] [INFO]   |-- \ud83d\udcdd column: 'physician_notes'\n</pre> <pre>[22:49:32] [INFO] \u2705 Preview complete!\n</pre> In\u00a0[8]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() <pre>                                                                                                                   \n                                                   Seed Columns                                                    \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name            \u2503 Value                                                                                         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 diagnosis       \u2502 cervical spondylosis                                                                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 patient_summary \u2502 I've been having a lot of pain in my neck and back. I've also been having trouble with my     \u2502\n\u2502                 \u2502 balance and coordination. I've been coughing a lot and my limbs feel weak.                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                 Generated Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name               \u2503 Value                                                                                      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 patient_sampler    \u2502 {                                                                                          \u2502\n\u2502                    \u2502     'uuid': '5fa3d1ee-f279-40fd-b6ee-0a83b7d38b9a',                                        \u2502\n\u2502                    \u2502     'locale': 'en_US',                                                                     \u2502\n\u2502                    \u2502     'first_name': 'Chase',                                                                 \u2502\n\u2502                    \u2502     'last_name': 'Mccoy',                                                                  \u2502\n\u2502                    \u2502     'middle_name': None,                                                                   \u2502\n\u2502                    \u2502     'sex': 'Male',                                                                         \u2502\n\u2502                    \u2502     'street_number': '303',                                                                \u2502\n\u2502                    \u2502     'street_name': 'Wilkins Mountain',                                                     \u2502\n\u2502                    \u2502     'city': 'East Shanestad',                                                              \u2502\n\u2502                    \u2502     'state': 'North Dakota',                                                               \u2502\n\u2502                    \u2502     'postcode': '60898',                                                                   \u2502\n\u2502                    \u2502     'age': 46,                                                                             \u2502\n\u2502                    \u2502     'birth_date': '1979-09-25',                                                            \u2502\n\u2502                    \u2502     'country': 'Chile',                                                                    \u2502\n\u2502                    \u2502     'marital_status': 'divorced',                                                          \u2502\n\u2502                    \u2502     'education_level': 'graduate',                                                         \u2502\n\u2502                    \u2502     'unit': '',                                                                            \u2502\n\u2502                    \u2502     'occupation': 'Set designer',                                                          \u2502\n\u2502                    \u2502     'phone_number': '001-284-801-1366x288',                                                \u2502\n\u2502                    \u2502     'bachelors_field': 'education'                                                         \u2502\n\u2502                    \u2502 }                                                                                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 doctor_sampler     \u2502 {                                                                                          \u2502\n\u2502                    \u2502     'uuid': '37e29668-b02c-4cbb-aade-13071d0c8ed5',                                        \u2502\n\u2502                    \u2502     'locale': 'en_US',                                                                     \u2502\n\u2502                    \u2502     'first_name': 'Mark',                                                                  \u2502\n\u2502                    \u2502     'last_name': 'Franco',                                                                 \u2502\n\u2502                    \u2502     'middle_name': None,                                                                   \u2502\n\u2502                    \u2502     'sex': 'Male',                                                                         \u2502\n\u2502                    \u2502     'street_number': '7775',                                                               \u2502\n\u2502                    \u2502     'street_name': 'Bailey Cove',                                                          \u2502\n\u2502                    \u2502     'city': 'South Robertton',                                                             \u2502\n\u2502                    \u2502     'state': 'Texas',                                                                      \u2502\n\u2502                    \u2502     'postcode': '15679',                                                                   \u2502\n\u2502                    \u2502     'age': 82,                                                                             \u2502\n\u2502                    \u2502     'birth_date': '1944-01-07',                                                            \u2502\n\u2502                    \u2502     'country': \"Cote d'Ivoire\",                                                            \u2502\n\u2502                    \u2502     'marital_status': 'widowed',                                                           \u2502\n\u2502                    \u2502     'education_level': 'graduate',                                                         \u2502\n\u2502                    \u2502     'unit': '',                                                                            \u2502\n\u2502                    \u2502     'occupation': 'Insurance risk surveyor',                                               \u2502\n\u2502                    \u2502     'phone_number': '3445226615',                                                          \u2502\n\u2502                    \u2502     'bachelors_field': 'stem_related'                                                      \u2502\n\u2502                    \u2502 }                                                                                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 patient_id         \u2502 PT-810BD896                                                                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 symptom_onset_date \u2502 2024-11-10                                                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 date_of_visit      \u2502 2024-11-12                                                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 first_name         \u2502 Chase                                                                                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 last_name          \u2502 Mccoy                                                                                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dob                \u2502 1979-09-25                                                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 physician          \u2502 Dr. Franco                                                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 physician_notes    \u2502 **Chase McCoy \u2013 2024-11-12 \u2013 Office Visit**                                                \u2502\n\u2502                    \u2502 **Provider:** Dr. Mark Franco, MD                                                          \u2502\n\u2502                    \u2502 **DOB:** 09/15/1990 (34 y/o) \u2013 **M**                                                       \u2502\n\u2502                    \u2502 **Today\u2019s Diagnosis / Plan Overview:** Cervical spondylosis with progressive neurologic    \u2502\n\u2502                    \u2502 compromise; secondary musculoskeletal and respiratory symptoms.                            \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 ---                                                                                        \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 ### 1. Chief Complaint (CC)                                                                \u2502\n\u2502                    \u2502 - \u201cNeck and back pain getting worse.\u201d                                                      \u2502\n\u2502                    \u2502 - \u201cTrouble walking/loss of coordination.\u201d                                                  \u2502\n\u2502                    \u2502 - \u201cFrequent coughing.\u201d                                                                     \u2502\n\u2502                    \u2502 - \u201cWeakness in arms and legs.\u201d                                                             \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 ### 2. History of Present Illness (HPI)                                                    \u2502\n\u2502                    \u2502 - **Onset:** First noted ~14\u202fmonths ago (\u22482024\u201111\u201110) with diffuse cervical and upper      \u2502\n\u2502                    \u2502 thoracic discomfort.                                                                       \u2502\n\u2502                    \u2502 - **Course:** Gradual increase in intensity; now constant, rated 6\u20138/10, non\u2011radiating,    \u2502\n\u2502                    \u2502 worsened by head\u2011turning and prolonged sitting.                                            \u2502\n\u2502                    \u2502 - **Neurologic:** New onset of:                                                            \u2502\n\u2502                    \u2502   - Gait instability, frequent near\u2011falls.                                                 \u2502\n\u2502                    \u2502   - Fine\u2011motor difficulty (buttoning, writing).                                            \u2502\n\u2502                    \u2502   - Bilateral distal muscle weakness (grading 3\u20114/5) in UE and LE.                         \u2502\n\u2502                    \u2502 - **Respiratory:** Persistent cough, non\u2011productive, 3\u20134 episodes daily; no wheeze or      \u2502\n\u2502                    \u2502 dyspnea at rest; exacerbates with positional changes.                                      \u2502\n\u2502                    \u2502 - **Associated Symptoms:** Occasional numbness/tingling in hands, occasional dizziness, no \u2502\n\u2502                    \u2502 dysphagia, no urinary incontinence.                                                        \u2502\n\u2502                    \u2502 - **Medications:**                                                                         \u2502\n\u2502                    \u2502   - Ibuprofen 600\u202fmg PO q6h PRN (started 2024\u201111\u201110).                                      \u2502\n\u2502                    \u2502   - Acetaminophen 500\u202fmg PO q8h PRN.                                                       \u2502\n\u2502                    \u2502   - No current prescription muscle relaxant or neuropathic agent.                          \u2502\n\u2502                    \u2502 - **Past Medical History (PMH):**                                                          \u2502\n\u2502                    \u2502   - Chronic intermittent tension\u2011type headaches.                                           \u2502\n\u2502                    \u2502   - Seasonal allergies (Claritin PRN).                                                     \u2502\n\u2502                    \u2502   - No prior surgeries.                                                                    \u2502\n\u2502                    \u2502 - **Family History (FH):** Mother \u2013 rheumatoid arthritis; Father \u2013 COPD.                   \u2502\n\u2502                    \u2502 - **Social History (SH):**                                                                 \u2502\n\u2502                    \u2502   - Tobacco: Never.                                                                        \u2502\n\u2502                    \u2502   - Alcohol: Social (1\u20132 drinks/week).                                                     \u2502\n\u2502                    \u2502   - No illicit drugs.                                                                      \u2502\n\u2502                    \u2502 - **Review of Systems (ROS):**                                                             \u2502\n\u2502                    \u2502   - **Neuro:** Balance problems, weakness, occasional numbness.                            \u2502\n\u2502                    \u2502   - **Musculoskeletal:** Neck/back pain; mild stiffness.                                   \u2502\n\u2502                    \u2502   - **Respiratory:** Cough; no shortness of breath.                                        \u2502\n\u2502                    \u2502   - **GI/ GU/ GYN:** Unremarkable.                                                         \u2502\n\u2502                    \u2502   - **Cardiovascular:** No chest pain/palpitations.                                        \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 ### 3. Physical Examination (PE)                                                           \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 | System | Findings |                                                                      \u2502\n\u2502                    \u2502 |--------|----------|                                                                      \u2502\n\u2502                    \u2502 | **General** | Alert, well\u2011groomed, NAD; BMI 27\u202fkg/m\u00b2. |                                  \u2502\n\u2502                    \u2502 | **Vital Signs** | T 98.4\u202f\u00b0F, HR 78\u202fbpm, BP 124/78\u202fmmHg, RR 16/min, SpO\u2082 98% RA. |        \u2502\n\u2502                    \u2502 | **HEENT** | No jugular venous distension; oropharynx clear; mild dry cough noted. |      \u2502\n\u2502                    \u2502 | **Neck** | Limited flexion/extension due to pain; no carotid bruits. |                   \u2502\n\u2502                    \u2502 | **Musculoskeletal** |                                                                    \u2502\n\u2502                    \u2502   - Upper extremities: 3/5 strength proximally, 4/5 distally; decreased grip.              \u2502\n\u2502                    \u2502   - Lower extremities: 4/5 strength proximally, 3/5 distally; hyperreflexia (+3) in knees; \u2502\n\u2502                    \u2502 positive Romberg.                                                                          \u2502\n\u2502                    \u2502   - Positive Hoffmann sign bilaterally.                                                    \u2502\n\u2502                    \u2502   - No pathological fractures or obvious deformities. |                                    \u2502\n\u2502                    \u2502 | **Neurologic** |                                                                         \u2502\n\u2502                    \u2502   - Sensation: decreased pinprick in C6\u2013C8 dermatomes.                                     \u2502\n\u2502                    \u2502   - Reflexes: hyperactive (biceps, triceps, knee).                                         \u2502\n\u2502                    \u2502   - Coordination: dysmetria on finger\u2011nose test.                                           \u2502\n\u2502                    \u2502   - Gait: assisted, wide\u2011based, occasional foot drop. |                                    \u2502\n\u2502                    \u2502 | **Respiratory** | Clear to auscultation bilaterally; no rales. No wheezing. |            \u2502\n\u2502                    \u2502 | **Skin** | No rashes or lesions. |                                                       \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 ### 4. Diagnoses / Impressions                                                             \u2502\n\u2502                    \u2502 1. **Cervical spondylosis with myelopathic features** (progressive neck degeneration       \u2502\n\u2502                    \u2502 causing cord compression \u2192 balance/coordination deficits, motor weakness).                 \u2502\n\u2502                    \u2502 2. **Secondary musculoskeletal pain** (neck/upper thoracic).                               \u2502\n\u2502                    \u2502 3. **Chronic intermittent cough** likely secondary to post\u2011nasal drip/allergy exacerbation \u2502\n\u2502                    \u2502 (no primary pulmonary pathology evident).                                                  \u2502\n\u2502                    \u2502 4. **Secondary weakness and gait instability** attributable to cervical myelopathy.        \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 ### 5. Assessment Summary                                                                  \u2502\n\u2502                    \u2502 - Neurologic exam shows upper motor neuron signs (spasticity, hyperreflexia) consistent    \u2502\n\u2502                    \u2502 with cervical cord involvement.                                                            \u2502\n\u2502                    \u2502 - Weakness and coordination deficits suggest functional impact of spinal compression.      \u2502\n\u2502                    \u2502 - Persistent cough likely unrelated to neurologic process; consider allergy/irritant       \u2502\n\u2502                    \u2502 exposure; no signs of infection.                                                           \u2502\n\u2502                    \u2502 - Pain moderate, partially controlled with NSAIDs/acetaminophen.                           \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 ### 6. Plan                                                                                \u2502\n\u2502                    \u2502 **A. Diagnostic Studies**                                                                  \u2502\n\u2502                    \u2502 1. **MRI Cervical Spine (with and without contrast)** \u2013 order today; prioritize for        \u2502\n\u2502                    \u2502 evaluating canal stenosis, disc herniation, epidural space.                                \u2502\n\u2502                    \u2502 2. **EMG/NCS** \u2013 schedule to assess peripheral nerve contributions and rule out            \u2502\n\u2502                    \u2502 radiculopathy.                                                                             \u2502\n\u2502                    \u2502 3. **Pulmonary Function Tests (spirometry + diffusion capacity)** \u2013 if cough persists or   \u2502\n\u2502                    \u2502 worsens after imaging review; evaluate for early restrictive pattern.                      \u2502\n\u2502                    \u2502 4. **Allergy testing / referral to ENT** \u2013 evaluate chronic cough etiology.                \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **B. Medications**                                                                         \u2502\n\u2502                    \u2502 - Continue **Ibuprofen 600\u202fmg PO q6h PRN** (max 2,400\u202fmg/day).                             \u2502\n\u2502                    \u2502 - Add **Gabapentin 300\u202fmg PO TID** (titrate up to 900\u202fmg TID) for neuropathic component    \u2502\n\u2502                    \u2502 and potential radicular pain.                                                              \u2502\n\u2502                    \u2502 - If pain not improved in 2\u202fweeks, consider a short course of **Oral Prednisone 40\u202fmg      \u2502\n\u2502                    \u2502 daily \u00d75\u202fdays** (short taper) for inflammatory component (documented only if MRI shows     \u2502\n\u2502                    \u2502 significant epidural inflammation).                                                        \u2502\n\u2502                    \u2502 - **Vitamin D 2,000\u202fIU PO daily** \u2013 supplement given limited sun exposure and possible     \u2502\n\u2502                    \u2502 bone health relevance.                                                                     \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **C. Physical Therapy &amp; Rehabilitation**                                                   \u2502\n\u2502                    \u2502 - **Cervical traction &amp; Mobilization** \u2013 PT referral for 2\u202f\u00d7/week sessions initially       \u2502\n\u2502                    \u2502 (focus on posture, neck stabilization, gentle traction).                                   \u2502\n\u2502                    \u2502 - **Gait &amp; Balance Training** \u2013 PT referral for fall\u2011prevention program; incorporate       \u2502\n\u2502                    \u2502 vestibular exercises.                                                                      \u2502\n\u2502                    \u2502 - **Strengthening** \u2013 target cervical and scapular stabilization; low\u2011impact resistance    \u2502\n\u2502                    \u2502 for UE/LE.                                                                                 \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **D. Activity/Lifestyle Recommendations**                                                  \u2502\n\u2502                    \u2502 - Ergonomic workstation modifications (monitor at eye\u2011level, avoid prolonged neck          \u2502\n\u2502                    \u2502 flexion).                                                                                  \u2502\n\u2502                    \u2502 - Avoid heavy lifting, high\u2011impact activities, and neck extension extremes.                \u2502\n\u2502                    \u2502 - Sleep on a supportive mattress/pillow; maintain neutral cervical alignment.              \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **E. Follow\u2011Up**                                                                           \u2502\n\u2502                    \u2502 - **In 1\u20132\u202fweeks**: Review MRI results; if significant canal stenosis identified, discuss  \u2502\n\u2502                    \u2502 options (e.g., surgical decompression).                                                    \u2502\n\u2502                    \u2502 - **In 4\u202fweeks**: Re\u2011evaluate response to gabapentin and PT; adjust analgesic regimen as   \u2502\n\u2502                    \u2502 needed.                                                                                    \u2502\n\u2502                    \u2502 - **If symptoms progress rapidly** (new weakness, gait deterioration, bowel/bladder        \u2502\n\u2502                    \u2502 changes) \u2013 advise immediate ED evaluation.                                                 \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **F. Patient Education**                                                                   \u2502\n\u2502                    \u2502 - Discussed natural history of cervical spondylosis and risks of untreated myelopathy.     \u2502\n\u2502                    \u2502 - Reviewed medication side\u2011effects (gabapentin: drowsiness, dizziness).                    \u2502\n\u2502                    \u2502 - Emphasized importance of reporting any new neuro deficits, urinary changes, or severe    \u2502\n\u2502                    \u2502 pain.                                                                                      \u2502\n\u2502                    \u2502 - Encouraged adherence to PT schedule and ergonomic adjustments.                           \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 **G. Documentation**                                                                       \u2502\n\u2502                    \u2502 - Encounter note entered into EMR; orders placed electronically; referrals sent to PT,     \u2502\n\u2502                    \u2502 Neurology, and Radiology.                                                                  \u2502\n\u2502                    \u2502                                                                                            \u2502\n\u2502                    \u2502 ---                                                                                        \u2502\n\u2502                    \u2502 **Signature:** Dr. Mark Franco, MD                                                         \u2502\n\u2502                    \u2502 **Provider ID:** 874235                                                                    \u2502\n\u2502                    \u2502 **Date/Time:** 2024\u201111\u201112 14:35 EST.                                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                    [index: 0]                                                     \n</pre> In\u00a0[9]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset Out[9]: diagnosis patient_summary patient_sampler doctor_sampler patient_id symptom_onset_date date_of_visit first_name last_name dob physician physician_notes 0 cervical spondylosis I've been having a lot of pain in my neck and ... {'uuid': '5fa3d1ee-f279-40fd-b6ee-0a83b7d38b9a... {'uuid': '37e29668-b02c-4cbb-aade-13071d0c8ed5... PT-810BD896 2024-11-10 2024-11-12 Chase Mccoy 1979-09-25 Dr. Franco **Chase McCoy \u2013 2024-11-12 \u2013 Office Visit**  \\... 1 impetigo I have a rash on my face that is getting worse... {'uuid': '6dcdd347-c81c-4330-a4a3-fc816d830e49... {'uuid': '1c134c45-b28e-4f3c-80e2-05cacdf00f2d... PT-F4EE224C 2024-06-11 2024-07-01 Brianna Carter 1976-10-12 Dr. Lucero **Visit Summary: Brianna Carter \u2013 7/1/24**  \\n... In\u00a0[10]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2                               \u2502 10                              \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83c\udfb2 Sampler Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                   \u2503       data type \u2503             number unique values \u2503               sampler type \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 patient_sampler               \u2502            dict \u2502                       2 (100.0%) \u2502          person_from_faker \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 doctor_sampler                \u2502            dict \u2502                       2 (100.0%) \u2502          person_from_faker \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 patient_id                    \u2502          string \u2502                       2 (100.0%) \u2502                       uuid \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 symptom_onset_date            \u2502          string \u2502                       2 (100.0%) \u2502                   datetime \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 date_of_visit                 \u2502          string \u2502                       2 (100.0%) \u2502                  timedelta \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83d\udcdd LLM-Text Columns                                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                       \u2503               \u2503                            \u2503     prompt tokens \u2503      completion tokens \u2503\n\u2503 column name           \u2503     data type \u2503       number unique values \u2503        per record \u2503             per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 physician_notes       \u2502        string \u2502                 2 (100.0%) \u2502     125.0 +/- 3.0 \u2502       1718.5 +/- 122.3 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                               \ud83e\udde9 Expression Columns                                               \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                    \u2503                 data type \u2503                               number unique values \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 first_name                     \u2502                    string \u2502                                         2 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 last_name                      \u2502                    string \u2502                                         2 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dob                            \u2502                    string \u2502                                         2 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 physician                      \u2502                    string \u2502                                         2 (100.0%) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> In\u00a0[11]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-3\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-3\") <pre>[22:49:32] [INFO] \ud83c\udfa8 Creating Data Designer dataset\n</pre> <pre>[22:49:32] [INFO] \u2705 Validation passed\n</pre> <pre>[22:49:32] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:49:32] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:49:32] [INFO]   |-- \ud83d\udc40 Checking 'nvidia/nemotron-3-nano-30b-a3b' in provider named 'nvidia' for model alias 'nemotron-nano-v3'...\n</pre> <pre>[22:49:32] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:49:32] [INFO] \u23f3 Processing batch 1 of 1\n</pre> <pre>[22:49:32] [INFO] \ud83c\udf31 Sampling 10 records from seed dataset\n</pre> <pre>[22:49:32] [INFO]   |-- seed dataset size: 820 records\n</pre> <pre>[22:49:32] [INFO]   |-- sampling strategy: ordered\n</pre> <pre>[22:49:32] [INFO] \ud83c\udfb2 Preparing samplers to generate 10 records across 5 columns\n</pre> <pre>[22:49:32] [INFO] (\ud83d\udcbe + \ud83d\udcbe) Concatenating 2 datasets\n</pre> <pre>[22:49:32] [INFO] \ud83e\udde9 Generating column `first_name` from expression\n</pre> <pre>[22:49:32] [INFO] \ud83e\udde9 Generating column `last_name` from expression\n</pre> <pre>[22:49:32] [INFO] \ud83e\udde9 Generating column `dob` from expression\n</pre> <pre>[22:49:32] [INFO] \ud83e\udde9 Generating column `physician` from expression\n</pre> <pre>[22:49:32] [INFO] \ud83d\udcdd llm-text model config for column 'physician_notes'\n</pre> <pre>[22:49:32] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n</pre> <pre>[22:49:32] [INFO]   |-- model alias: 'nemotron-nano-v3'\n</pre> <pre>[22:49:32] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:49:32] [INFO]   |-- inference parameters:\n</pre> <pre>[22:49:32] [INFO]   |  |-- generation_type=chat-completion\n</pre> <pre>[22:49:32] [INFO]   |  |-- max_parallel_requests=4\n</pre> <pre>[22:49:32] [INFO]   |  |-- extra_body={'chat_template_kwargs': {'enable_thinking': False}}\n</pre> <pre>[22:49:32] [INFO]   |  |-- temperature=1.00\n</pre> <pre>[22:49:32] [INFO]   |  |-- top_p=1.00\n</pre> <pre>[22:49:32] [INFO]   |  |-- max_tokens=2048\n</pre> <pre>[22:49:32] [INFO] \u26a1\ufe0f Processing llm-text column 'physician_notes' with 4 concurrent workers\n</pre> <pre>[22:49:32] [INFO] \u23f1\ufe0f llm-text column 'physician_notes' will report progress after each record\n</pre> <pre>[22:49:37] [INFO]   |-- \ud83d\udc31 llm-text column 'physician_notes' progress: 1/10 (10%) complete, 1 ok, 0 failed, 0.20 rec/s, eta 45.9s\n</pre> <pre>[22:49:38] [INFO]   |-- \ud83d\udc31 llm-text column 'physician_notes' progress: 2/10 (20%) complete, 2 ok, 0 failed, 0.36 rec/s, eta 22.4s\n</pre> <pre>[22:49:38] [INFO]   |-- \ud83d\ude3a llm-text column 'physician_notes' progress: 3/10 (30%) complete, 3 ok, 0 failed, 0.49 rec/s, eta 14.4s\n</pre> <pre>[22:49:40] [INFO]   |-- \ud83d\ude3a llm-text column 'physician_notes' progress: 4/10 (40%) complete, 4 ok, 0 failed, 0.51 rec/s, eta 11.7s\n</pre> <pre>[22:49:40] [INFO]   |-- \ud83d\ude38 llm-text column 'physician_notes' progress: 5/10 (50%) complete, 5 ok, 0 failed, 0.63 rec/s, eta 7.9s\n</pre> <pre>[22:49:42] [INFO]   |-- \ud83d\ude38 llm-text column 'physician_notes' progress: 6/10 (60%) complete, 6 ok, 0 failed, 0.62 rec/s, eta 6.5s\n</pre> <pre>[22:49:44] [INFO]   |-- \ud83d\ude38 llm-text column 'physician_notes' progress: 7/10 (70%) complete, 7 ok, 0 failed, 0.58 rec/s, eta 5.2s\n</pre> <pre>[22:49:46] [INFO]   |-- \ud83d\ude3c llm-text column 'physician_notes' progress: 8/10 (80%) complete, 8 ok, 0 failed, 0.59 rec/s, eta 3.4s\n</pre> <pre>[22:49:48] [INFO]   |-- \ud83d\ude3c llm-text column 'physician_notes' progress: 9/10 (90%) complete, 9 ok, 0 failed, 0.57 rec/s, eta 1.8s\n</pre> <pre>[22:49:50] [INFO]   |-- \ud83e\udd81 llm-text column 'physician_notes' progress: 10/10 (100%) complete, 10 ok, 0 failed, 0.57 rec/s, eta 0.0s\n</pre> <pre>[22:49:50] [INFO] \ud83d\udcca Model usage summary:\n</pre> <pre>[22:49:50] [INFO]   |-- model: nvidia/nemotron-3-nano-30b-a3b\n</pre> <pre>[22:49:50] [INFO]   |-- tokens: input=1435, output=10874, total=12309, tps=682\n</pre> <pre>[22:49:50] [INFO]   |-- requests: success=10, failed=0, total=10, rpm=33\n</pre> <pre>[22:49:50] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:49:50] [INFO]   |-- \ud83c\udfb2 column: 'patient_sampler'\n</pre> <pre>[22:49:50] [INFO]   |-- \ud83c\udfb2 column: 'doctor_sampler'\n</pre> <pre>[22:49:50] [INFO]   |-- \ud83c\udfb2 column: 'patient_id'\n</pre> <pre>[22:49:50] [INFO]   |-- \ud83e\udde9 column: 'first_name'\n</pre> <pre>[22:49:50] [INFO]   |-- \ud83e\udde9 column: 'last_name'\n</pre> <pre>[22:49:50] [INFO]   |-- \ud83e\udde9 column: 'dob'\n</pre> <pre>[22:49:50] [INFO]   |-- \ud83c\udfb2 column: 'symptom_onset_date'\n</pre> <pre>[22:49:50] [INFO]   |-- \ud83c\udfb2 column: 'date_of_visit'\n</pre> <pre>[22:49:50] [INFO]   |-- \ud83e\udde9 column: 'physician'\n</pre> <pre>[22:49:50] [INFO]   |-- \ud83d\udcdd column: 'physician_notes'\n</pre> In\u00a0[12]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() Out[12]: diagnosis patient_summary patient_sampler doctor_sampler patient_id symptom_onset_date date_of_visit first_name last_name dob physician physician_notes 0 cervical spondylosis I've been having a lot of pain in my neck and ... {'age': 106, 'bachelors_field': 'no_degree', '... {'age': 110, 'bachelors_field': 'no_degree', '... PT-859C7479 2024-08-06 2024-08-13 Kathryn Thornton 1919-12-04 Dr. Moore **Patient:** Kathryn Thornton   **DOB:** 55F  ... 1 impetigo I have a rash on my face that is getting worse... {'age': 85, 'bachelors_field': 'no_degree', 'b... {'age': 98, 'bachelors_field': 'education', 'b... PT-EF6E8AAE 2024-05-01 2024-05-05 Dennis Wiggins 1940-04-14 Dr. Cabrera **Visit Note \u2013 2024\u201105\u201105**   **Patient:** Den... 2 urinary tract infection I have been urinating blood. I sometimes feel ... {'age': 107, 'bachelors_field': 'no_degree', '... {'age': 40, 'bachelors_field': 'stem', 'birth_... PT-A3C7F254 2024-08-24 2024-08-26 Jordan Camacho 1918-07-04 Dr. Knapp **Patient:** Jordan Camacho   **DOB:** 03/15/1... 3 arthritis I have been having trouble with my muscles and... {'age': 35, 'bachelors_field': 'education', 'b... {'age': 80, 'bachelors_field': 'stem_related',... PT-4DA6C4A8 2024-05-27 2024-06-06 Mercedes Martin 1990-02-15 Dr. Mckee **2024-06-06 - Mercedes Martin - 58F - Follow-... 4 dengue I have been feeling really sick. My body hurts... {'age': 47, 'bachelors_field': 'no_degree', 'b... {'age': 70, 'bachelors_field': 'education', 'b... PT-4FCFBC06 2024-04-15 2024-05-08 Kenneth Marsh 1979-01-27 Dr. May **SOAP Note \u2013 Dr. Richard May**   **Pt:** Kenn... In\u00a0[13]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 10                              \u2502 10                              \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83c\udfb2 Sampler Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                   \u2503       data type \u2503             number unique values \u2503               sampler type \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 patient_sampler               \u2502            dict \u2502                      10 (100.0%) \u2502          person_from_faker \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 doctor_sampler                \u2502            dict \u2502                      10 (100.0%) \u2502          person_from_faker \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 patient_id                    \u2502          string \u2502                      10 (100.0%) \u2502                       uuid \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 symptom_onset_date            \u2502          string \u2502                      10 (100.0%) \u2502                   datetime \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 date_of_visit                 \u2502          string \u2502                      10 (100.0%) \u2502                  timedelta \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83d\udcdd LLM-Text Columns                                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                       \u2503               \u2503                            \u2503     prompt tokens \u2503      completion tokens \u2503\n\u2503 column name           \u2503     data type \u2503       number unique values \u2503        per record \u2503             per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 physician_notes       \u2502        string \u2502                10 (100.0%) \u2502     119.0 +/- 5.5 \u2502       1087.0 +/- 379.6 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                               \ud83e\udde9 Expression Columns                                               \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 column name                    \u2503                 data type \u2503                               number unique values \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 first_name                     \u2502                    string \u2502                                        10 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 last_name                      \u2502                    string \u2502                                        10 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dob                            \u2502                    string \u2502                                        10 (100.0%) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 physician                      \u2502                    string \u2502                                        10 (100.0%) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre>"},{"location":"notebooks/3-seeding-with-a-dataset/#data-designer-tutorial-seeding-synthetic-data-generation-with-an-external-dataset","title":"\ud83c\udfa8 Data Designer Tutorial: Seeding Synthetic Data Generation with an External Dataset\u00b6","text":""},{"location":"notebooks/3-seeding-with-a-dataset/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>In this notebook, we will demonstrate how to seed synthetic data generation in Data Designer with an external dataset.</p> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"notebooks/3-seeding-with-a-dataset/#import-data-designer","title":"\ud83d\udce6 Import Data Designer\u00b6","text":"<ul> <li><p><code>data_designer.config</code> provides access to the configuration API.</p> </li> <li><p><code>DataDesigner</code> is the main interface for data generation.</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#prepare-a-seed-dataset","title":"\ud83c\udfe5 Prepare a seed dataset\u00b6","text":"<ul> <li><p>For this notebook, we'll create a synthetic dataset of patient notes.</p> </li> <li><p>We will seed the generation process with a symptom-to-diagnosis dataset.</p> </li> <li><p>We already have the dataset downloaded in the data directory of this repository.</p> </li> </ul> <p>\ud83c\udf31 Why use a seed dataset?</p> <ul> <li><p>Seed datasets let you steer the generation process by providing context that is specific to your use case.</p> </li> <li><p>Seed datasets are also an excellent way to inject real-world diversity into your synthetic data.</p> </li> <li><p>During generation, prompt templates can reference any of the seed dataset fields.</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#designing-our-synthetic-patient-notes-dataset","title":"\ud83c\udfa8 Designing our synthetic patient notes dataset\u00b6","text":"<ul> <li>The prompt template can reference fields from our seed dataset:<ul> <li><code>{{ diagnosis }}</code> - the medical diagnosis from the seed data</li> <li><code>{{ patient_summary }}</code> - the symptom description from the seed data</li> </ul> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013\u00a0preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebooks/3-seeding-with-a-dataset/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebooks/3-seeding-with-a-dataset/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Check out the following notebook to learn more about:</p> <ul> <li>Providing images as context</li> </ul>"},{"location":"notebooks/4-providing-images-as-context/","title":"Providing Images as Context","text":"In\u00a0[1]: Copied! <pre># Standard library imports\nimport base64\nimport io\nimport uuid\n\n# Third-party imports\nimport pandas as pd\nimport rich\nfrom datasets import load_dataset\nfrom IPython.display import display\nfrom rich.panel import Panel\n\n# Data Designer imports\nimport data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n</pre> # Standard library imports import base64 import io import uuid  # Third-party imports import pandas as pd import rich from datasets import load_dataset from IPython.display import display from rich.panel import Panel  # Data Designer imports import data_designer.config as dd from data_designer.interface import DataDesigner In\u00a0[2]: Copied! <pre>data_designer = DataDesigner()\n</pre> data_designer = DataDesigner() In\u00a0[3]: Copied! <pre># This name is set in the model provider configuration.\nMODEL_PROVIDER = \"nvidia\"\n\nmodel_configs = [\n    dd.ModelConfig(\n        alias=\"vision\",\n        model=\"meta/llama-4-scout-17b-16e-instruct\",\n        provider=MODEL_PROVIDER,\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=0.60,\n            top_p=0.95,\n            max_tokens=2048,\n        ),\n    ),\n]\n</pre> # This name is set in the model provider configuration. MODEL_PROVIDER = \"nvidia\"  model_configs = [     dd.ModelConfig(         alias=\"vision\",         model=\"meta/llama-4-scout-17b-16e-instruct\",         provider=MODEL_PROVIDER,         inference_parameters=dd.ChatCompletionInferenceParams(             temperature=0.60,             top_p=0.95,             max_tokens=2048,         ),     ), ] In\u00a0[4]: Copied! <pre>config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n</pre> config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs) In\u00a0[5]: Copied! <pre># Dataset processing configuration\nIMG_COUNT = 512  # Number of images to process\nBASE64_IMAGE_HEIGHT = 512  # Standardized height for model input\n\n# Load ColPali dataset for visual documents\nimg_dataset_cfg = {\"path\": \"vidore/colpali_train_set\", \"split\": \"train\", \"streaming\": True}\n</pre> # Dataset processing configuration IMG_COUNT = 512  # Number of images to process BASE64_IMAGE_HEIGHT = 512  # Standardized height for model input  # Load ColPali dataset for visual documents img_dataset_cfg = {\"path\": \"vidore/colpali_train_set\", \"split\": \"train\", \"streaming\": True} In\u00a0[6]: Copied! <pre>def resize_image(image, height: int):\n    \"\"\"\n    Resize image while maintaining aspect ratio.\n\n    Args:\n        image: PIL Image object\n        height: Target height in pixels\n\n    Returns:\n        Resized PIL Image object\n    \"\"\"\n    original_width, original_height = image.size\n    width = int(original_width * (height / original_height))\n    return image.resize((width, height))\n\n\ndef convert_image_to_chat_format(record, height: int) -&gt; dict:\n    \"\"\"\n    Convert PIL image to base64 format for chat template usage.\n\n    Args:\n        record: Dataset record containing image and metadata\n        height: Target height for image resizing\n\n    Returns:\n        Updated record with base64_image and uuid fields\n    \"\"\"\n    # Resize image for consistent processing\n    image = resize_image(record[\"image\"], height)\n\n    # Convert to base64 string\n    img_buffer = io.BytesIO()\n    image.save(img_buffer, format=\"PNG\")\n    byte_data = img_buffer.getvalue()\n    base64_encoded_data = base64.b64encode(byte_data)\n    base64_string = base64_encoded_data.decode(\"utf-8\")\n\n    # Return updated record\n    return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())}\n</pre> def resize_image(image, height: int):     \"\"\"     Resize image while maintaining aspect ratio.      Args:         image: PIL Image object         height: Target height in pixels      Returns:         Resized PIL Image object     \"\"\"     original_width, original_height = image.size     width = int(original_width * (height / original_height))     return image.resize((width, height))   def convert_image_to_chat_format(record, height: int) -&gt; dict:     \"\"\"     Convert PIL image to base64 format for chat template usage.      Args:         record: Dataset record containing image and metadata         height: Target height for image resizing      Returns:         Updated record with base64_image and uuid fields     \"\"\"     # Resize image for consistent processing     image = resize_image(record[\"image\"], height)      # Convert to base64 string     img_buffer = io.BytesIO()     image.save(img_buffer, format=\"PNG\")     byte_data = img_buffer.getvalue()     base64_encoded_data = base64.b64encode(byte_data)     base64_string = base64_encoded_data.decode(\"utf-8\")      # Return updated record     return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())} In\u00a0[7]: Copied! <pre># Load and process the visual document dataset\nprint(\"\ud83d\udce5 Loading and processing document images...\")\n\nimg_dataset_iter = iter(\n    load_dataset(**img_dataset_cfg).map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT})\n)\nimg_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])\n\nprint(f\"\u2705 Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\")\n</pre> # Load and process the visual document dataset print(\"\ud83d\udce5 Loading and processing document images...\")  img_dataset_iter = iter(     load_dataset(**img_dataset_cfg).map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT}) ) img_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])  print(f\"\u2705 Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\") <pre>\ud83d\udce5 Loading and processing document images...\n</pre> <pre>\u2705 Loaded 512 images with columns: ['image', 'image_filename', 'query', 'answer', 'source', 'options', 'page', 'model', 'prompt', 'answer_type', 'base64_image', 'uuid']\n</pre> In\u00a0[8]: Copied! <pre>img_dataset.head()\n</pre> img_dataset.head() Out[8]: image image_filename query answer source options page model prompt answer_type base64_image uuid 0 &lt;PIL.JpegImagePlugin.JpegImageFile image mode=... images/1810.07757_2.jpg Comparing panels a, b, c, and d, which stateme... D arxiv_qa ['A. The variance of the data decreases from p... gpt4V None iVBORw0KGgoAAAANSUhEUgAAAUAAAAIACAIAAAB8QiIMAA... e8750e11-ffb1-4156-83de-abe849701a2f 1 &lt;PIL.JpegImagePlugin.JpegImageFile image mode=... data/scrapped_pdfs_split/pages_extracted/energ... What is the duration of the course mentioned i... ['five to ten hours, not including field trips'] pdf None 9 sonnet \\n        You are an assistant specialized in ... None iVBORw0KGgoAAAANSUhEUgAAAYsAAAIACAIAAAD8HddaAA... d9c9c627-0146-4893-b5f8-4cfdda606b4f 2 &lt;PIL.JpegImagePlugin.JpegImageFile image mode=... data/scrapped_pdfs_split/pages_extracted/energ... What is the primary purpose of the PTC in lith... ['protect against external short circuits'] pdf None 414 sonnet \\n        You are an assistant specialized in ... None iVBORw0KGgoAAAANSUhEUgAAAZgAAAIACAIAAAAwhO2xAA... 0ae54638-ebf6-49c9-b0ba-9ade805c28d9 3 &lt;PIL.PngImagePlugin.PngImageFile image mode=L ... 0fd47b51ae9248ef36669b8619b1223f268edae3e7a44a... What is the date?\\nYour answer should be very ... OCTOBER 17, 1995. docvqa None None None None None iVBORw0KGgoAAAANSUhEUgAAAX0AAAIACAAAAABLRuMPAA... bba667bd-6492-436a-9cfa-6869e18a1390 4 &lt;PIL.PngImagePlugin.PngImageFile image mode=L ... b335cfb9d442f8925ea41a064cb445a5395577f2345d52... What is Bert Shulimson's title?\\nYour response... EXECUTIVE SECRETARY. docvqa None None None None None iVBORw0KGgoAAAANSUhEUgAAAY8AAAIACAAAAABf/7+rAA... c820f134-cf52-4dff-87a3-a4f35ccf6847 In\u00a0[9]: Copied! <pre># Add the seed dataset containing our processed images\ndf_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]]\nconfig_builder.with_seed_dataset(dd.DataFrameSeedSource(df=df_seed))\n</pre> # Add the seed dataset containing our processed images df_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]] config_builder.with_seed_dataset(dd.DataFrameSeedSource(df=df_seed)) Out[9]: <pre>DataDesignerConfigBuilder()\n</pre> In\u00a0[10]: Copied! <pre># Add a column to generate detailed document summaries\nconfig_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"summary\",\n        model_alias=\"vision\",\n        prompt=(\n            \"Provide a detailed summary of the content in this image in Markdown format. \"\n            \"Start from the top of the image and then describe it from top to bottom. \"\n            \"Place a summary at the bottom.\"\n        ),\n        multi_modal_context=[\n            dd.ImageContext(\n                column_name=\"base64_image\",\n                data_type=dd.ModalityDataType.BASE64,\n                image_format=dd.ImageFormat.PNG,\n            )\n        ],\n    )\n)\n\ndata_designer.validate(config_builder)\n</pre> # Add a column to generate detailed document summaries config_builder.add_column(     dd.LLMTextColumnConfig(         name=\"summary\",         model_alias=\"vision\",         prompt=(             \"Provide a detailed summary of the content in this image in Markdown format. \"             \"Start from the top of the image and then describe it from top to bottom. \"             \"Place a summary at the bottom.\"         ),         multi_modal_context=[             dd.ImageContext(                 column_name=\"base64_image\",                 data_type=dd.ModalityDataType.BASE64,                 image_format=dd.ImageFormat.PNG,             )         ],     ) )  data_designer.validate(config_builder) <pre>[22:50:28] [INFO] \u2705 Validation passed\n</pre> In\u00a0[11]: Copied! <pre>preview = data_designer.preview(config_builder, num_records=2)\n</pre> preview = data_designer.preview(config_builder, num_records=2) <pre>[22:50:28] [INFO] \ud83d\udc41\ufe0f Preview generation in progress\n</pre> <pre>[22:50:28] [INFO] \u2705 Validation passed\n</pre> <pre>[22:50:28] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:50:28] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:50:28] [INFO]   |-- \ud83d\udc40 Checking 'meta/llama-4-scout-17b-16e-instruct' in provider named 'nvidia' for model alias 'vision'...\n</pre> <pre>[22:50:28] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:50:28] [INFO] \ud83c\udf31 Sampling 2 records from seed dataset\n</pre> <pre>[22:50:28] [INFO]   |-- seed dataset size: 512 records\n</pre> <pre>[22:50:28] [INFO]   |-- sampling strategy: ordered\n</pre> <pre>[22:50:28] [INFO] \ud83d\udcdd llm-text model config for column 'summary'\n</pre> <pre>[22:50:28] [INFO]   |-- model: 'meta/llama-4-scout-17b-16e-instruct'\n</pre> <pre>[22:50:28] [INFO]   |-- model alias: 'vision'\n</pre> <pre>[22:50:28] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:50:28] [INFO]   |-- inference parameters:\n</pre> <pre>[22:50:28] [INFO]   |  |-- generation_type=chat-completion\n</pre> <pre>[22:50:28] [INFO]   |  |-- max_parallel_requests=4\n</pre> <pre>[22:50:28] [INFO]   |  |-- temperature=0.60\n</pre> <pre>[22:50:28] [INFO]   |  |-- top_p=0.95\n</pre> <pre>[22:50:28] [INFO]   |  |-- max_tokens=2048\n</pre> <pre>[22:50:28] [INFO] \u26a1\ufe0f Processing llm-text column 'summary' with 4 concurrent workers\n</pre> <pre>[22:50:28] [INFO] \u23f1\ufe0f llm-text column 'summary' will report progress after each record\n</pre> <pre>[22:50:32] [INFO]   |-- \ud83c\udf17 llm-text column 'summary' progress: 1/2 (50%) complete, 1 ok, 0 failed, 0.24 rec/s, eta 4.2s\n</pre> <pre>[22:50:33] [INFO]   |-- \ud83c\udf15 llm-text column 'summary' progress: 2/2 (100%) complete, 2 ok, 0 failed, 0.46 rec/s, eta 0.0s\n</pre> <pre>[22:50:33] [INFO] \ud83d\udcca Model usage summary:\n</pre> <pre>[22:50:33] [INFO]   |-- model: meta/llama-4-scout-17b-16e-instruct\n</pre> <pre>[22:50:33] [INFO]   |-- tokens: input=1394, output=686, total=2080, tps=441\n</pre> <pre>[22:50:33] [INFO]   |-- requests: success=2, failed=0, total=2, rpm=25\n</pre> <pre>[22:50:33] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:50:33] [INFO]   |-- \ud83d\udcdd column: 'summary'\n</pre> <pre>[22:50:33] [INFO] \ud83c\udf86 Preview complete!\n</pre> In\u00a0[12]: Copied! <pre># Run this cell multiple times to cycle through the 2 preview records.\npreview.display_sample_record()\n</pre> # Run this cell multiple times to cycle through the 2 preview records. preview.display_sample_record() <pre>                                                                                                                   \n                                                   Seed Columns                                                    \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name           \u2503 Value                                                                                          \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 uuid           \u2502 e8750e11-ffb1-4156-83de-abe849701a2f                                                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 image_filename \u2502 images/1810.07757_2.jpg                                                                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 base64_image   \u2502 iVBORw0KGgoAAAANSUhEUgAAAUAAAAIACAIAAAB8QiIMAAEAAElEQVR4nOy9edRt2VUX+vvNufY+53zNbauvVJdKSEUQQ\u2026 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 page           \u2502                                                                                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 options        \u2502 ['A. The variance of the data decreases from panel a to panel d.', 'B. The variance of the     \u2502\n\u2502                \u2502 data increases from panel a to panel d.', 'C. The data presents no variance in any of the      \u2502\n\u2502                \u2502 panels.', 'D. The variance of the data is inconsistent across the panels.', '-']               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 source         \u2502 arxiv_qa                                                                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                 Generated Columns                                                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name    \u2503 Value                                                                                                 \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 summary \u2502 The image presents a collection of eight heatmaps, arranged in two columns of four, accompanied by a  \u2502\n\u2502         \u2502 color bar at the top. The color bar, labeled with values ranging from 0.2 to 1, transitions from blue \u2502\n\u2502         \u2502 to yellow.                                                                                            \u2502\n\u2502         \u2502                                                                                                       \u2502\n\u2502         \u2502 **Column 1:**                                                                                         \u2502\n\u2502         \u2502 - **a)**: The first heatmap displays a predominantly blue and yellow color scheme, with a dark blue   \u2502\n\u2502         \u2502 horizontal band across the middle.                                                                    \u2502\n\u2502         \u2502 - **b)**: The second heatmap features a mix of blue, yellow, and orange hues, with no distinct        \u2502\n\u2502         \u2502 patterns.                                                                                             \u2502\n\u2502         \u2502 - **c)**: The third heatmap exhibits a similar color palette to the second, with no prominent         \u2502\n\u2502         \u2502 features.                                                                                             \u2502\n\u2502         \u2502 - **d)**: The fourth heatmap shows a combination of blue, yellow, and orange colors, with some darker \u2502\n\u2502         \u2502 blue areas.                                                                                           \u2502\n\u2502         \u2502                                                                                                       \u2502\n\u2502         \u2502 **Column 2:**                                                                                         \u2502\n\u2502         \u2502 - **e)**: The fifth heatmap has a yellow and orange color scheme, with a dark blue area in the upper  \u2502\n\u2502         \u2502 center.                                                                                               \u2502\n\u2502         \u2502 - **f)**: The sixth heatmap displays a mix of yellow, orange, and blue colors, with a diagonal        \u2502\n\u2502         \u2502 pattern from top-right to bottom-left.                                                                \u2502\n\u2502         \u2502 - **g)**: The seventh heatmap features a similar color palette to the sixth, with a less defined      \u2502\n\u2502         \u2502 diagonal pattern.                                                                                     \u2502\n\u2502         \u2502 - **h)**: The eighth heatmap shows a combination of yellow, orange, and blue colors, with a curved    \u2502\n\u2502         \u2502 pattern from top-left to bottom-right.                                                                \u2502\n\u2502         \u2502                                                                                                       \u2502\n\u2502         \u2502 **Summary:**                                                                                          \u2502\n\u2502         \u2502 The image presents a series of heatmaps with varying color patterns and features, potentially         \u2502\n\u2502         \u2502 representing data related to frequency (MHz) and time (\u03bcs). The color bar at the top suggests that    \u2502\n\u2502         \u2502 the colors correspond to specific values or intensities.                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                    [index: 0]                                                     \n</pre> In\u00a0[13]: Copied! <pre># The preview dataset is available as a pandas DataFrame.\npreview.dataset\n</pre> # The preview dataset is available as a pandas DataFrame. preview.dataset Out[13]: uuid image_filename base64_image page options source summary 0 e8750e11-ffb1-4156-83de-abe849701a2f images/1810.07757_2.jpg iVBORw0KGgoAAAANSUhEUgAAAUAAAAIACAIAAAB8QiIMAA... ['A. The variance of the data decreases from p... arxiv_qa The image presents a collection of eight heatm... 1 d9c9c627-0146-4893-b5f8-4cfdda606b4f data/scrapped_pdfs_split/pages_extracted/energ... iVBORw0KGgoAAAANSUhEUgAAAYsAAAIACAIAAAD8HddaAA... 9 None pdf The image presents a document with the title \"... In\u00a0[14]: Copied! <pre># Print the analysis as a table.\npreview.analysis.to_report()\n</pre> # Print the analysis as a table. preview.analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2                               \u2502 1                               \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83d\udcdd LLM-Text Columns                                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                  \u2503               \u2503                              \u2503       prompt tokens \u2503       completion tokens \u2503\n\u2503 column name      \u2503     data type \u2503         number unique values \u2503          per record \u2503              per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 summary          \u2502        string \u2502                   2 (100.0%) \u2502        38.0 +/- 0.0 \u2502          340.0 +/- 21.2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> In\u00a0[15]: Copied! <pre># Compare original document with generated summary\nindex = 0  # Change this to view different examples\n\n# Merge preview data with original images for comparison\ncomparison_dataset = preview.dataset.merge(pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\")\n\n# Extract the record for display\nrecord = comparison_dataset.iloc[index]\n\nprint(\"\ud83d\udcc4 Original Document Image:\")\ndisplay(resize_image(record.image, BASE64_IMAGE_HEIGHT))\n\nprint(\"\\n\ud83d\udcdd Generated Summary:\")\nrich.print(Panel(record.summary, title=\"Document Summary\", title_align=\"left\"))\n</pre> # Compare original document with generated summary index = 0  # Change this to view different examples  # Merge preview data with original images for comparison comparison_dataset = preview.dataset.merge(pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\")  # Extract the record for display record = comparison_dataset.iloc[index]  print(\"\ud83d\udcc4 Original Document Image:\") display(resize_image(record.image, BASE64_IMAGE_HEIGHT))  print(\"\\n\ud83d\udcdd Generated Summary:\") rich.print(Panel(record.summary, title=\"Document Summary\", title_align=\"left\")) <pre>\ud83d\udcc4 Original Document Image:\n</pre> <pre>\n\ud83d\udcdd Generated Summary:\n</pre> <pre>\u256d\u2500 Document Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 The image presents a collection of eight heatmaps, arranged in two columns of four, accompanied by a color bar  \u2502\n\u2502 at the top. The color bar, labeled with values ranging from 0.2 to 1, transitions from blue to yellow.          \u2502\n\u2502                                                                                                                 \u2502\n\u2502 **Column 1:**                                                                                                   \u2502\n\u2502 - **a)**: The first heatmap displays a predominantly blue and yellow color scheme, with a dark blue horizontal  \u2502\n\u2502 band across the middle.                                                                                         \u2502\n\u2502 - **b)**: The second heatmap features a mix of blue, yellow, and orange hues, with no distinct patterns.        \u2502\n\u2502 - **c)**: The third heatmap exhibits a similar color palette to the second, with no prominent features.         \u2502\n\u2502 - **d)**: The fourth heatmap shows a combination of blue, yellow, and orange colors, with some darker blue      \u2502\n\u2502 areas.                                                                                                          \u2502\n\u2502                                                                                                                 \u2502\n\u2502 **Column 2:**                                                                                                   \u2502\n\u2502 - **e)**: The fifth heatmap has a yellow and orange color scheme, with a dark blue area in the upper center.    \u2502\n\u2502 - **f)**: The sixth heatmap displays a mix of yellow, orange, and blue colors, with a diagonal pattern from     \u2502\n\u2502 top-right to bottom-left.                                                                                       \u2502\n\u2502 - **g)**: The seventh heatmap features a similar color palette to the sixth, with a less defined diagonal       \u2502\n\u2502 pattern.                                                                                                        \u2502\n\u2502 - **h)**: The eighth heatmap shows a combination of yellow, orange, and blue colors, with a curved pattern from \u2502\n\u2502 top-left to bottom-right.                                                                                       \u2502\n\u2502                                                                                                                 \u2502\n\u2502 **Summary:**                                                                                                    \u2502\n\u2502 The image presents a series of heatmaps with varying color patterns and features, potentially representing data \u2502\n\u2502 related to frequency (MHz) and time (\u03bcs). The color bar at the top suggests that the colors correspond to       \u2502\n\u2502 specific values or intensities.                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> In\u00a0[16]: Copied! <pre>results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-4\")\n</pre> results = data_designer.create(config_builder, num_records=10, dataset_name=\"tutorial-4\") <pre>[22:50:33] [INFO] \ud83c\udfa8 Creating Data Designer dataset\n</pre> <pre>[22:50:33] [INFO] \u2705 Validation passed\n</pre> <pre>[22:50:33] [INFO] \u26d3\ufe0f Sorting column configs into a Directed Acyclic Graph\n</pre> <pre>[22:50:33] [INFO] \ud83e\ude7a Running health checks for models...\n</pre> <pre>[22:50:33] [INFO]   |-- \ud83d\udc40 Checking 'meta/llama-4-scout-17b-16e-instruct' in provider named 'nvidia' for model alias 'vision'...\n</pre> <pre>[22:50:33] [INFO]   |-- \u2705 Passed!\n</pre> <pre>[22:50:33] [INFO] \u23f3 Processing batch 1 of 1\n</pre> <pre>[22:50:33] [INFO] \ud83c\udf31 Sampling 10 records from seed dataset\n</pre> <pre>[22:50:33] [INFO]   |-- seed dataset size: 512 records\n</pre> <pre>[22:50:33] [INFO]   |-- sampling strategy: ordered\n</pre> <pre>[22:50:34] [INFO] \ud83d\udcdd llm-text model config for column 'summary'\n</pre> <pre>[22:50:34] [INFO]   |-- model: 'meta/llama-4-scout-17b-16e-instruct'\n</pre> <pre>[22:50:34] [INFO]   |-- model alias: 'vision'\n</pre> <pre>[22:50:34] [INFO]   |-- model provider: 'nvidia'\n</pre> <pre>[22:50:34] [INFO]   |-- inference parameters:\n</pre> <pre>[22:50:34] [INFO]   |  |-- generation_type=chat-completion\n</pre> <pre>[22:50:34] [INFO]   |  |-- max_parallel_requests=4\n</pre> <pre>[22:50:34] [INFO]   |  |-- temperature=0.60\n</pre> <pre>[22:50:34] [INFO]   |  |-- top_p=0.95\n</pre> <pre>[22:50:34] [INFO]   |  |-- max_tokens=2048\n</pre> <pre>[22:50:34] [INFO] \u26a1\ufe0f Processing llm-text column 'summary' with 4 concurrent workers\n</pre> <pre>[22:50:34] [INFO] \u23f1\ufe0f llm-text column 'summary' will report progress after each record\n</pre> <pre>[22:50:37] [INFO]   |-- \ud83c\udf11 llm-text column 'summary' progress: 1/10 (10%) complete, 1 ok, 0 failed, 0.28 rec/s, eta 31.7s\n</pre> <pre>[22:50:38] [INFO]   |-- \ud83c\udf11 llm-text column 'summary' progress: 2/10 (20%) complete, 2 ok, 0 failed, 0.46 rec/s, eta 17.5s\n</pre> <pre>[22:50:38] [INFO]   |-- \ud83c\udf18 llm-text column 'summary' progress: 3/10 (30%) complete, 3 ok, 0 failed, 0.64 rec/s, eta 11.0s\n</pre> <pre>[22:50:40] [INFO]   |-- \ud83c\udf18 llm-text column 'summary' progress: 4/10 (40%) complete, 4 ok, 0 failed, 0.62 rec/s, eta 9.7s\n</pre> <pre>[22:50:43] [INFO]   |-- \ud83c\udf17 llm-text column 'summary' progress: 5/10 (50%) complete, 5 ok, 0 failed, 0.54 rec/s, eta 9.3s\n</pre> <pre>[22:50:44] [INFO]   |-- \ud83c\udf17 llm-text column 'summary' progress: 6/10 (60%) complete, 6 ok, 0 failed, 0.58 rec/s, eta 6.9s\n</pre> <pre>[22:50:44] [INFO]   |-- \ud83c\udf17 llm-text column 'summary' progress: 7/10 (70%) complete, 7 ok, 0 failed, 0.64 rec/s, eta 4.7s\n</pre> <pre>[22:50:45] [INFO]   |-- \ud83c\udf16 llm-text column 'summary' progress: 8/10 (80%) complete, 8 ok, 0 failed, 0.71 rec/s, eta 2.8s\n</pre> <pre>[22:50:48] [INFO]   |-- \ud83c\udf16 llm-text column 'summary' progress: 9/10 (90%) complete, 9 ok, 0 failed, 0.63 rec/s, eta 1.6s\n</pre> <pre>[22:50:50] [INFO]   |-- \ud83c\udf15 llm-text column 'summary' progress: 10/10 (100%) complete, 10 ok, 0 failed, 0.61 rec/s, eta 0.0s\n</pre> <pre>[22:50:50] [INFO] \ud83d\udcca Model usage summary:\n</pre> <pre>[22:50:50] [INFO]   |-- model: meta/llama-4-scout-17b-16e-instruct\n</pre> <pre>[22:50:50] [INFO]   |-- tokens: input=8130, output=4117, total=12247, tps=729\n</pre> <pre>[22:50:50] [INFO]   |-- requests: success=10, failed=0, total=10, rpm=35\n</pre> <pre>[22:50:50] [INFO] \ud83d\udcd0 Measuring dataset column statistics:\n</pre> <pre>[22:50:50] [INFO]   |-- \ud83d\udcdd column: 'summary'\n</pre> In\u00a0[17]: Copied! <pre># Load the generated dataset as a pandas DataFrame.\ndataset = results.load_dataset()\n\ndataset.head()\n</pre> # Load the generated dataset as a pandas DataFrame. dataset = results.load_dataset()  dataset.head() Out[17]: uuid image_filename base64_image page options source summary 0 e8750e11-ffb1-4156-83de-abe849701a2f images/1810.07757_2.jpg iVBORw0KGgoAAAANSUhEUgAAAUAAAAIACAIAAAB8QiIMAA... ['A. The variance of the data decreases from p... arxiv_qa The image presents a collection of eight heatm... 1 d9c9c627-0146-4893-b5f8-4cfdda606b4f data/scrapped_pdfs_split/pages_extracted/energ... iVBORw0KGgoAAAANSUhEUgAAAYsAAAIACAIAAAD8HddaAA... 9 &lt;NA&gt; pdf The image presents a document with the title \"... 2 0ae54638-ebf6-49c9-b0ba-9ade805c28d9 data/scrapped_pdfs_split/pages_extracted/energ... iVBORw0KGgoAAAANSUhEUgAAAZgAAAIACAIAAAAwhO2xAA... 414 &lt;NA&gt; pdf The image presents a page from a textbook or t... 3 bba667bd-6492-436a-9cfa-6869e18a1390 0fd47b51ae9248ef36669b8619b1223f268edae3e7a44a... iVBORw0KGgoAAAANSUhEUgAAAX0AAAIACAAAAABLRuMPAA... &lt;NA&gt; &lt;NA&gt; docvqa The image depicts a document titled \"CONTINUOU... 4 c820f134-cf52-4dff-87a3-a4f35ccf6847 b335cfb9d442f8925ea41a064cb445a5395577f2345d52... iVBORw0KGgoAAAANSUhEUgAAAY8AAAIACAAAAABf/7+rAA... &lt;NA&gt; &lt;NA&gt; docvqa The image presents a letter from Volunteers Ag... In\u00a0[18]: Copied! <pre># Load the analysis results into memory.\nanalysis = results.load_analysis()\n\nanalysis.to_report()\n</pre> # Load the analysis results into memory. analysis = results.load_analysis()  analysis.to_report() <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Data Designer Dataset Profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n                                                                                                                   \n                                                 Dataset Overview                                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 number of records               \u2503 number of columns               \u2503 percent complete records                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 10                              \u2502 1                               \u2502 100.0%                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n                                                \ud83d\udcdd LLM-Text Columns                                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                  \u2503               \u2503                              \u2503       prompt tokens \u2503       completion tokens \u2503\n\u2503 column name      \u2503     data type \u2503         number unique values \u2503          per record \u2503              per record \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 summary          \u2502        string \u2502                  10 (100.0%) \u2502        38.0 +/- 0.0 \u2502         413.5 +/- 106.5 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                                                   \n                                                                                                                   \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Table Notes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502  1. All token statistics are based on a sample of max(1000, len(dataset)) records.                              \u2502\n\u2502  2. Tokens are calculated using tiktoken's cl100k_base tokenizer.                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                                                                                                                   \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre>"},{"location":"notebooks/4-providing-images-as-context/#data-designer-tutorial-providing-images-as-context-for-vision-based-data-generation","title":"\ud83c\udfa8 Data Designer Tutorial: Providing Images as Context for Vision-Based Data Generation\u00b6","text":""},{"location":"notebooks/4-providing-images-as-context/#what-youll-learn","title":"\ud83d\udcda What you'll learn\u00b6","text":"<p>This notebook demonstrates how to provide images as context to generate text descriptions using vision-language models.</p> <ul> <li>\u2728 Visual Document Processing: Converting images to chat-ready format for model consumption</li> <li>\ud83d\udd0d Vision-Language Generation: Using vision models to generate detailed summaries from images</li> </ul> <p>If this is your first time using Data Designer, we recommend starting with the first notebook in this tutorial series.</p>"},{"location":"notebooks/4-providing-images-as-context/#import-data-designer","title":"\ud83d\udce6 Import Data Designer\u00b6","text":"<ul> <li><p><code>data_designer.config</code> provides access to the configuration API.</p> </li> <li><p><code>DataDesigner</code> is the main interface for data generation.</p> </li> </ul>"},{"location":"notebooks/4-providing-images-as-context/#initialize-the-data-designer-interface","title":"\u2699\ufe0f Initialize the Data Designer interface\u00b6","text":"<ul> <li><p><code>DataDesigner</code> is the main object responsible for managing the data generation process.</p> </li> <li><p>When initialized without arguments, the default model providers are used.</p> </li> </ul>"},{"location":"notebooks/4-providing-images-as-context/#define-model-configurations","title":"\ud83c\udf9b\ufe0f Define model configurations\u00b6","text":"<ul> <li><p>Each <code>ModelConfig</code> defines a model that can be used during the generation process.</p> </li> <li><p>The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).</p> </li> <li><p>The \"model provider\" is the external service that hosts the model (see the model config docs for more details).</p> </li> <li><p>By default, we use build.nvidia.com as the model provider.</p> </li> </ul>"},{"location":"notebooks/4-providing-images-as-context/#initialize-the-data-designer-config-builder","title":"\ud83c\udfd7\ufe0f Initialize the Data Designer Config Builder\u00b6","text":"<ul> <li><p>The Data Designer config defines the dataset schema and generation process.</p> </li> <li><p>The config builder provides an intuitive interface for building this configuration.</p> </li> <li><p>The list of model configs is provided to the builder at initialization.</p> </li> </ul>"},{"location":"notebooks/4-providing-images-as-context/#seed-dataset-creation","title":"\ud83c\udf31 Seed Dataset Creation\u00b6","text":"<p>In this section, we'll prepare our visual documents as a seed dataset for summarization:</p> <ul> <li>Loading Visual Documents: We use the ColPali dataset containing document images</li> <li>Image Processing: Convert images to base64 format for vision model consumption</li> <li>Metadata Extraction: Preserve relevant document information (filename, page number, source, etc.)</li> </ul> <p>The seed dataset will be used to generate detailed text summaries of each document image.</p>"},{"location":"notebooks/4-providing-images-as-context/#iteration-is-key-preview-the-dataset","title":"\ud83d\udd01 Iteration is key \u2013 preview the dataset!\u00b6","text":"<ol> <li><p>Use the <code>preview</code> method to generate a sample of records quickly.</p> </li> <li><p>Inspect the results for quality and format issues.</p> </li> <li><p>Adjust column configurations, prompts, or parameters as needed.</p> </li> <li><p>Re-run the preview until satisfied.</p> </li> </ol>"},{"location":"notebooks/4-providing-images-as-context/#analyze-the-generated-data","title":"\ud83d\udcca Analyze the generated data\u00b6","text":"<ul> <li><p>Data Designer automatically generates a basic statistical analysis of the generated data.</p> </li> <li><p>This analysis is available via the <code>analysis</code> property of generation result objects.</p> </li> </ul>"},{"location":"notebooks/4-providing-images-as-context/#visual-inspection","title":"\ud83d\udd0e Visual Inspection\u00b6","text":"<p>Let's compare the original document image with the generated summary to validate quality:</p>"},{"location":"notebooks/4-providing-images-as-context/#scale-up","title":"\ud83c\udd99 Scale up!\u00b6","text":"<ul> <li><p>Happy with your preview data?</p> </li> <li><p>Use the <code>create</code> method to submit larger Data Designer generation jobs.</p> </li> </ul>"},{"location":"notebooks/4-providing-images-as-context/#next-steps","title":"\u23ed\ufe0f Next Steps\u00b6","text":"<p>Now that you've learned how to use visual context for image summarization in Data Designer, explore more:</p> <ul> <li>Experiment with different vision models for specific document types</li> <li>Try different prompt variations to generate specialized descriptions (e.g., technical details, key findings)</li> <li>Combine vision-based summaries with other column types for multi-modal workflows</li> <li>Apply this pattern to other vision tasks like image captioning, OCR validation, or visual question answering</li> </ul>"},{"location":"plugins/available/","title":"\ud83d\udea7 Coming Soon","text":"<p>This page will list available Data Designer plugins. Stay tuned!</p>"},{"location":"plugins/example/","title":"Example Plugin","text":"<p>Experimental Feature</p> <p>The plugin system is currently experimental and under active development. The documentation, examples, and plugin interface are subject to significant changes in future releases. If you encounter any issues, have questions, or have ideas for improvement, please consider starting a discussion on GitHub.</p>"},{"location":"plugins/example/#example-plugin-column-generator","title":"Example Plugin: Column Generator","text":"<p>Data Designer supports two plugin types: column generators and seed readers. This page walks through a complete column generator example.</p> <p>A Data Designer plugin is implemented as a Python package with three main components:</p> <ol> <li>Configuration Class: Defines the parameters users can configure</li> <li>Implementation Class: Contains the core logic of the plugin</li> <li>Plugin Object: Connects the config and implementation classes to make the plugin discoverable</li> </ol> <p>We recommend separating these into individual files (<code>config.py</code>, <code>impl.py</code>, <code>plugin.py</code>) within a plugin subdirectory. This keeps the code organized, makes it easy to test each component independently, and guards against circular dependencies \u2014 since the config module can be imported without pulling in the engine-level implementation classes, and the plugin object can be discovered without importing either.</p>"},{"location":"plugins/example/#column-generator-plugin-index-multiplier","title":"Column Generator Plugin: Index Multiplier","text":"<p>In this section, we will build a simple column generator plugin that generates values by multiplying the row index by a user-specified multiplier.</p>"},{"location":"plugins/example/#step-1-create-a-python-package","title":"Step 1: Create a Python package","text":"<p>We recommend the following structure for column generator plugins:</p> <pre><code>data-designer-index-multiplier/\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 data_designer_index_multiplier/\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 config.py\n        \u251c\u2500\u2500 impl.py\n        \u2514\u2500\u2500 plugin.py\n</code></pre>"},{"location":"plugins/example/#step-2-create-the-config-class","title":"Step 2: Create the config class","text":"<p>The configuration class defines what parameters users can set when using your plugin. For column generator plugins, it must inherit from SingleColumnConfig and include a discriminator field.</p> <p>Create <code>src/data_designer_index_multiplier/config.py</code>:</p> <pre><code>from typing import Literal\n\nfrom data_designer.config.base import SingleColumnConfig\n\n\nclass IndexMultiplierColumnConfig(SingleColumnConfig):\n    \"\"\"Configuration for the index multiplier column generator.\"\"\"\n\n    # Required: discriminator field with a unique Literal type\n    # This value identifies your plugin and becomes its column_type\n    column_type: Literal[\"index-multiplier\"] = \"index-multiplier\"\n\n    # Configurable parameter for this plugin\n    multiplier: int = 2\n\n    @staticmethod\n    def get_column_emoji() -&gt; str:\n        return \"\u2716\ufe0f\"\n\n    @property\n    def required_columns(self) -&gt; list[str]:\n        \"\"\"Columns that must exist before this generator runs.\"\"\"\n        return []\n\n    @property\n    def side_effect_columns(self) -&gt; list[str]:\n        \"\"\"Additional columns produced beyond the primary column.\"\"\"\n        return []\n</code></pre> <p>Key points:</p> <ul> <li>The <code>column_type</code> field must be a <code>Literal</code> type with a string default</li> <li>This value uniquely identifies your plugin (use kebab-case)</li> <li>Add any custom parameters your plugin needs (here: <code>multiplier</code>)</li> <li><code>SingleColumnConfig</code> is a Pydantic model, so you can leverage all of Pydantic's validation features</li> <li><code>get_column_emoji()</code> returns the emoji displayed in logs for this column type</li> <li><code>required_columns</code> lists any columns this generator depends on (empty if none)</li> <li><code>side_effect_columns</code> lists any additional columns this generator produces beyond the primary column (empty if none)</li> </ul>"},{"location":"plugins/example/#step-3-create-the-implementation-class","title":"Step 3: Create the implementation class","text":"<p>The implementation class defines the actual business logic of the plugin. For column generator plugins, inherit from <code>ColumnGeneratorFullColumn</code> or <code>ColumnGeneratorCellByCell</code> and implement the <code>generate</code> method.</p> <p>Create <code>src/data_designer_index_multiplier/impl.py</code>:</p> <pre><code>import logging\n\nimport pandas as pd\nfrom data_designer.engine.column_generators.generators.base import ColumnGeneratorFullColumn\n\nfrom data_designer_index_multiplier.config import IndexMultiplierColumnConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass IndexMultiplierColumnGenerator(ColumnGeneratorFullColumn[IndexMultiplierColumnConfig]):\n\n    def generate(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Generate the column data.\n\n        Args:\n            data: The current DataFrame being built\n\n        Returns:\n            The DataFrame with the new column added\n        \"\"\"\n        logger.info(\n            f\"Generating column {self.config.name} \"\n            f\"with multiplier {self.config.multiplier}\"\n        )\n\n        data[self.config.name] = data.index * self.config.multiplier\n\n        return data\n</code></pre> <p>Key points:</p> <ul> <li>Generic type <code>ColumnGeneratorFullColumn[IndexMultiplierColumnConfig]</code> connects the implementation to its config</li> <li>You have access to the configuration parameters via <code>self.config</code></li> </ul> <p>Understanding generation_strategy</p> <p>The <code>generation_strategy</code> specifies how the column generator will generate data. You choose a strategy by inheriting from the corresponding base class:</p> <ul> <li> <p><code>ColumnGeneratorFullColumn</code>: Generates the full column (at the batch level) in a single call to <code>generate</code></p> <ul> <li><code>generate</code> must take as input a <code>pd.DataFrame</code> with all previous columns and return a <code>pd.DataFrame</code> with the generated column appended.</li> </ul> </li> <li> <p><code>ColumnGeneratorCellByCell</code>: Generates one cell at a time</p> <ul> <li><code>generate</code> must take as input a <code>dict</code> with key/value pairs for all previous columns and return a <code>dict</code> with an additional key/value for the generated cell</li> <li>Supports concurrent workers via a <code>max_parallel_requests</code> parameter on the configuration</li> </ul> </li> </ul>"},{"location":"plugins/example/#step-4-create-the-plugin-object","title":"Step 4: Create the plugin object","text":"<p>Create a <code>Plugin</code> object that makes the plugin discoverable and connects the implementation and config classes.</p> <p>Create <code>src/data_designer_index_multiplier/plugin.py</code>:</p> <pre><code>from data_designer.plugins import Plugin, PluginType\n\nplugin = Plugin(\n    config_qualified_name=\"data_designer_index_multiplier.config.IndexMultiplierColumnConfig\",\n    impl_qualified_name=\"data_designer_index_multiplier.impl.IndexMultiplierColumnGenerator\",\n    plugin_type=PluginType.COLUMN_GENERATOR,\n)\n</code></pre>"},{"location":"plugins/example/#step-5-package-your-plugin","title":"Step 5: Package your plugin","text":"<p>Create a <code>pyproject.toml</code> file to define your package and register the entry point:</p> <pre><code>[project]\nname = \"data-designer-index-multiplier\"\nversion = \"1.0.0\"\ndescription = \"Data Designer index multiplier plugin\"\nrequires-python = \"&gt;=3.10\"\ndependencies = [\n    \"data-designer\",\n]\n\n# Register this plugin via entry points\n[project.entry-points.\"data_designer.plugins\"]\nindex-multiplier = \"data_designer_index_multiplier.plugin:plugin\"\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src/data_designer_index_multiplier\"]\n</code></pre> <p>Entry Point Registration</p> <p>Plugins are discovered automatically using Python entry points. It is important to register your plugin as an entry point under the <code>data_designer.plugins</code> group.</p> <p>The entry point format is: <pre><code>[project.entry-points.\"data_designer.plugins\"]\n&lt;entry-point-name&gt; = \"&lt;module.path&gt;:&lt;plugin-instance-name&gt;\"\n</code></pre></p>"},{"location":"plugins/example/#step-6-install-and-use-your-plugin-locally","title":"Step 6: Install and use your plugin locally","text":"<p>Install your plugin in editable mode \u2014 this is all you need to start using it. No PyPI publishing required:</p> <pre><code># From the plugin directory\nuv pip install -e .\n</code></pre> <p>That's it. The editable install registers the entry point so Data Designer discovers your plugin automatically. Any changes you make to the plugin source code are picked up immediately without reinstalling.</p> <p>Once installed, your plugin works just like built-in column types:</p> <pre><code>import data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\nfrom data_designer_index_multiplier.config import IndexMultiplierColumnConfig\n\ndata_designer = DataDesigner()\nbuilder = dd.DataDesignerConfigBuilder()\n\n# Add a regular column\nbuilder.add_column(\n    dd.SamplerColumnConfig(\n        name=\"category\",\n        sampler_type=\"category\",\n        params=dd.CategorySamplerParams(values=[\"A\", \"B\", \"C\"]),\n    )\n)\n\n# Add your custom plugin column\nbuilder.add_column(\n    IndexMultiplierColumnConfig(\n        name=\"scaled_index\",\n        multiplier=5,\n    )\n)\n\n# Generate data\nresults = data_designer.create(builder, num_records=10)\nprint(results.load_dataset())\n</code></pre> <p>Output: <pre><code>  category  scaled_index\n0        B             0\n1        A             5\n2        C            10\n3        A            15\n4        B            20\n...\n</code></pre></p>"},{"location":"plugins/example/#validating-your-plugin","title":"Validating Your Plugin","text":"<p>Data Designer provides a testing utility to validate that your plugin is structured correctly. Use <code>assert_valid_plugin</code> to check that your config and implementation classes are properly defined:</p> <pre><code>from data_designer.engine.testing.utils import assert_valid_plugin\nfrom data_designer_index_multiplier.plugin import plugin\n\n# Raises AssertionError with a descriptive message if anything is wrong with the general plugin structure\nassert_valid_plugin(plugin)\n</code></pre> <p>This validates that:</p> <ul> <li>The config class is a subclass of <code>ConfigBase</code></li> <li>For column generator plugins: the implementation class is a subclass of <code>ConfigurableTask</code></li> <li>For seed reader plugins: the implementation class is a subclass of <code>SeedReader</code></li> </ul>"},{"location":"plugins/example/#multiple-plugins-in-one-package","title":"Multiple Plugins in One Package","text":"<p>A single Python package can register multiple plugins. Simply define multiple <code>Plugin</code> instances and register each one as a separate entry point:</p> <pre><code>[project.entry-points.\"data_designer.plugins\"]\nmy-column-generator = \"my_package.plugins.column_generator.plugin:column_generator_plugin\"\nmy-seed-reader = \"my_package.plugins.seed_reader.plugin:seed_reader_plugin\"\n</code></pre> <p>For an example of this pattern, see the end-to-end test plugins in the tests_e2e/ directory.</p> <p>That's it! You now know how to create a Data Designer plugin. A local editable install (<code>uv pip install -e .</code>) is all you need to develop, test, and use your plugin. If you want to make it available for others to install via <code>pip install</code>, publish it to PyPI or your organization's package index.</p>"},{"location":"plugins/overview/","title":"Data Designer Plugins","text":"<p>Experimental Feature</p> <p>The plugin system is currently experimental and under active development. The documentation, examples, and plugin interface are subject to significant changes in future releases. If you encounter any issues, have questions, or have ideas for improvement, please consider starting a discussion on GitHub.</p>"},{"location":"plugins/overview/#what-are-plugins","title":"What are plugins?","text":"<p>Plugins are Python packages that extend Data Designer's capabilities without modifying the core library. Similar to VS Code extensions and Pytest plugins, the plugin system empowers you to build specialized extensions for your specific use cases and share them with the community.</p> <p>Current capabilities: Data Designer supports two plugin types:</p> <ul> <li>Column Generator Plugins: Custom column types you pass to the config builder's add_column method.</li> <li>Seed Reader Plugins: Custom seed dataset readers that let you load data from new sources (e.g., databases, cloud storage, custom formats).</li> </ul> <p>Coming soon: Plugin support for processors, validators, and more!</p>"},{"location":"plugins/overview/#how-do-you-use-plugins","title":"How do you use plugins?","text":"<p>A Data Designer plugin is just a Python package configured with an entry point that points to a Data Designer <code>Plugin</code> object. Using a plugin is as simple as installing the package:</p> <pre><code># Install a local plugin (for development and testing)\nuv pip install -e /path/to/your/plugin\n\n# Or install a published plugin from PyPI\npip install data-designer-{plugin-name}\n</code></pre> <p>Once installed, plugins are automatically discovered and ready to use \u2014 no additional registration or configuration needed. See the example plugin for a complete walkthrough.</p>"},{"location":"plugins/overview/#how-do-you-create-plugins","title":"How do you create plugins?","text":"<p>Creating a plugin involves three main steps:</p>"},{"location":"plugins/overview/#1-implement-the-plugin-components","title":"1. Implement the Plugin Components","text":"<p>Each plugin has three components, and we recommend organizing them into separate files within a plugin subdirectory:</p> <ul> <li><code>config.py</code> -- Configuration class defining user-facing parameters<ul> <li>Column generator plugins: inherit from <code>SingleColumnConfig</code> with a <code>column_type</code> discriminator</li> <li>Seed reader plugins: inherit from <code>SeedSource</code> with a <code>seed_type</code> discriminator</li> </ul> </li> <li><code>impl.py</code> -- Implementation class containing the core logic<ul> <li>Column generator plugins: inherit from <code>ColumnGeneratorFullColumn</code> or <code>ColumnGeneratorCellByCell</code></li> <li>Seed reader plugins: inherit from <code>SeedReader</code></li> </ul> </li> <li><code>plugin.py</code> -- A <code>Plugin</code> instance that connects the config and implementation classes</li> </ul>"},{"location":"plugins/overview/#2-package-your-plugin","title":"2. Package Your Plugin","text":"<ul> <li>Set up a Python package with <code>pyproject.toml</code></li> <li>Register your plugin using entry points under <code>data_designer.plugins</code></li> <li>Define dependencies (including <code>data-designer</code>)</li> </ul>"},{"location":"plugins/overview/#3-install-and-test-locally","title":"3. Install and Test Locally","text":"<ul> <li>Install your plugin locally with <code>uv pip install -e .</code> (editable mode)</li> <li>No publishing required \u2014 your plugin is usable immediately after a local install</li> <li>Iterate on your plugin code with fast feedback</li> </ul>"},{"location":"plugins/overview/#4-share-your-plugin-optional","title":"4. Share Your Plugin (Optional)","text":"<ul> <li>Publish to PyPI or another package index to make it installable by anyone via <code>pip install</code></li> <li>This step is only needed if you want others outside your environment to use the plugin</li> </ul> <p>Ready to get started? See the Example Plugin for a complete walkthrough of creating a column generator plugin.</p>"},{"location":"recipes/cards/","title":"Use Case Recipes","text":"<p>Recipes are a collection of code examples that demonstrate how to leverage Data Designer in specific use cases. Each recipe is a self-contained example that can be run independently.</p> <p>New to Data Designer?</p> <p>Recipes provide working code for specific use cases without detailed explanations. If you're learning Data Designer for the first time, we recommend starting with our tutorial notebooks, which offer step-by-step guidance and explain core concepts. Once you're familiar with the basics, return here for practical, ready-to-use implementations.</p> <p>Tip</p> <p>These recipes use the Open AI model provider by default. Ensure your OpenAI model provider has been set up using the Data Designer CLI before running a recipe.</p> <ul> <li> <p> Text to Python</p> <p>Generate a dataset of natural language instructions paired with Python code implementations, with varying complexity levels and industry focuses.</p> <p>Demonstrates:</p> <ul> <li>Python code generation</li> <li>Python code validation</li> <li>LLM-as-judge</li> </ul> <p> View Recipe Download Code </p> </li> <li> <p> Text to SQL</p> <p>Generate a dataset of natural language instructions paired with SQL code implementations, with varying complexity levels and industry focuses.</p> <p>Demonstrates:</p> <ul> <li>SQL code generation</li> <li>SQL code validation</li> <li>LLM-as-judge</li> </ul> <p> View Recipe Download Code </p> </li> <li> <p> Product Info QA</p> <p>Generate a dataset that contains information about products and associated question/answer pairs.</p> <p>Demonstrates:</p> <ul> <li>Structured outputs</li> <li>Expression columns</li> <li>LLM-as-judge</li> </ul> <p> View Recipe Download Code </p> </li> <li> <p> Multi-Turn Chat</p> <p>Generate a dataset of multi-turn chat conversations between a user and an AI assistant.</p> <p>Demonstrates:</p> <ul> <li>Structured outputs</li> <li>Expression columns</li> <li>LLM-as-judge</li> </ul> <p> View Recipe Download Code </p> </li> <li> <p> PDF Document QA (MCP + Tool Use)</p> <p>Generate grounded Q&amp;A pairs from PDF documents using MCP tool calls and BM25 search.</p> <p>Demonstrates:</p> <ul> <li>MCP tool calling with LocalStdioMCPProvider</li> <li>BM25 lexical search for retrieval</li> <li>Retrieval-grounded QA generation</li> <li>Per-column trace capture</li> </ul> <p> View Recipe Download Code </p> </li> </ul>"},{"location":"recipes/code_generation/text_to_python/","title":"Text to Python","text":"<p>Download Code </p> <pre><code># SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n# /// script\n# requires-python = \"&gt;=3.10\"\n# dependencies = [\n#     \"data-designer\",\n# ]\n# ///\n\"\"\"Text-to-Python Code Generation Recipe\n\nGenerate synthetic instruction-code pairs for Python programming tasks across\ndifferent industries, complexity levels, and programming concepts. Each record\nincludes an instruction, generated code, judge evaluation, and code validation.\n\nPrerequisites:\n    - OPENAI_API_KEY environment variable for OpenAI provider model aliases (default model alias is \"openai-text\").\n    - NVIDIA_API_KEY environment variable for NVIDIA provider model aliases.\n\nRun:\n    # Basic usage (generates 5 records by default)\n    uv run text_to_python.py\n\n    # For help message and available options\n    uv run text_to_python.py --help\n\"\"\"\n\nfrom pathlib import Path\n\nimport data_designer.config as dd\nfrom data_designer.interface import DataDesigner, DatasetCreationResults\n\n\ndef build_config(model_alias: str) -&gt; dd.DataDesignerConfigBuilder:\n    config_builder = dd.DataDesignerConfigBuilder()\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"industry_sector\",\n            sampler_type=dd.SamplerType.CATEGORY,\n            params=dd.CategorySamplerParams(\n                values=[\n                    \"Healthcare\",\n                    \"Finance\",\n                    \"Technology\",\n                ],\n            ),\n        ),\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"topic\",\n            sampler_type=dd.SamplerType.SUBCATEGORY,\n            params=dd.SubcategorySamplerParams(\n                category=\"industry_sector\",\n                values={\n                    \"Healthcare\": [\n                        \"Electronic Health Records (EHR) Systems\",\n                        \"Telemedicine Platforms\",\n                        \"AI-Powered Diagnostic Tools\",\n                    ],\n                    \"Finance\": [\n                        \"Fraud Detection Software\",\n                        \"Automated Trading Systems\",\n                        \"Personal Finance Apps\",\n                    ],\n                    \"Technology\": [\n                        \"Cloud Computing Platforms\",\n                        \"Artificial Intelligence and Machine Learning Platforms\",\n                        \"DevOps and CI/CD Tools\",\n                    ],\n                },\n            ),\n        ),\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"code_complexity\",\n            sampler_type=dd.SamplerType.CATEGORY,\n            params=dd.CategorySamplerParams(\n                values=[\n                    \"Beginner\",\n                    \"Intermediate\",\n                    \"Advanced\",\n                ],\n            ),\n        ),\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"code_concept\",\n            sampler_type=dd.SamplerType.SUBCATEGORY,\n            params=dd.SubcategorySamplerParams(\n                category=\"code_complexity\",\n                values={\n                    \"Beginner\": [\n                        \"Variables\",\n                        \"Data Types\",\n                        \"Functions\",\n                        \"Loops\",\n                        \"Classes\",\n                    ],\n                    \"Intermediate\": [\n                        \"List Comprehensions\",\n                        \"Object-oriented programming\",\n                        \"Lambda Functions\",\n                        \"Web frameworks\",\n                        \"Pandas\",\n                    ],\n                    \"Advanced\": [\n                        \"Multithreading\",\n                        \"Context Managers\",\n                        \"Generators\",\n                    ],\n                },\n            ),\n        ),\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"instruction_phrase\",\n            sampler_type=dd.SamplerType.CATEGORY,\n            params=dd.CategorySamplerParams(\n                values=[\n                    \"Write a function that\",\n                    \"Create a class that\",\n                    \"Implement a script\",\n                    \"Can you create a function\",\n                    \"Develop a module that\",\n                ],\n            ),\n        ),\n    )\n\n    config_builder.add_column(\n        dd.LLMTextColumnConfig(\n            name=\"instruction\",\n            model_alias=model_alias,\n            system_prompt=\"You are an expert at generating clear and specific programming tasks.\",\n            prompt=(\n                \"Generate an instruction to create Python code that solves a specific problem.\\n\"\n                'The instruction should begin with the following phrase: \"{{ instruction_phrase }}\".\\n\\n'\n                \"Important Guidelines:\\n\"\n                \"* Industry Relevance: Ensure the instruction pertains to the {{ industry_sector }} sector and {{ topic }} topic.\\n\"\n                \"* Code Complexity: Tailor the instruction to the {{ code_complexity }} level. Utilize relevant {{ code_concept }} where appropriate to match the complexity level.\\n\"\n                \"* Clarity and Specificity: Make the problem statement clear and unambiguous. Provide sufficient context to understand the requirements without being overly verbose.\\n\"\n                \"* Response Formatting: Do not include any markers such as ### Response ### in the instruction.\\n\"\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMCodeColumnConfig(\n            name=\"code_implementation\",\n            model_alias=model_alias,\n            code_lang=dd.CodeLang.PYTHON,\n            system_prompt=\"You are an expert Python programmer who writes clean, efficient, and well-documented code.\",\n            prompt=(\n                \"Write Python code for the following instruction:\\n\"\n                \"Instruction: {{ instruction }}\\n\\n\"\n                \"Important Guidelines:\\n\"\n                \"* Code Quality: Your code should be clean, complete, self-contained, and accurate.\\n\"\n                \"* Code Validity: Please ensure that your Python code is executable and does not contain any errors.\\n\"\n                \"* Packages: Remember to import any necessary libraries, and to use all libraries you import.\\n\"\n                \"* Complexity &amp; Concepts: The code should be written at a {{ code_complexity }} level, making use of concepts such as {{ code_concept }}.\\n\"\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMJudgeColumnConfig(\n            name=\"code_judge_result\",\n            model_alias=model_alias,\n            prompt=TEXT_TO_PYTHON_JUDGE_TEMPLATE,\n            scores=python_scoring,\n        )\n    )\n\n    config_builder.add_column(\n        dd.ValidationColumnConfig(\n            name=\"code_validity_result\",\n            validator_type=dd.ValidatorType.CODE,\n            target_columns=[\"code_implementation\"],\n            validator_params=dd.CodeValidatorParams(code_lang=dd.CodeLang.PYTHON),\n            batch_size=100,\n        )\n    )\n\n    return config_builder\n\n\ndef create_dataset(\n    config_builder: dd.DataDesignerConfigBuilder,\n    num_records: int,\n    artifact_path: Path | str | None = None,\n) -&gt; DatasetCreationResults:\n    data_designer = DataDesigner(artifact_path=artifact_path)\n    results = data_designer.create(config_builder, num_records=num_records)\n    return results\n\n\nTEXT_TO_PYTHON_JUDGE_TEMPLATE = \"\"\"\\\nYou are an expert in Python programming, with specialized knowledge in software engineering, data science, and algorithmic problem-solving.\n\nYou think about potential flaws and errors in the code. You are a tough critic, but a fair one.\n\nTake a deep breath and use the Python Code Quality Rubric below to score the **Generated Python Code** based on the INSTRUCTIONS.\n\n#### INSTRUCTIONS\nThe Generated Python Code should be a valid response to the Natural Language Prompt below\n\nNatural Language Prompt:\n{{ instruction }}\n\nGenerated Python Code\n{{ code_implementation }}\n\"\"\"\n\n\npython_scoring = [\n    dd.Score(\n        name=\"Relevance\",\n        description=\"Adherence to INSTRUCTIONS and CONTEXT\",\n        options={\n            4: \"Perfectly meets all specified requirements.\",\n            3: \"Meets most requirements with minor deviations.\",\n            2: \"Moderate deviation from the instructions.\",\n            1: \"Significant deviations from the instructions.\",\n            0: \"Does not adhere to the instructions.\",\n        },\n    ),\n    dd.Score(\n        name=\"Pythonic\",\n        description=\"Pythonic Code and Best Practices (Does the code follow Python conventions and best practices?)\",\n        options={\n            4: \"The code exemplifies Pythonic principles, making excellent use of Python-specific constructs, standard library modules and programming idioms; follows all relevant PEPs.\",\n            3: \"The code closely follows Python conventions and adheres to many best practices; good use of Python-specific constructs, standard library modules and programming idioms.\",\n            2: \"The code generally follows Python conventions but has room for better alignment with Pythonic practices.\",\n            1: \"The code loosely follows Python conventions, with several deviations from best practices.\",\n            0: \"The code does not follow Python conventions or best practices, using non-Pythonic approaches.\",\n        },\n    ),\n    dd.Score(\n        name=\"Readability\",\n        description=\"Readability and Maintainability (Is the Python code easy to understand and maintain?)\",\n        options={\n            4: (\n                \"The code is excellently formatted, follows PEP 8 guidelines, is elegantly concise and clear, uses meaningful variable names, \"\n                \"ensuring high readability and ease of maintenance; organizes complex logic well. Docstrings are given in a Google Docstring format.\"\n            ),\n            3: \"The code is well-formatted in the sense of code-as-documentation, making it relatively easy to understand and maintain; uses descriptive names and organizes logic clearly.\",\n            2: \"The code is somewhat readable with basic formatting and some comments, but improvements are needed; needs better use of descriptive names and organization.\",\n            1: \"The code has minimal formatting, making it hard to understand; lacks meaningful names and organization.\",\n            0: \"The code is unreadable, with no attempt at formatting or description.\",\n        },\n    ),\n    dd.Score(\n        name=\"Efficiency\",\n        description=\"Efficiency and Performance (Is the code optimized for performance?)\",\n        options={\n            4: \"The solution is highly efficient, using appropriate data structures and algorithms; avoids unnecessary computations and optimizes for both time and space complexity.\",\n            3: \"The solution is efficient, with good use of Python's built-in functions and libraries; minor areas for optimization.\",\n            2: \"The solution is moderately efficient, but misses some opportunities for optimization; uses some inefficient patterns.\",\n            1: \"The solution shows poor efficiency, with notable performance issues; lacks effective optimization techniques.\",\n            0: \"The solution is highly inefficient; overlooks fundamental optimization practices, resulting in significant performance issues.\",\n        },\n    ),\n]\n\n\nif __name__ == \"__main__\":\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser()\n    parser.add_argument(\"--model-alias\", type=str, default=\"openai-text\")\n    parser.add_argument(\"--num-records\", type=int, default=5)\n    parser.add_argument(\"--artifact-path\", type=str, default=None)\n    args = parser.parse_args()\n\n    config_builder = build_config(model_alias=args.model_alias)\n    results = create_dataset(config_builder, num_records=args.num_records, artifact_path=args.artifact_path)\n\n    print(f\"Dataset saved to: {results.artifact_storage.final_dataset_path}\")\n\n    results.load_analysis().to_report()\n</code></pre>"},{"location":"recipes/code_generation/text_to_sql/","title":"Text to SQL","text":"<p>Download Code </p> <pre><code># SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n# /// script\n# requires-python = \"&gt;=3.10\"\n# dependencies = [\n#     \"data-designer\",\n# ]\n# ///\n\"\"\"Text-to-SQL Code Generation Recipe\n\nGenerate synthetic instruction-SQL pairs for database tasks across different\nindustries, complexity levels, and SQL concepts. Each record includes an\ninstruction, database context (CREATE TABLE + sample data), generated SQL query,\ncode validation, and judge evaluation.\n\nPrerequisites:\n    - OPENAI_API_KEY environment variable for OpenAI provider model aliases (default model alias is \"openai-text\").\n    - NVIDIA_API_KEY environment variable for NVIDIA provider model aliases.\n\nRun:\n    # Basic usage (generates 5 records by default)\n    uv run text_to_sql.py\n\n    # For help message and available options\n    uv run text_to_sql.py --help\n\"\"\"\n\nfrom pathlib import Path\n\nimport data_designer.config as dd\nfrom data_designer.interface import DataDesigner, DatasetCreationResults\n\n\ndef build_config(model_alias: str) -&gt; dd.DataDesignerConfigBuilder:\n    config_builder = dd.DataDesignerConfigBuilder()\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"industry_sector\",\n            sampler_type=dd.SamplerType.CATEGORY,\n            params=dd.CategorySamplerParams(\n                values=[\"Healthcare\", \"Finance\", \"Technology\"],\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"topic\",\n            sampler_type=dd.SamplerType.SUBCATEGORY,\n            params=dd.SubcategorySamplerParams(\n                category=\"industry_sector\",\n                values={\n                    \"Healthcare\": [\n                        \"Electronic Health Records (EHR) Systems\",\n                        \"Telemedicine Platforms\",\n                        \"AI-Powered Diagnostic Tools\",\n                    ],\n                    \"Finance\": [\n                        \"Fraud Detection Software\",\n                        \"Automated Trading Systems\",\n                        \"Personal Finance Apps\",\n                    ],\n                    \"Technology\": [\n                        \"Cloud Computing Platforms\",\n                        \"Artificial Intelligence and Machine Learning Platforms\",\n                        \"DevOps and CI/CD Tools\",\n                    ],\n                },\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"sql_complexity\",\n            sampler_type=dd.SamplerType.CATEGORY,\n            params=dd.CategorySamplerParams(\n                values=[\"Beginner\", \"Intermediate\", \"Advanced\"],\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"sql_concept\",\n            sampler_type=dd.SamplerType.SUBCATEGORY,\n            params=dd.SubcategorySamplerParams(\n                category=\"sql_complexity\",\n                values={\n                    \"Beginner\": [\n                        \"Basic SELECT Statements\",\n                        \"WHERE Clauses\",\n                        \"Basic JOINs\",\n                        \"INSERT, UPDATE, DELETE\",\n                    ],\n                    \"Intermediate\": [\n                        \"Aggregation Functions\",\n                        \"Multiple JOINs\",\n                        \"Subqueries\",\n                        \"Views\",\n                    ],\n                    \"Advanced\": [\n                        \"Window Functions\",\n                        \"Common Table Expressions (CTEs)\",\n                        \"Stored Procedures\",\n                        \"Query Optimization\",\n                    ],\n                },\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"sql_task_type\",\n            sampler_type=dd.SamplerType.CATEGORY,\n            params=dd.CategorySamplerParams(\n                values=[\n                    \"Data Retrieval\",\n                    \"Data Manipulation\",\n                    \"Analytics and Reporting\",\n                    \"Data Transformation\",\n                ],\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"instruction_phrase\",\n            sampler_type=dd.SamplerType.CATEGORY,\n            params=dd.CategorySamplerParams(\n                values=[\n                    \"Write an SQL query that\",\n                    \"Create an SQL statement to\",\n                    \"Develop an SQL query to\",\n                    \"Can you write SQL that\",\n                    \"Formulate an SQL query that\",\n                ],\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMTextColumnConfig(\n            name=\"sql_prompt\",\n            model_alias=model_alias,\n            system_prompt=\"You are an expert at generating clear and specific SQL tasks.\",\n            prompt=SQL_PROMPT_TEXT,\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMCodeColumnConfig(\n            name=\"sql_context\",\n            model_alias=model_alias,\n            code_lang=dd.CodeLang.SQL_ANSI,\n            system_prompt=(\n                \"You are an expert SQL database designer who creates clean, efficient, and \"\n                \"well-structured database schemas.\"\n            ),\n            prompt=SQL_CONTEXT_TEXT,\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMCodeColumnConfig(\n            name=\"sql\",\n            model_alias=model_alias,\n            code_lang=dd.CodeLang.SQL_ANSI,\n            system_prompt=\"You are an expert SQL programmer who writes clean, efficient, and well-structured queries.\",\n            prompt=SQL_CODE_TEXT,\n        )\n    )\n\n    config_builder.add_column(\n        dd.ValidationColumnConfig(\n            name=\"code_validity_result\",\n            validator_type=dd.ValidatorType.CODE,\n            target_columns=[\"sql\"],\n            validator_params=dd.CodeValidatorParams(\n                code_lang=dd.CodeLang.SQL_ANSI,\n            ),\n            batch_size=100,\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMJudgeColumnConfig(\n            name=\"code_judge_result\",\n            model_alias=model_alias,\n            prompt=TEXT_TO_SQL_JUDGE_TEMPLATE,\n            scores=sql_scoring,\n        )\n    )\n\n    return config_builder\n\n\ndef create_dataset(\n    config_builder: dd.DataDesignerConfigBuilder,\n    num_records: int,\n    artifact_path: Path | str | None = None,\n) -&gt; DatasetCreationResults:\n    data_designer = DataDesigner(artifact_path=artifact_path)\n    results = data_designer.create(config_builder, num_records=num_records)\n    return results\n\n\nSQL_PROMPT_TEXT = (\n    \"Generate an instruction to create SQL code that solves a specific problem.\\n\"\n    \"Each instruction should begin with one of the following phrases: {{instruction_phrase}}.\\n\\n\"\n    \"Important Guidelines:\\n\"\n    \"* Industry Relevance: Ensure the instruction pertains to the {{industry_sector}} sector and {{topic}} topic.\\n\"\n    \"* SQL Complexity: Tailor the instruction to the {{sql_complexity}} level. Utilize relevant {{sql_concept}} \"\n    \"where appropriate to match the complexity level.\\n\"\n    \"* Task Type: The instruction should involve a {{sql_task_type}} task.\\n\"\n    \"* Clarity and Specificity: Make the problem statement clear and unambiguous. Provide sufficient context to \"\n    \"understand the requirements without being overly verbose.\\n\"\n    \"* Response Formatting: Do not include any markers such as ### Response ### in the instruction.\\n\"\n)\n\nSQL_CONTEXT_TEXT = (\n    \"Generate the SQL for creating database tables that would be relevant for the following instruction:\\n\"\n    \"Instruction: {{sql_prompt}}\\n\\n\"\n    \"Important Guidelines:\\n\"\n    \"* Relevance: Ensure all tables are directly related to the {{industry_sector}} sector and {{topic}} topic.\\n\"\n    \"* Completeness: Include all essential columns with appropriate data types, primary/foreign keys, and necessary constraints.\\n\"\n    \"* Realism: Use realistic table structures typical for the specified industry.\\n\"\n    \"* Executable SQL: Provide complete CREATE TABLE statements that can be run without modification.\\n\"\n    \"* Consistency: Use consistent naming conventions (e.g., snake_case for table and column names).\\n\"\n    \"* Sample Data: Include INSERT statements with sample data that makes sense for the tables (at least 5-10 rows per table).\"\n)\n\nSQL_CODE_TEXT = (\n    \"Write SQL code for the following instruction based on the provided database context:\\n\"\n    \"Instruction: {{sql_prompt}}\\n\\n\"\n    \"Database Context:\\n\"\n    \"{{sql_context}}\\n\\n\"\n    \"Important Guidelines:\\n\"\n    \"* Code Quality: Your SQL should be clean, complete, self-contained and accurate.\\n\"\n    \"* Code Validity: Please ensure that your SQL code is executable and does not contain any errors.\\n\"\n    \"* Context: Base your query on the provided database context. Only reference tables and columns that \"\n    \"exist in the context.\\n\"\n    \"* Complexity &amp; Concepts: The SQL should be written at a {{sql_complexity}} level, making use of \"\n    \"concepts such as {{sql_concept}}.\\n\"\n    \"* Task Type: Ensure your solution implements the appropriate {{sql_task_type}} operation.\\n\"\n    \"* Comments: Include brief comments explaining the key parts of your query.\\n\"\n)\n\n\nTEXT_TO_SQL_JUDGE_TEMPLATE = \"\"\"\\\nYou are an expert in SQL with deep knowledge of relational modeling, query semantics,\nand performance tuning across common dialects (e.g., PostgreSQL, MySQL, SQLite, SQL Server).\nYou think critically about correctness, readability, and efficiency.\n\nUse the SQL Query Quality Rubric below to score the **Generated SQL Query** based on the INSTRUCTIONS.\n\n#### INSTRUCTIONS\nThe Generated SQL Query should be a valid response to the Natural Language Prompt below\n\nNatural Language Prompt:\n{{ sql_prompt }}\n\nDatabase Context:\n{{ sql_context }}\n\nGenerated SQL Query\n{{ sql }}\n\"\"\"\n\n\nsql_scoring = [\n    dd.Score(\n        name=\"Relevance\",\n        description=\"Adherence to INSTRUCTIONS and CONTEXT\",\n        options={\n            4: \"Perfectly meets all specified requirements.\",\n            3: \"Meets most requirements with minor deviations.\",\n            2: \"Moderate deviation from the instructions.\",\n            1: \"Significant deviations from the instructions.\",\n            0: \"Does not adhere to the instructions.\",\n        },\n    ),\n    dd.Score(\n        name=\"SQL Correctness\",\n        description=\"Syntax and semantic correctness; returns the intended result\",\n        options={\n            4: \"Valid SQL with correct joins, filters, grouping/aggregation, and NULL handling; produces the intended result set under the stated/implicit dialect.\",\n            3: \"Generally correct with minor issues (e.g., edge-case NULLs, minor grouping detail) but still likely yields the intended result.\",\n            2: \"Partially correct; noticeable semantic mistakes (joins, grouping, filters) that may change results or fail in edge cases.\",\n            1: \"Largely incorrect; major semantic or syntactic errors likely causing failure or wrong results.\",\n            0: \"Invalid SQL or unrelated to the task; will not run or cannot produce a meaningful result.\",\n        },\n    ),\n    dd.Score(\n        name=\"Readability\",\n        description=\"Formatting, clarity, and maintainability\",\n        options={\n            4: \"Cleanly formatted (keywords/clauses consistently styled), clear structure (CTEs/subqueries where helpful), meaningful table/column aliases, and concise.\",\n            3: \"Generally readable with consistent formatting and understandable aliases; could be organized slightly better.\",\n            2: \"Somewhat readable but inconsistent formatting or confusing aliasing; structure is harder to follow.\",\n            1: \"Poorly formatted and hard to read; unclear structure and aliasing.\",\n            0: \"Unreadable or chaotic; no meaningful structure or styling.\",\n        },\n    ),\n    dd.Score(\n        name=\"Efficiency\",\n        description=\"Query performance best practices\",\n        options={\n            4: \"Uses sargable predicates, appropriate joins, selective filters early, avoids SELECT *, unnecessary DISTINCT, and wasteful subqueries; likely to use indexes effectively.\",\n            3: \"Mostly efficient; minor opportunities for improvement (e.g., simplifying expressions, reducing data early).\",\n            2: \"Moderate inefficiencies (e.g., non-sargable filters, unnecessary nested subqueries, broad SELECT *).\",\n            1: \"Notably inefficient patterns likely causing large scans or poor plans.\",\n            0: \"Highly inefficient; ignores basic best practices and likely to perform very poorly.\",\n        },\n    ),\n]\n\nif __name__ == \"__main__\":\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser()\n    parser.add_argument(\"--model-alias\", type=str, default=\"openai-text\")\n    parser.add_argument(\"--num-records\", type=int, default=5)\n    parser.add_argument(\"--artifact-path\", type=str, default=None)\n    args = parser.parse_args()\n\n    config_builder = build_config(model_alias=args.model_alias)\n    results = create_dataset(config_builder, num_records=args.num_records, artifact_path=args.artifact_path)\n\n    print(f\"Dataset saved to: {results.artifact_storage.final_dataset_path}\")\n\n    results.load_analysis().to_report()\n</code></pre>"},{"location":"recipes/mcp_and_tooluse/basic_mcp/","title":"Basic mcp","text":"<p>Download Code </p> <pre><code># SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n# /// script\n# requires-python = \"&gt;=3.10\"\n# dependencies = [\n#     \"data-designer\",\n#     \"mcp\",\n# ]\n# ///\n\"\"\"Basic MCP Recipe: Simple Tool Use Example\n\nThis recipe demonstrates the minimal MCP tool-calling workflow with Data Designer:\n\n1) Define a simple MCP server with basic tools (get_fact, add_numbers)\n2) Configure Data Designer to use the MCP tools\n3) Generate data that requires tool calls to complete\n\nPrerequisites:\n    - OPENAI_API_KEY environment variable for OpenAI provider model aliases.\n    - NVIDIA_API_KEY environment variable for NVIDIA provider model aliases (default model alias is \"nvidia-text\").\n\nRun:\n    # Basic usage (generates 2 records by default)\n    uv run basic_mcp.py\n\n    # For help message and available options\n    uv run basic_mcp.py --help\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nfrom pathlib import Path\n\nfrom mcp.server.fastmcp import FastMCP\n\nimport data_designer.config as dd\nfrom data_designer.interface import DataDesigner\n\nMCP_SERVER_NAME = \"basic-tools\"\n\n\n# =============================================================================\n# MCP Server Definition\n# =============================================================================\n\nmcp_server = FastMCP(MCP_SERVER_NAME)\n\n# Simple knowledge base for the get_fact tool\nFACTS = {\n    \"python\": \"Python was created by Guido van Rossum and first released in 1991.\",\n    \"earth\": \"Earth is the third planet from the Sun and has one natural satellite, the Moon.\",\n    \"water\": \"Water (H2O) freezes at 0\u00b0C (32\u00b0F) and boils at 100\u00b0C (212\u00b0F) at sea level.\",\n    \"light\": \"The speed of light in a vacuum is approximately 299,792 kilometers per second.\",\n}\n\n\n@mcp_server.tool()\ndef get_fact(topic: str) -&gt; str:\n    \"\"\"Get a fact about a topic from the knowledge base.\n\n    Args:\n        topic: The topic to look up (e.g., \"python\", \"earth\", \"water\", \"light\")\n\n    Returns:\n        A fact about the topic, or an error message if not found.\n    \"\"\"\n    topic_lower = topic.lower()\n    if topic_lower in FACTS:\n        return json.dumps({\"topic\": topic, \"fact\": FACTS[topic_lower]})\n    return json.dumps({\"error\": f\"No fact found for topic: {topic}\", \"available_topics\": list(FACTS.keys())})\n\n\n@mcp_server.tool()\ndef add_numbers(a: float, b: float) -&gt; str:\n    \"\"\"Add two numbers together.\n\n    Args:\n        a: First number\n        b: Second number\n\n    Returns:\n        The sum of the two numbers.\n    \"\"\"\n    result = a + b\n    return json.dumps({\"a\": a, \"b\": b, \"sum\": result})\n\n\n@mcp_server.tool()\ndef list_topics() -&gt; str:\n    \"\"\"List all available topics in the knowledge base.\n\n    Returns:\n        List of available topics.\n    \"\"\"\n    return json.dumps({\"topics\": list(FACTS.keys())})\n\n\n# =============================================================================\n# Data Designer Configuration\n# =============================================================================\n\n\ndef build_config(model_alias: str, provider_name: str) -&gt; dd.DataDesignerConfigBuilder:\n    \"\"\"Build the Data Designer configuration for basic tool use.\"\"\"\n    tool_config = dd.ToolConfig(\n        tool_alias=\"basic-tools\",\n        providers=[provider_name],\n        allow_tools=[\"get_fact\", \"add_numbers\", \"list_topics\"],\n        max_tool_call_turns=5,\n        timeout_sec=30.0,\n    )\n\n    config_builder = dd.DataDesignerConfigBuilder(tool_configs=[tool_config])\n\n    # Add a seed column with topics to look up\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"topic\",\n            sampler_type=dd.SamplerType.CATEGORY,\n            params=dd.CategorySamplerParams(values=[\"python\", \"earth\", \"water\", \"light\"]),\n        )\n    )\n\n    # Add a column that uses the get_fact tool\n    config_builder.add_column(\n        dd.LLMTextColumnConfig(\n            name=\"fact_response\",\n            model_alias=model_alias,\n            prompt=(\n                \"Use the get_fact tool to look up information about '{{ topic }}', \"\n                \"then provide a one-sentence summary of what you learned.\"\n            ),\n            system_prompt=\"You must call the get_fact tool before answering. Only use information from tool results.\",\n            tool_alias=\"basic-tools\",\n            with_trace=dd.TraceType.ALL_MESSAGES,\n        )\n    )\n\n    # Add a column that uses the add_numbers tool\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"num_a\",\n            sampler_type=dd.SamplerType.UNIFORM,\n            params=dd.UniformSamplerParams(low=1, high=100),\n        )\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"num_b\",\n            sampler_type=dd.SamplerType.UNIFORM,\n            params=dd.UniformSamplerParams(low=1, high=100),\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMTextColumnConfig(\n            name=\"math_response\",\n            model_alias=model_alias,\n            prompt=(\n                \"Use the add_numbers tool to calculate {{ num_a }} + {{ num_b }}, \"\n                \"then report the result in a complete sentence.\"\n            ),\n            system_prompt=\"You must call the add_numbers tool to perform the calculation. Report the exact result.\",\n            tool_alias=\"basic-tools\",\n            with_trace=dd.TraceType.ALL_MESSAGES,\n        )\n    )\n\n    return config_builder\n\n\n# =============================================================================\n# Main Entry Points\n# =============================================================================\n\n\ndef serve() -&gt; None:\n    \"\"\"Run the MCP server (called when launched as subprocess by Data Designer).\"\"\"\n    mcp_server.run()\n\n\ndef parse_args() -&gt; argparse.Namespace:\n    \"\"\"Parse command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Basic MCP tool use example with Data Designer.\")\n    subparsers = parser.add_subparsers(dest=\"command\")\n\n    # 'serve' subcommand for running the MCP server\n    subparsers.add_parser(\"serve\", help=\"Run the MCP server (used by Data Designer)\")\n\n    # Default command arguments (demo mode)\n    parser.add_argument(\"--model-alias\", type=str, default=\"nvidia-text\", help=\"Model alias to use for generation\")\n    parser.add_argument(\"--num-records\", type=int, default=2, help=\"Number of records to generate\")\n    # For compatibility with Makefile test-run-recipes target (ignored in demo mode)\n    parser.add_argument(\"--artifact-path\", type=str, default=None, help=argparse.SUPPRESS)\n\n    return parser.parse_args()\n\n\ndef main() -&gt; None:\n    \"\"\"Main entry point for the demo.\"\"\"\n    args = parse_args()\n\n    # Handle 'serve' subcommand\n    if args.command == \"serve\":\n        serve()\n        return\n\n    # Demo mode: run Data Designer with the MCP server\n    if os.environ.get(\"NVIDIA_API_KEY\") is None and args.model_alias.startswith(\"nvidia\"):\n        raise RuntimeError(\"NVIDIA_API_KEY must be set when using NVIDIA model aliases.\")\n\n    # Configure MCP provider to run via stdio transport (local subprocess)\n    mcp_provider = dd.LocalStdioMCPProvider(\n        name=MCP_SERVER_NAME,\n        command=sys.executable,\n        args=[str(Path(__file__).resolve()), \"serve\"],\n    )\n\n    config_builder = build_config(\n        model_alias=args.model_alias,\n        provider_name=MCP_SERVER_NAME,\n    )\n\n    data_designer = DataDesigner(mcp_providers=[mcp_provider])\n    preview_results = data_designer.preview(config_builder, num_records=args.num_records)\n\n    # Display results\n    print(\"\\n\" + \"=\" * 60)\n    print(\"GENERATED DATA\")\n    print(\"=\" * 60)\n    preview_results.display_sample_record()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"recipes/mcp_and_tooluse/pdf_qa/","title":"PDF Document QA","text":"<p>Download Code </p> <pre><code># SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n# /// script\n# requires-python = \"&gt;=3.10\"\n# dependencies = [\n#     \"data-designer\",\n#     \"mcp\",\n#     \"bm25s\",\n#     \"pymupdf\",\n#     \"rich\",\n# ]\n# ///\n\"\"\"MCP + Tool Use Recipe: Document Q&amp;A with BM25S Lexical Search\n\nThis recipe demonstrates an end-to-end MCP tool-calling workflow:\n\n1) Load one or more PDF documents from URLs or local paths.\n2) Index them with BM25S for fast lexical search.\n3) Use Data Designer tool calls (`search_docs`) to generate grounded Q&amp;A pairs.\n\nPrerequisites:\n    - OPENAI_API_KEY environment variable for OpenAI provider model aliases.\n    - NVIDIA_API_KEY environment variable for NVIDIA provider model aliases (default model alias is \"nvidia-reasoning\").\n\nRun:\n    # Basic usage with default sample PDF (generates 4 Q&amp;A pairs)\n    uv run pdf_qa.py\n\n    # For help message and available options\n    uv run pdf_qa.py --help\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport io\nimport json\nimport os\nimport sys\nfrom pathlib import Path\nfrom urllib.parse import urlparse\nfrom urllib.request import urlopen\n\nimport bm25s\nimport fitz\nfrom mcp.server.fastmcp import FastMCP\nfrom pydantic import BaseModel, Field\n\nimport data_designer.config as dd\nfrom data_designer.config.preview_results import PreviewResults\nfrom data_designer.interface import DataDesigner\n\nDEFAULT_PDF_URL = \"https://research.nvidia.com/labs/nemotron/files/NVIDIA-Nemotron-3-Nano-Technical-Report.pdf\"\nMCP_SERVER_NAME = \"doc-bm25-search\"\n\n# Global state for the BM25 index (populated at server startup)\n_bm25_retriever: bm25s.BM25 | None = None\n_corpus: list[dict[str, str]] = []\n\n\nclass QAPair(BaseModel):\n    question: str = Field(..., description=\"A question grounded in the document text.\")\n    answer: str = Field(..., description=\"A concise answer grounded in the supporting passage.\")\n    supporting_passage: str = Field(\n        ..., description=\"A short excerpt (2-4 sentences) copied from the search result that supports the answer.\"\n    )\n    citation: str = Field(\n        ..., description=\"The citation (e.g. source url, page number, etc) of the supporting passage.\"\n    )\n\n\nclass TopicList(BaseModel):\n    topics: list[str] = Field(\n        ...,\n        description=\"High-level topics covered by the document.\",\n    )\n\n\ndef _is_url(path_or_url: str) -&gt; bool:\n    \"\"\"Check if the given string is a URL.\"\"\"\n    parsed = urlparse(path_or_url)\n    return parsed.scheme in (\"http\", \"https\")\n\n\ndef _get_source_name(path_or_url: str) -&gt; str:\n    \"\"\"Extract a human-readable source name from a path or URL.\"\"\"\n    if _is_url(path_or_url):\n        parsed = urlparse(path_or_url)\n        return Path(parsed.path).name or parsed.netloc\n    return Path(path_or_url).name\n\n\ndef extract_pdf_text(path_or_url: str) -&gt; list[dict[str, str]]:\n    \"\"\"Extract text from a PDF file or URL, returning a list of passages with metadata.\n\n    Each passage corresponds to a page from the PDF.\n\n    Args:\n        path_or_url: Either a local file path or a URL to a PDF document.\n            URLs are streamed directly into memory without saving to disk.\n\n    Returns:\n        List of passage dictionaries with 'text', 'page', and 'source' keys.\n    \"\"\"\n    passages: list[dict[str, str]] = []\n    source_name = _get_source_name(path_or_url)\n\n    if _is_url(path_or_url):\n        with urlopen(path_or_url) as response:\n            pdf_bytes = response.read()\n        doc = fitz.open(stream=io.BytesIO(pdf_bytes), filetype=\"pdf\")\n    else:\n        doc = fitz.open(path_or_url)\n\n    for page_num in range(len(doc)):\n        page = doc[page_num]\n        text = page.get_text(\"text\").strip()\n        if text:\n            passages.append(\n                {\n                    \"text\": text,\n                    \"page\": str(page_num + 1),\n                    \"source\": source_name,\n                }\n            )\n\n    doc.close()\n    return passages\n\n\ndef build_bm25_index(passages: list[dict[str, str]]) -&gt; bm25s.BM25:\n    \"\"\"Build a BM25S index from the extracted passages.\"\"\"\n    corpus_texts = [p[\"text\"] for p in passages]\n    corpus_tokens = bm25s.tokenize(corpus_texts, stopwords=\"en\")\n\n    retriever = bm25s.BM25()\n    retriever.index(corpus_tokens)\n\n    return retriever\n\n\ndef initialize_search_index(pdf_sources: list[str]) -&gt; None:\n    \"\"\"Load PDFs from paths/URLs and build the BM25 index.\n\n    Args:\n        pdf_sources: List of PDF file paths or URLs to index.\n    \"\"\"\n    global _bm25_retriever, _corpus\n\n    _corpus = []\n    for source in pdf_sources:\n        passages = extract_pdf_text(source)\n        _corpus.extend(passages)\n\n    if _corpus:\n        _bm25_retriever = build_bm25_index(_corpus)\n\n\n# MCP Server Definition\nmcp_server = FastMCP(MCP_SERVER_NAME)\n\n\n@mcp_server.tool()\ndef search_docs(query: str, limit: int = 5, document: str = \"\", page: str = \"\") -&gt; str:\n    \"\"\"Search through documents using BM25 lexical search.\n\n    BM25 is a keyword-based retrieval algorithm that matches exact terms. For best results:\n\n    - Use specific keywords, not full questions (e.g., \"configuration parameters timeout\" not \"How do I set the timeout?\")\n    - Include domain-specific terms that would appear in the source text\n    - Combine multiple relevant terms to narrow results (e.g., \"installation requirements dependencies\")\n    - Try synonyms or alternative phrasings if initial searches return poor results\n    - Avoid filler words and focus on content-bearing terms\n\n    Examples:\n        Good queries:\n        - \"error handling retry mechanism\"\n        - \"authentication token expiration\"\n        - \"memory allocation buffer size\"\n\n        Less effective queries:\n        - \"What are the error handling options?\"\n        - \"Tell me about authentication\"\n        - \"How does memory work?\"\n\n    Args:\n        query: Search query string - use specific keywords for best results\n        limit: Maximum number of results to return (default: 5)\n        document: Optional document source name to restrict search to (use list_docs to see available documents)\n        page: Optional page number to restrict search to (requires document to be specified)\n\n    Returns:\n        JSON string with search results including text excerpts and page numbers\n    \"\"\"\n    global _bm25_retriever, _corpus\n\n    if _bm25_retriever is None or not _corpus:\n        return json.dumps({\"error\": \"Search index not initialized\"})\n\n    # Validate that page requires document\n    if page and not document:\n        return json.dumps({\"error\": \"The 'page' parameter requires 'document' to be specified\"})\n\n    query_tokens = bm25s.tokenize([query], stopwords=\"en\")\n\n    # When filtering, retrieve more results to ensure we have enough after filtering\n    retrieve_limit = len(_corpus) if (document or page) else limit\n    results, scores = _bm25_retriever.retrieve(query_tokens, k=min(retrieve_limit, len(_corpus)))\n\n    search_results: list[dict[str, str | float]] = []\n    for i in range(results.shape[1]):\n        doc_idx = results[0, i]\n        score = float(scores[0, i])\n\n        if score &lt;= 0:\n            continue\n\n        passage = _corpus[doc_idx]\n\n        # Apply document filter\n        if document and passage[\"source\"] != document:\n            continue\n\n        # Apply page filter\n        if page and passage[\"page\"] != page:\n            continue\n\n        search_results.append(\n            {\n                \"text\": passage[\"text\"][:2000],\n                \"page\": passage[\"page\"],\n                \"source\": passage[\"source\"],\n                \"score\": round(score, 4),\n                \"url\": f\"file://{passage['source']}#page={passage['page']}\",\n            }\n        )\n\n        # Stop once we have enough results\n        if len(search_results) &gt;= limit:\n            break\n\n    return json.dumps({\"results\": search_results, \"query\": query, \"total\": len(search_results)})\n\n\n@mcp_server.tool()\ndef list_docs() -&gt; str:\n    \"\"\"List all documents in the search index with their page counts.\n\n    Returns:\n        JSON string with a list of documents, each containing the source name and page count.\n    \"\"\"\n    global _corpus\n\n    if not _corpus:\n        return json.dumps({\"error\": \"Search index not initialized\", \"documents\": []})\n\n    doc_pages: dict[str, set[str]] = {}\n    for passage in _corpus:\n        source = passage[\"source\"]\n        page = passage[\"page\"]\n        if source not in doc_pages:\n            doc_pages[source] = set()\n        doc_pages[source].add(page)\n\n    documents = [{\"source\": source, \"page_count\": len(pages)} for source, pages in sorted(doc_pages.items())]\n\n    return json.dumps({\"documents\": documents, \"total_documents\": len(documents)})\n\n\ndef build_config(model_alias: str, provider_name: str) -&gt; dd.DataDesignerConfigBuilder:\n    \"\"\"Build the Data Designer configuration for document Q&amp;A generation.\"\"\"\n    tool_config = dd.ToolConfig(\n        tool_alias=\"doc-search\",\n        providers=[provider_name],\n        allow_tools=[\"list_docs\", \"search_docs\"],\n        max_tool_call_turns=100,\n        timeout_sec=30.0,\n    )\n\n    config_builder = dd.DataDesignerConfigBuilder(tool_configs=[tool_config])\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"seed_id\",\n            sampler_type=dd.SamplerType.UUID,\n            params=dd.UUIDSamplerParams(),\n            drop=True,\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMStructuredColumnConfig(\n            name=\"topic_candidates\",\n            model_alias=model_alias,\n            prompt=\"Extract a high-level list of all topics covered by documents our knowledge base.\",\n            system_prompt=(\n                \"You must call tools before answering. \"\n                \"Do not use outside knowledge; only use tool results. \"\n                \"You can use as many tool calls as required to answer the user query.\"\n            ),\n            output_format=TopicList,\n            tool_alias=\"doc-search\",\n            with_trace=dd.TraceType.ALL_MESSAGES,  # Enable trace to capture tool call history\n        )\n    )\n\n    config_builder.add_column(\n        dd.ExpressionColumnConfig(\n            name=\"topic\",\n            expr=\"{{ topic_candidates.topics | random }}\",\n        )\n    )\n\n    qa_prompt = \"\"\"\\\nCreate a question-answer pair on the topic \"{{topic}}\", with supporting text and citation.\nThe supporting_passage must be a 2-4 sentence excerpt copied from the tool result that demonstrates\nwhy the answer is correct.\n\"\"\"\n\n    config_builder.add_column(\n        dd.LLMStructuredColumnConfig(\n            name=\"qa_pair\",\n            model_alias=model_alias,\n            prompt=qa_prompt,\n            system_prompt=(\n                \"You must call tools before answering. \"\n                \"Do not use outside knowledge; only use tool results. \"\n                \"You can use as many tool calls as required to answer the user query.\"\n            ),\n            output_format=QAPair,\n            tool_alias=\"doc-search\",\n            with_trace=dd.TraceType.ALL_MESSAGES,  # Enable trace to capture tool call history\n            extract_reasoning_content=True,\n        )\n    )\n\n    config_builder.add_column(\n        dd.ExpressionColumnConfig(\n            name=\"question\",\n            expr=\"{{ qa_pair.question }}\",\n        )\n    )\n    config_builder.add_column(\n        dd.ExpressionColumnConfig(\n            name=\"answer\",\n            expr=\"{{ qa_pair.answer }}\",\n        )\n    )\n    config_builder.add_column(\n        dd.ExpressionColumnConfig(\n            name=\"supporting_passage\",\n            expr=\"{{ qa_pair.supporting_passage }}\",\n        )\n    )\n    config_builder.add_column(\n        dd.ExpressionColumnConfig(\n            name=\"citation\",\n            expr=\"{{ qa_pair.citation }}\",\n        )\n    )\n    return config_builder\n\n\ndef generate_preview(\n    config_builder: dd.DataDesignerConfigBuilder,\n    num_records: int,\n    mcp_provider: dd.LocalStdioMCPProvider,\n) -&gt; PreviewResults:\n    \"\"\"Run Data Designer preview with the MCP provider.\"\"\"\n    data_designer = DataDesigner(mcp_providers=[mcp_provider])\n    # Traces are enabled per-column via with_trace=True on LLM column configs\n    return data_designer.preview(config_builder, num_records=num_records)\n\n\ndef _truncate(text: str, max_length: int = 100) -&gt; str:\n    \"\"\"Truncate text to max_length, adding ellipsis if needed.\"\"\"\n    text = text.replace(\"\\n\", \" \").strip()\n    if len(text) &lt;= max_length:\n        return text\n    return text[: max_length - 3] + \"...\"\n\n\ndef _summarize_content(content: object) -&gt; str:\n    \"\"\"Summarize ChatML-style content blocks for display.\"\"\"\n    if isinstance(content, list):\n        parts: list[str] = []\n        for block in content:\n            if isinstance(block, dict):\n                block_type = block.get(\"type\", \"block\")\n                if block_type == \"text\":\n                    text = str(block.get(\"text\", \"\"))\n                    if text:\n                        parts.append(text)\n                elif block_type == \"image_url\":\n                    parts.append(\"[image]\")\n                else:\n                    parts.append(f\"[{block_type}]\")\n            else:\n                parts.append(str(block))\n        return \" \".join(parts)\n    return str(content)\n\n\ndef _format_trace_step(msg: dict[str, object]) -&gt; str:\n    \"\"\"Format a single trace message as a concise one-liner.\"\"\"\n    role = msg.get(\"role\", \"unknown\")\n    content = _summarize_content(msg.get(\"content\", \"\"))\n    reasoning = msg.get(\"reasoning_content\")\n    tool_calls = msg.get(\"tool_calls\")\n    tool_call_id = msg.get(\"tool_call_id\")\n\n    if role == \"system\":\n        return f\"[bold cyan]system[/]({_truncate(str(content))})\"\n\n    if role == \"user\":\n        return f\"[bold green]user[/]({_truncate(str(content))})\"\n\n    if role == \"assistant\":\n        parts: list[str] = []\n        if reasoning:\n            parts.append(f\"[bold magenta]reasoning[/]({_truncate(str(reasoning))})\")\n        if tool_calls and isinstance(tool_calls, list):\n            for tc in tool_calls:\n                if isinstance(tc, dict):\n                    func = tc.get(\"function\", {})\n                    if isinstance(func, dict):\n                        name = func.get(\"name\", \"?\")\n                        args = func.get(\"arguments\", \"\")\n                        parts.append(f\"[bold yellow]tool_call[/]({name}: {_truncate(str(args), 60)})\")\n        if content:\n            parts.append(f\"[bold blue]content[/]({_truncate(str(content))})\")\n        return \"\\n\".join(parts) if parts else \"[bold blue]assistant[/](empty)\"\n\n    if role == \"tool\":\n        tool_id = str(tool_call_id or \"?\")[:8]\n        return f\"[bold red]tool_response[/]([{tool_id}] {_truncate(str(content), 80)})\"\n\n    return f\"[dim]{role}[/]({_truncate(str(content))})\"\n\n\ndef _display_column_trace(column_name: str, trace: list[dict[str, object]]) -&gt; None:\n    \"\"\"Display a trace for a single column using Rich Panel.\"\"\"\n    from rich.console import Console\n    from rich.panel import Panel\n\n    console = Console()\n    lines: list[str] = []\n\n    for msg in trace:\n        if not isinstance(msg, dict):\n            continue\n        formatted = _format_trace_step(msg)\n        for line in formatted.split(\"\\n\"):\n            lines.append(f\"  * {line}\")\n\n    trace_content = \"\\n\".join(lines) if lines else \"  (no trace messages)\"\n    panel = Panel(\n        trace_content,\n        title=f\"[bold]Column Trace: {column_name}[/]\",\n        border_style=\"blue\",\n        padding=(0, 1),\n    )\n    console.print(panel)\n\n\ndef display_preview_record(preview_results: PreviewResults) -&gt; None:\n    \"\"\"Display a sample record from the preview results with trace visualization.\"\"\"\n    from rich.console import Console\n\n    console = Console()\n    dataset = preview_results.dataset\n\n    if dataset is None or dataset.empty:\n        console.print(\"[red]No preview records generated.[/]\")\n        return\n\n    record = dataset.iloc[0].to_dict()\n\n    # Find trace columns and their base column names\n    trace_columns = [col for col in dataset.columns if col.endswith(\"__trace\")]\n\n    # Display non-trace columns as summary\n    non_trace_record = {k: v for k, v in record.items() if not k.endswith(\"__trace\")}\n    console.print(\"\\n[bold]Sample Record (data columns):[/]\")\n    console.print(json.dumps(non_trace_record, indent=2, default=str))\n\n    # Display each trace column in its own panel\n    if trace_columns:\n        console.print(\"\\n[bold]Generation Traces:[/]\")\n        for trace_col in trace_columns:\n            base_name = trace_col.replace(\"__trace\", \"\")\n            trace_data = record.get(trace_col)\n            if isinstance(trace_data, list):\n                _display_column_trace(base_name, trace_data)\n\n    preview_results.display_sample_record()\n\n\ndef serve() -&gt; None:\n    \"\"\"Run the MCP server (called when launched as subprocess by Data Designer).\"\"\"\n    pdf_sources_json = os.environ.get(\"PDF_SOURCES\", \"[]\")\n    pdf_sources = json.loads(pdf_sources_json)\n    if not pdf_sources:\n        pdf_sources = [DEFAULT_PDF_URL]\n    initialize_search_index(pdf_sources)\n    mcp_server.run()\n\n\ndef parse_args() -&gt; argparse.Namespace:\n    \"\"\"Parse command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Generate document Q&amp;A pairs using MCP tool calls with BM25S search.\")\n    subparsers = parser.add_subparsers(dest=\"command\")\n\n    # 'serve' subcommand for running the MCP server\n    subparsers.add_parser(\"serve\", help=\"Run the MCP server (used by Data Designer)\")\n\n    # Default command arguments (demo mode)\n    parser.add_argument(\"--model-alias\", type=str, default=\"nvidia-reasoning\", help=\"Model alias to use for generation\")\n    parser.add_argument(\"--num-records\", type=int, default=4, help=\"Number of Q&amp;A pairs to generate\")\n    parser.add_argument(\n        \"--pdf\",\n        type=str,\n        action=\"append\",\n        dest=\"pdfs\",\n        metavar=\"PATH_OR_URL\",\n        help=\"PDF file path or URL to index (can be specified multiple times). Defaults to a sample PDF if not provided.\",\n    )\n    # For compatibility with Makefile test-run-recipes target (ignored in demo mode)\n    parser.add_argument(\"--artifact-path\", type=str, default=None, help=argparse.SUPPRESS)\n\n    return parser.parse_args()\n\n\ndef main() -&gt; None:\n    \"\"\"Main entry point for the demo.\"\"\"\n    args = parse_args()\n\n    # Handle 'serve' subcommand\n    if args.command == \"serve\":\n        serve()\n        return\n\n    # Demo mode: run Data Designer with the BM25S MCP server\n    if os.environ.get(\"NVIDIA_API_KEY\") is None and args.model_alias.startswith(\"nvidia\"):\n        raise RuntimeError(\"NVIDIA_API_KEY must be set when using NVIDIA model aliases.\")\n\n    # Use provided PDFs or fall back to default\n    pdf_sources = args.pdfs if args.pdfs else [DEFAULT_PDF_URL]\n\n    # Configure MCP provider to run via stdio transport (local subprocess)\n    mcp_provider = dd.LocalStdioMCPProvider(\n        name=MCP_SERVER_NAME,\n        command=sys.executable,\n        args=[str(Path(__file__).resolve()), \"serve\"],\n        env={\"PDF_SOURCES\": json.dumps(pdf_sources)},\n    )\n\n    config_builder = build_config(\n        model_alias=args.model_alias,\n        provider_name=MCP_SERVER_NAME,\n    )\n\n    preview_results = generate_preview(\n        config_builder=config_builder,\n        num_records=args.num_records,\n        mcp_provider=mcp_provider,\n    )\n\n    display_preview_record(preview_results)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"recipes/qa_and_chat/multi_turn_chat/","title":"Multi-Turn Chat","text":"<p>Download Code </p> <pre><code># SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n# /// script\n# requires-python = \"&gt;=3.10\"\n# dependencies = [\n#     \"data-designer\",\n#     \"pydantic\",\n# ]\n# ///\n\"\"\"Multi-Turn Chat Generation Recipe\n\nGenerate synthetic multi-turn conversations between users and AI assistants across\ndifferent domains (Tech Support, Personal Finances, Educational Guidance). Each\nconversation varies in length, complexity, and user mood. Includes toxicity\nevaluation of user messages using an LLM judge.\n\nPrerequisites:\n    - OPENAI_API_KEY environment variable for OpenAI provider model aliases (default model alias is \"openai-text\").\n    - NVIDIA_API_KEY environment variable for NVIDIA provider model aliases.\n\nRun:\n    # Basic usage (generates 5 records by default)\n    uv run multi_turn_chat.py\n\n    # For help message and available options\n    uv run multi_turn_chat.py --help\n\"\"\"\n\nfrom pathlib import Path\nfrom typing import Literal\n\nfrom pydantic import BaseModel, Field\n\nimport data_designer.config as dd\nfrom data_designer.interface import DataDesigner, DatasetCreationResults\n\n\ndef build_config(model_alias: str) -&gt; dd.DataDesignerConfigBuilder:\n    config_builder = dd.DataDesignerConfigBuilder()\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"domain\",\n            sampler_type=dd.SamplerType.CATEGORY,\n            params=dd.CategorySamplerParams(values=[\"Tech Support\", \"Personal Finances\", \"Educational Guidance\"]),\n        )\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"topic\",\n            sampler_type=dd.SamplerType.SUBCATEGORY,\n            params=dd.SubcategorySamplerParams(\n                category=\"domain\",\n                values={\n                    \"Tech Support\": [\n                        \"Troubleshooting a Laptop\",\n                        \"Setting Up a Home Wi-Fi Network\",\n                        \"Installing Software Updates\",\n                    ],\n                    \"Personal Finances\": [\n                        \"Budgeting Advice\",\n                        \"Understanding Taxes\",\n                        \"Investment Strategies\",\n                    ],\n                    \"Educational Guidance\": [\n                        \"Choosing a College Major\",\n                        \"Effective Studying Techniques\",\n                        \"Learning a New Language\",\n                    ],\n                },\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"complexity\",\n            sampler_type=dd.SamplerType.CATEGORY,\n            params=dd.CategorySamplerParams(values=[\"Basic\", \"Intermediate\", \"Advanced\"]),\n        )\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"conversation_length\",\n            sampler_type=dd.SamplerType.CATEGORY,\n            params=dd.CategorySamplerParams(values=[2, 4, 6, 8]),\n        )\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"user_mood\",\n            sampler_type=dd.SamplerType.CATEGORY,\n            params=dd.CategorySamplerParams(\n                values=[\"happy\", \"silly\", \"sarcastic\", \"combative\", \"disappointed\", \"toxic\"]\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMTextColumnConfig(\n            name=\"assistant_system_prompt\",\n            prompt=(\n                \"Write a reasonable system prompt for a helpful AI assistant with expertise in \"\n                \"{{domain}} and {{topic}}. The AI assistant must not engage in harmful behaviors.\"\n            ),\n            model_alias=model_alias,\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMTextColumnConfig(\n            name=\"user_task\",\n            prompt=\"Define a simple task related to {{topic}} of {{complexity}} complexity for the user.\",\n            model_alias=model_alias,\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMStructuredColumnConfig(\n            name=\"conversation\",\n            prompt=(\n                \"&lt;task&gt;\\n{{user_task}}\\n&lt;/task&gt;\\n\\n\"\n                \"&lt;system_prompt&gt;{{assistant_system_prompt}}&lt;/system_prompt&gt;\\n\\n\"\n                \"Generate a conversation between a user and an AI assistant with &lt;system_prompt&gt; about &lt;task&gt;.\\n\"\n                \"User is asking the assistant for advice and is in a {{user_mood}} mood.\\n\"\n                \"The conversation must be {{conversation_length}} messages in length.\\n\"\n                \"The conversation must come to a natural end in {{conversation_length}} messages and if the assistant is unable \"\n                \"to solve the user's needs by then, they should offer to continue the conversation later or redirect them to additional resources.\\n\"\n                \"The conversation must be realistic and natural, incorporating details from both the user's and assistant's profiles.\\n\"\n                \"The AI assistant role has the system prompt defined in &lt;system_prompt&gt;. All of its responses must conform to &lt;system_prompt&gt;:\\n\"\n            ),\n            output_format=ChatConversation,\n            model_alias=model_alias,\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMJudgeColumnConfig(\n            name=\"toxicity_evaluation\",\n            prompt=TOXICITY_PROMPT_TEMPLATE,\n            scores=[toxicity_score],\n            model_alias=model_alias,\n        )\n    )\n\n    return config_builder\n\n\ndef create_dataset(\n    config_builder: dd.DataDesignerConfigBuilder,\n    num_records: int,\n    artifact_path: Path | str | None = None,\n) -&gt; DatasetCreationResults:\n    data_designer = DataDesigner(artifact_path=artifact_path)\n    results = data_designer.create(config_builder, num_records=num_records)\n    return results\n\n\ntoxicity_score = dd.Score(\n    name=\"Toxicity\",\n    description=\"Evaluation of harmful, harassing, or hateful language in conversations\",\n    options={\n        4: \"No toxicity detected; conversation is respectful and appropriate.\",\n        3: \"Mild toxicity; slightly rude or sarcastic but not hateful or harmful.\",\n        2: \"Moderate toxicity; contains disrespectful or harassing language.\",\n        1: \"Severe toxicity; contains overt hate, harassment, or harmful content.\",\n        0: \"Extreme toxicity; contains dangerous, threatening, or abusive content.\",\n    },\n)\n\nTOXICITY_PROMPT_TEMPLATE = (\n    \"You are an expert in content moderation and digital communication ethics. \"\n    \"You are evaluating the toxicity level in user messages.\\n\"\n    \"Take a deep breath and carefully analyze the conversation provided. \\n\"\n    \"Use the rubric to score the level of toxicity in the USER'S messages only.\\n\"\n    \"Consider:\\n\"\n    \"- Focus primarily on harassment, hate, or harmful language\\n\"\n    \"- Evaluate the full context of the conversation\\n\"\n    \"- Be objective and consistent in your assessment\\n\"\n    \"- If any single message exhibits toxicity, it should influence the final score\\n\"\n    \"## CONVERSATION\\n\"\n    \"{{ conversation }}\"\n)\n\n\nclass Message(BaseModel):\n    \"\"\"A single message turn in the conversation.\"\"\"\n\n    role: Literal[\"user\", \"assistant\"] = Field(..., description=\"Which role is writing the message.\")\n    content: str = Field(..., description=\"Message contents.\")\n\n\nclass ChatConversation(BaseModel):\n    \"\"\"A chat conversation between a specific user and an AI assistant.\n    * All conversations are initiated by the user role.\n    * The assistant role always responds to the user message.\n    * Turns alternate between user and assistant roles.\n    * The last message is always from the assistant role.\n    * Message content can be long or short.\n    * All assistant messages are faithful responses and must be answered fully.\n    \"\"\"\n\n    conversation: list[Message] = Field(..., description=\"List of all messages in the conversation.\")\n\n\nif __name__ == \"__main__\":\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser()\n    parser.add_argument(\"--model-alias\", type=str, default=\"openai-text\")\n    parser.add_argument(\"--num-records\", type=int, default=5)\n    parser.add_argument(\"--artifact-path\", type=str, default=None)\n    args = parser.parse_args()\n\n    config_builder = build_config(model_alias=args.model_alias)\n    results = create_dataset(config_builder, num_records=args.num_records, artifact_path=args.artifact_path)\n\n    print(f\"Dataset saved to: {results.artifact_storage.final_dataset_path}\")\n\n    results.load_analysis().to_report()\n</code></pre>"},{"location":"recipes/qa_and_chat/product_info_qa/","title":"Product Info QA","text":"<p>Download Code </p> <pre><code># SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n# /// script\n# requires-python = \"&gt;=3.10\"\n# dependencies = [\n#     \"data-designer\",\n#     \"pydantic\",\n# ]\n# ///\n\"\"\"Product Info Q&amp;A Recipe\n\nGenerate synthetic product Q&amp;A pairs with controlled hallucination for training\nand evaluating AI assistants. Each record includes a generated product (with name,\nfeatures, description, price), a user question, and an AI answer. Half of the\nanswers are generated without product context (hallucinated). Includes LLM judge\nevaluation for completeness and accuracy.\n\nPrerequisites:\n    - OPENAI_API_KEY environment variable for OpenAI provider model aliases (default model alias is \"openai-text\").\n    - NVIDIA_API_KEY environment variable for NVIDIA provider model aliases.\n\nRun:\n    # Basic usage (generates 5 records by default)\n    uv run product_info_qa.py\n\n    # For help message and available options\n    uv run product_info_qa.py --help\n\"\"\"\n\nimport string\nfrom pathlib import Path\n\nfrom pydantic import BaseModel, Field\n\nimport data_designer.config as dd\nfrom data_designer.interface import DataDesigner, DatasetCreationResults\n\n\ndef build_config(model_alias: str) -&gt; dd.DataDesignerConfigBuilder:\n    config_builder = dd.DataDesignerConfigBuilder()\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"category\",\n            sampler_type=dd.SamplerType.CATEGORY,\n            params=dd.CategorySamplerParams(\n                values=[\n                    \"Electronics\",\n                    \"Clothing\",\n                    \"Home Appliances\",\n                    \"Groceries\",\n                    \"Toiletries\",\n                    \"Sports Equipment\",\n                    \"Toys\",\n                    \"Books\",\n                    \"Pet Supplies\",\n                    \"Tools &amp; Home Improvement\",\n                    \"Beauty\",\n                    \"Health &amp; Wellness\",\n                    \"Outdoor Gear\",\n                    \"Automotive\",\n                    \"Jewelry\",\n                    \"Watches\",\n                    \"Office Supplies\",\n                    \"Gifts\",\n                    \"Arts &amp; Crafts\",\n                    \"Baby &amp; Kids\",\n                    \"Music\",\n                    \"Video Games\",\n                    \"Movies\",\n                    \"Software\",\n                    \"Tech Devices\",\n                ]\n            ),\n        )\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"price_tens_of_dollars\",\n            sampler_type=dd.SamplerType.UNIFORM,\n            params=dd.UniformSamplerParams(low=1, high=200),\n        )\n    )\n\n    config_builder.add_column(\n        dd.ExpressionColumnConfig(\n            name=\"product_price\",\n            expr=\"{{ (price_tens_of_dollars * 10) - 0.01 | round(2) }}\",\n            dtype=\"float\",\n        )\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"first_letter\",\n            sampler_type=dd.SamplerType.CATEGORY,\n            params=dd.CategorySamplerParams(values=list(string.ascii_uppercase)),\n        )\n    )\n\n    config_builder.add_column(\n        dd.SamplerColumnConfig(\n            name=\"is_hallucination\",\n            sampler_type=dd.SamplerType.BERNOULLI,\n            params=dd.BernoulliSamplerParams(p=0.5),\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMStructuredColumnConfig(\n            name=\"product_info\",\n            model_alias=model_alias,\n            prompt=(\n                \"Generate a realistic product description for a product in the {{ category }} \"\n                \"category that costs {{ product_price }}.\\n\"\n                \"The name of the product MUST start with the letter {{ first_letter }}.\\n\"\n            ),\n            output_format=ProductInfo,\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMTextColumnConfig(\n            name=\"question\",\n            model_alias=model_alias,\n            prompt=(\"Ask a question about the following product:\\n\\n {{ product_info }}\"),\n        )\n    )\n\n    config_builder.add_column(\n        dd.LLMTextColumnConfig(\n            name=\"answer\",\n            model_alias=model_alias,\n            prompt=(\n                \"{%- if is_hallucination == 0 -%}\\n\"\n                \"&lt;product_info&gt;\\n\"\n                \"{{ product_info }}\\n\"\n                \"&lt;/product_info&gt;\\n\"\n                \"{%- endif -%}\\n\"\n                \"User Question: {{ question }}\\n\"\n                \"Directly and succinctly answer the user's question.\\n\"\n                \"{%- if is_hallucination == 1 -%}\\n\"\n                \"Make up whatever information you need to in order to answer the user's request.\\n\"\n                \"{%- endif -%}\"\n            ),\n        )\n    )\n\n    # Evaluate answer quality\n    config_builder.add_column(\n        dd.LLMJudgeColumnConfig(\n            name=\"llm_answer_metrics\",\n            model_alias=model_alias,\n            prompt=(\n                \"&lt;product_info&gt;\\n\"\n                \"{{ product_info }}\\n\"\n                \"&lt;/product_info&gt;\\n\"\n                \"User Question: {{question }}\\n\"\n                \"AI Assistant Answer: {{ answer }}\\n\"\n                \"Judge the AI assistant's response to the user's question about the product described in &lt;product_info&gt;.\"\n            ),\n            scores=answer_quality_scores,\n        )\n    )\n\n    config_builder.add_column(\n        dd.ExpressionColumnConfig(\n            name=\"completeness_result\",\n            expr=\"{{ llm_answer_metrics.Completeness.score }}\",\n        )\n    )\n\n    config_builder.add_column(\n        dd.ExpressionColumnConfig(\n            name=\"accuracy_result\",\n            expr=\"{{ llm_answer_metrics.Accuracy.score }}\",\n        )\n    )\n\n    return config_builder\n\n\ndef create_dataset(\n    config_builder: dd.DataDesignerConfigBuilder,\n    num_records: int,\n    artifact_path: Path | str | None = None,\n) -&gt; DatasetCreationResults:\n    data_designer = DataDesigner(artifact_path=artifact_path)\n    results = data_designer.create(config_builder, num_records=num_records)\n    return results\n\n\nclass ProductInfo(BaseModel):\n    product_name: str = Field(..., description=\"A realistic product name for the market.\")\n    key_features: list[str] = Field(..., min_length=1, max_length=3, description=\"Key product features.\")\n    description: str = Field(\n        ...,\n        description=\"A short, engaging description of what the product does, highlighting a unique but believable feature.\",\n    )\n    price_usd: float = Field(..., description=\"The price of the product\", ge=10, le=1000, decimal_places=2)\n\n\ncompleteness_score = dd.Score(\n    name=\"Completeness\",\n    description=\"Evaluation of AI assistant's thoroughness in addressing all aspects of the user's query.\",\n    options={\n        \"Complete\": \"The response thoroughly covers all key points requested in the question, providing sufficient detail to satisfy the user's information needs.\",\n        \"PartiallyComplete\": \"The response addresses the core question but omits certain important details or fails to elaborate on relevant aspects that were requested.\",\n        \"Incomplete\": \"The response significantly lacks necessary information, missing major components of what was asked and leaving the query largely unanswered.\",\n    },\n)\n\naccuracy_score = dd.Score(\n    name=\"Accuracy\",\n    description=\"Evaluation of how factually correct the AI assistant's response is relative to the product information.\",\n    options={\n        \"Accurate\": \"The information provided aligns perfectly with the product specifications without introducing any misleading or incorrect details.\",\n        \"PartiallyAccurate\": \"While some information is correctly stated, the response contains minor factual errors or potentially misleading statements about the product.\",\n        \"Inaccurate\": \"The response presents significantly wrong information about the product, with claims that contradict the actual product details.\",\n    },\n)\n\nanswer_quality_scores = [completeness_score, accuracy_score]\n\n\nif __name__ == \"__main__\":\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser()\n    parser.add_argument(\"--model-alias\", type=str, default=\"openai-text\")\n    parser.add_argument(\"--num-records\", type=int, default=5)\n    parser.add_argument(\"--artifact-path\", type=str, default=None)\n    args = parser.parse_args()\n\n    config_builder = build_config(model_alias=args.model_alias)\n    results = create_dataset(config_builder, num_records=args.num_records, artifact_path=args.artifact_path)\n\n    print(f\"Dataset saved to: {results.artifact_storage.final_dataset_path}\")\n\n    results.load_analysis().to_report()\n</code></pre>"},{"location":"scripts/generate_colab_notebooks/","title":"Generate colab notebooks","text":"In\u00a0[\u00a0]: Copied! <pre># SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"Script to generate Colab-compatible notebooks from notebook source files.\n\nThis script processes jupytext percent-format Python files and:\n1. Injects Colab-specific setup cells (pip install, API key from secrets)\n2. Injects cells before the \"Import the essentials\" section\n3. Saves the result as .ipynb files in docs/colab_notebooks\n\"\"\"\n</pre> # SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. # SPDX-License-Identifier: Apache-2.0 \"\"\"Script to generate Colab-compatible notebooks from notebook source files.  This script processes jupytext percent-format Python files and: 1. Injects Colab-specific setup cells (pip install, API key from secrets) 2. Injects cells before the \"Import the essentials\" section 3. Saves the result as .ipynb files in docs/colab_notebooks \"\"\" In\u00a0[\u00a0]: Copied! <pre>from __future__ import annotations\n</pre> from __future__ import annotations In\u00a0[\u00a0]: Copied! <pre>import argparse\nfrom pathlib import Path\n</pre> import argparse from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import jupytext\nfrom nbformat import NotebookNode\nfrom nbformat.v4 import new_code_cell, new_markdown_cell\n</pre> import jupytext from nbformat import NotebookNode from nbformat.v4 import new_code_cell, new_markdown_cell In\u00a0[\u00a0]: Copied! <pre>COLAB_SETUP_MARKDOWN = \"\"\"\\\n### \u26a1 Colab Setup\n\nRun the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from [build.nvidia.com](https://build.nvidia.com).\n\"\"\"\n</pre> COLAB_SETUP_MARKDOWN = \"\"\"\\ ### \u26a1 Colab Setup  Run the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from [build.nvidia.com](https://build.nvidia.com). \"\"\" In\u00a0[\u00a0]: Copied! <pre>ADDITIONAL_DEPENDENCIES = {\n    \"4-providing-images-as-context.py\": '\"pillow&gt;=12.0.0,&lt;13\" \"datasets&gt;=4.0.0,&lt;5\"',\n}\n</pre> ADDITIONAL_DEPENDENCIES = {     \"4-providing-images-as-context.py\": '\"pillow&gt;=12.0.0,&lt;13\" \"datasets&gt;=4.0.0,&lt;5\"', } In\u00a0[\u00a0]: Copied! <pre>COLAB_INSTALL_CELL = \"\"\"\\\n%%capture\n!pip install -U data-designer\"\"\"\n</pre> COLAB_INSTALL_CELL = \"\"\"\\ %%capture !pip install -U data-designer\"\"\" In\u00a0[\u00a0]: Copied! <pre>COLAB_API_KEY_CELL = \"\"\"\\\nimport getpass\nimport os\n\nfrom google.colab import userdata\n\ntry:\n    os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\nexcept userdata.SecretNotFoundError:\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")\"\"\"\n</pre> COLAB_API_KEY_CELL = \"\"\"\\ import getpass import os  from google.colab import userdata  try:     os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\") except userdata.SecretNotFoundError:     os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")\"\"\" In\u00a0[\u00a0]: Copied! <pre>def create_colab_setup_cells(additional_dependencies: str) -&gt; list[NotebookNode]:\n    \"\"\"Create the Colab-specific setup cells to inject before imports.\"\"\"\n    cells = []\n    cells += [new_markdown_cell(source=COLAB_SETUP_MARKDOWN)]\n\n    install_cell = COLAB_INSTALL_CELL\n    if additional_dependencies:\n        install_cell += f\" {additional_dependencies}\"\n    cells += [new_code_cell(source=install_cell)]\n\n    cells += [new_code_cell(source=COLAB_API_KEY_CELL)]\n    return cells\n</pre> def create_colab_setup_cells(additional_dependencies: str) -&gt; list[NotebookNode]:     \"\"\"Create the Colab-specific setup cells to inject before imports.\"\"\"     cells = []     cells += [new_markdown_cell(source=COLAB_SETUP_MARKDOWN)]      install_cell = COLAB_INSTALL_CELL     if additional_dependencies:         install_cell += f\" {additional_dependencies}\"     cells += [new_code_cell(source=install_cell)]      cells += [new_code_cell(source=COLAB_API_KEY_CELL)]     return cells In\u00a0[\u00a0]: Copied! <pre>def find_import_section_index(cells: list[NotebookNode]) -&gt; int:\n    \"\"\"Find the index of the 'Import the essentials' markdown cell.\"\"\"\n    first_code_cell_index = -1\n    for i, cell in enumerate(cells):\n        if first_code_cell_index == -1 and cell.get(\"cell_type\") == \"code\":\n            first_code_cell_index = i\n\n        if cell.get(\"cell_type\") == \"markdown\":\n            source = cell.get(\"source\", \"\")\n            if \"import\" in source.lower() and \"essentials\" in source.lower():\n                return i\n    return first_code_cell_index\n</pre> def find_import_section_index(cells: list[NotebookNode]) -&gt; int:     \"\"\"Find the index of the 'Import the essentials' markdown cell.\"\"\"     first_code_cell_index = -1     for i, cell in enumerate(cells):         if first_code_cell_index == -1 and cell.get(\"cell_type\") == \"code\":             first_code_cell_index = i          if cell.get(\"cell_type\") == \"markdown\":             source = cell.get(\"source\", \"\")             if \"import\" in source.lower() and \"essentials\" in source.lower():                 return i     return first_code_cell_index In\u00a0[\u00a0]: Copied! <pre>def process_notebook(notebook: NotebookNode, source_path: Path) -&gt; NotebookNode:\n    \"\"\"Process a notebook to make it Colab-compatible.\n\n    Args:\n        notebook: The input notebook\n\n    Returns:\n        The processed notebook with Colab setup cells injected\n    \"\"\"\n    cells = notebook.cells\n\n    additional_dependencies = ADDITIONAL_DEPENDENCIES.get(source_path.name, \"\")\n\n    # Find where to insert Colab setup (before \"Import the essentials\")\n    import_idx = find_import_section_index(cells)\n\n    if import_idx == -1:\n        # If not found, insert after first cell (title)\n        import_idx = 1\n\n    # Insert Colab setup cells before the import section\n    colab_cells = create_colab_setup_cells(additional_dependencies)\n    processed_cells = cells[:import_idx] + colab_cells + cells[import_idx:]\n\n    notebook.cells = processed_cells\n    return notebook\n</pre> def process_notebook(notebook: NotebookNode, source_path: Path) -&gt; NotebookNode:     \"\"\"Process a notebook to make it Colab-compatible.      Args:         notebook: The input notebook      Returns:         The processed notebook with Colab setup cells injected     \"\"\"     cells = notebook.cells      additional_dependencies = ADDITIONAL_DEPENDENCIES.get(source_path.name, \"\")      # Find where to insert Colab setup (before \"Import the essentials\")     import_idx = find_import_section_index(cells)      if import_idx == -1:         # If not found, insert after first cell (title)         import_idx = 1      # Insert Colab setup cells before the import section     colab_cells = create_colab_setup_cells(additional_dependencies)     processed_cells = cells[:import_idx] + colab_cells + cells[import_idx:]      notebook.cells = processed_cells     return notebook In\u00a0[\u00a0]: Copied! <pre>def generate_colab_notebook(source_path: Path, output_dir: Path) -&gt; Path:\n    \"\"\"Generate a Colab-compatible notebook from a source file.\n\n    Args:\n        source_path: Path to the jupytext percent-format Python source file\n        output_dir: Directory to save the output notebook\n\n    Returns:\n        Path to the generated notebook\n    \"\"\"\n    # Read the source file using jupytext\n    notebook = jupytext.read(source_path)\n\n    # Process the notebook for Colab\n    notebook = process_notebook(notebook, source_path)\n\n    # Determine output path\n    output_path = output_dir / f\"{source_path.stem}.ipynb\"\n\n    # Ensure output directory exists\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Write the notebook\n    jupytext.write(notebook, output_path, config={\"metadata\": {\"jupytext\": {\"cell_metadata_filter\": \"-id\"}}})\n\n    return output_path\n</pre> def generate_colab_notebook(source_path: Path, output_dir: Path) -&gt; Path:     \"\"\"Generate a Colab-compatible notebook from a source file.      Args:         source_path: Path to the jupytext percent-format Python source file         output_dir: Directory to save the output notebook      Returns:         Path to the generated notebook     \"\"\"     # Read the source file using jupytext     notebook = jupytext.read(source_path)      # Process the notebook for Colab     notebook = process_notebook(notebook, source_path)      # Determine output path     output_path = output_dir / f\"{source_path.stem}.ipynb\"      # Ensure output directory exists     output_dir.mkdir(parents=True, exist_ok=True)      # Write the notebook     jupytext.write(notebook, output_path, config={\"metadata\": {\"jupytext\": {\"cell_metadata_filter\": \"-id\"}}})      return output_path In\u00a0[\u00a0]: Copied! <pre>def main() -&gt; None:\n    \"\"\"Main entry point for the script.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Generate Colab-compatible notebooks from notebook source files.\")\n    parser.add_argument(\n        \"--source-dir\",\n        type=Path,\n        default=Path(\"docs/notebook_source\"),\n        help=\"Directory containing notebook source files (default: docs/notebook_source)\",\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        type=Path,\n        default=Path(\"docs/colab_notebooks\"),\n        help=\"Directory to save Colab notebooks (default: docs/colab_notebooks)\",\n    )\n    parser.add_argument(\n        \"--files\",\n        nargs=\"*\",\n        help=\"Specific files to process (if not specified, process all .py files)\",\n    )\n\n    args = parser.parse_args()\n\n    # Get list of source files\n    if args.files:\n        source_files = [args.source_dir / f for f in args.files]\n    else:\n        source_files = sorted(args.source_dir.glob(\"*.py\"))\n        # Filter out files starting with underscore (like _README.md, _pyproject.toml)\n        source_files = [f for f in source_files if not f.name.startswith(\"_\")]\n\n    if not source_files:\n        print(f\"No source files found in {args.source_dir}\")\n        return\n\n    print(f\"\ud83d\udcd3 Generating Colab notebooks from {len(source_files)} source file(s)...\")\n    print(f\"   Source: {args.source_dir}\")\n    print(f\"   Output: {args.output_dir}\")\n    print()\n\n    for source_path in source_files:\n        if not source_path.exists():\n            print(f\"\u26a0\ufe0f  Skipping {source_path} (file not found)\")\n            continue\n\n        try:\n            output_path = generate_colab_notebook(source_path, args.output_dir)\n            print(f\"\u2705 {source_path.name} \u2192 {output_path.name}\")\n        except Exception as e:\n            print(f\"\u274c {source_path.name}: {e}\")\n\n    print()\n    print(f\"\u2728 Colab notebooks saved to {args.output_dir}/\")\n</pre> def main() -&gt; None:     \"\"\"Main entry point for the script.\"\"\"     parser = argparse.ArgumentParser(description=\"Generate Colab-compatible notebooks from notebook source files.\")     parser.add_argument(         \"--source-dir\",         type=Path,         default=Path(\"docs/notebook_source\"),         help=\"Directory containing notebook source files (default: docs/notebook_source)\",     )     parser.add_argument(         \"--output-dir\",         type=Path,         default=Path(\"docs/colab_notebooks\"),         help=\"Directory to save Colab notebooks (default: docs/colab_notebooks)\",     )     parser.add_argument(         \"--files\",         nargs=\"*\",         help=\"Specific files to process (if not specified, process all .py files)\",     )      args = parser.parse_args()      # Get list of source files     if args.files:         source_files = [args.source_dir / f for f in args.files]     else:         source_files = sorted(args.source_dir.glob(\"*.py\"))         # Filter out files starting with underscore (like _README.md, _pyproject.toml)         source_files = [f for f in source_files if not f.name.startswith(\"_\")]      if not source_files:         print(f\"No source files found in {args.source_dir}\")         return      print(f\"\ud83d\udcd3 Generating Colab notebooks from {len(source_files)} source file(s)...\")     print(f\"   Source: {args.source_dir}\")     print(f\"   Output: {args.output_dir}\")     print()      for source_path in source_files:         if not source_path.exists():             print(f\"\u26a0\ufe0f  Skipping {source_path} (file not found)\")             continue          try:             output_path = generate_colab_notebook(source_path, args.output_dir)             print(f\"\u2705 {source_path.name} \u2192 {output_path.name}\")         except Exception as e:             print(f\"\u274c {source_path.name}: {e}\")      print()     print(f\"\u2728 Colab notebooks saved to {args.output_dir}/\") In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"devnotes/archive/2026/","title":"2026","text":""}]}